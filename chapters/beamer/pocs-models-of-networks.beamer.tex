%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% models and analyses
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% add more about recent work on small world search
%% a bit of zombie theory

%% \section{Basic\ models\ of\ complex\ networks}

\begin{frame}
  \frametitle{Models}

  \begin{block}{Some important models:}
    \begin{enumerate}
    \item<2-> 
      Generalized random networks;
    \item<2-> 
      Small-world networks;
    \item<2-> 
      Generalized affiliation networks;
    \item<2-> 
      Scale-free networks;
    \item<2-> 
      Statistical generative models ($p^\ast$).
    \end{enumerate}
  \end{block}
  
\end{frame}

\section{Generalized\ random\ networks}

\begin{frame}
  \frametitle{Models}

  \begin{block}<1->{Generalized random networks:}
    \begin{itemize}
    \item<2-> Arbitrary degree distribution $P_k$.
    \item<3-> Create (unconnected) nodes with degrees sampled from $P_k$.
    \item<4-> Wire nodes together randomly.
    \item<5-> Create ensemble to test deviations from randomness.
    \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}[label=]
  \frametitle{Building random networks: Stubs}

  \begin{block}{Phase 1:}
    \begin{itemize}
    \item<1-> \alertg{Idea:} start with a soup of unconnected nodes
      with \alertb{stubs} (half-edges):
      \begin{center}
        \includegraphics[angle=-90,width=0.7\textwidth]{rn_stubs02_examples}
      \end{center}
    \end{itemize}
    \begin{columns}<2->
      \column{0.5\textwidth}
      \includegraphics[angle=-90,width=\textwidth]{rn_stubs03_examples}
      \column{0.5\textwidth}
      \begin{itemize}
      \item<3-> Randomly select stubs (not nodes!) and connect them.
      \item<4-> Must have an even number of stubs.
      \item<5-> Initially allow \alertg{self-} and \alertg{repeat} connections.
      \end{itemize}
    \end{columns}

  \end{block}

\end{frame}



\begin{frame}[label=]
  \frametitle{Building random networks: First rewiring}

  \begin{block}{Phase 2:}
    \begin{itemize}
    \item<1-> Now find any (A) self-loops and (B) repeat edges 
      and \alertg{randomly rewire} them.
      \begin{center}
        (A)
        \includegraphics[height=0.125\textheight]{rn_stubs04_examples}
        \qquad
        (B)
        \includegraphics[height=0.1\textheight]{rn_stubs05_examples}
      \end{center}
    \item<2-> \alertg{Being careful:} we can't change
      the degree of any node, so we can't simply move
      links around.
    \item<3-> \alertg{Simplest solution:} 
      randomly rewire \alertb{two edges} at a time.
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[label=]
  \small

  \frametitle{General random rewiring algorithm}

  \begin{columns}<1->
    \column{0.4\textwidth}
    \includegraphics[width=\textwidth]{rn_stubs06_examples}
    \column{0.6\textwidth}
    \begin{block}{}
      \begin{itemize}
      \item<1-> Randomly choose \alertg{two edges}.\\
        (Or choose problem edge and a random edge)
      \item<2-> Check to make sure edges are \alertb{disjoint}.
      \end{itemize}
    \end{block}
  \end{columns}
  \begin{columns}<3->
    \column{0.4\textwidth}
    \begin{overprint}
      \onslide<1-3 | handout: 0 | trans:0>
      \onslide<4->
      \includegraphics[width=\textwidth]{rn_stubs08_examples}
    \end{overprint}
    \column{0.6\textwidth}
    \begin{block}{}
      \begin{itemize}
      \item<3-> Rewire one end of each edge.
      \item<4-> Node degrees \alertg{do not change}.
      \item<5-> Works if $e_1$ is a self-loop or repeated edge.
      \item<6-> Same as finding on/off/on/off 4-cycles.
        and rotating them.
      \end{itemize}
    \end{block}
  \end{columns}
  
\end{frame}

\begin{frame}[label=]
  \frametitle{Sampling random networks}

  \begin{block}<1->{Phase 2:}
    \begin{itemize}
    \item<1-> Use rewiring algorithm to remove
      all self and repeat loops.
    \end{itemize}
  \end{block}  

  \begin{block}<2->{Phase 3:}
    \begin{itemize}
    \item<2-> \alertg{Randomize network} wiring by applying
      rewiring algorithm liberally.
    \item<3-> \alertb{Rule of thumb}:
      \# Rewirings $\simeq$
      10 $\times$ \# edges\cite{milo2003a}.
    \end{itemize}
  \end{block}  
  
\end{frame}


\section{Small-world\ networks}

\subsection{Main\ story}

\begin{frame}
  \small
  \frametitle{People thinking about people:}
  
  \begin{block}<1->{How are social networks structured?}
    \begin{itemize}
    \item<1-> How do we define and measure connections?
    \item<1-> Methods/issues of self-report and remote sensing.
    \end{itemize}
  \end{block}

  \begin{block}<2->{What about the dynamics of social networks?}
    \begin{itemize}
    \item<2-> How do social networks/movements begin \& evolve? 
    \item<2-> How does collective problem solving work? 
    \item<2-> How does information move through social networks?
    \item<2-> Which rules give the best `game of society?'
    \end{itemize}
  \end{block}

  \begin{block}<3->{Sociotechnical phenomena and algorithms:}
    \begin{itemize}
    \item<3->
      What can people and computers do together? (google)
    \item<3->
      Use \alertb{Play + Crunch} to solve problems.  Which problems?
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Social Search}

  \begin{block}<1->{A small slice of the pie:}
    \begin{itemize}
    \item<1-> 
      \alertg{Q.} Can people pass messages between distant individuals 
      using only their existing social connections?
    \item<2->
      \alertg{A.} Apparently yes...
    \end{itemize}
  \end{block}

%%   \begin{block}<3->{Handles:}
%%     \begin{itemize}
%%     \item<4-> 
%%       The Small World Phenomenon
%%     \item<5->
%%       or ``Six Degrees of Separation.''
%%     \end{itemize}
%%   \end{block}

\end{frame}

%% add milgram videos


\begin{frame}
  \frametitle{Milgram's social search experiment (1960s)}

  \begin{columns}
    \column{0.4\textwidth}
    \includegraphics[width=\textwidth]{milgrambook2.jpg}\\
    \tiny{\url{http://www.stanleymilgram.com}}

    \column{0.6\textwidth}
    \begin{block}{}
      \begin{itemize}
      \item Target person = \\ Boston stockbroker.
      \item 296 senders from Boston and Omaha.
      \item<2-> 20\% of senders reached target.
      \item<2-> chain length $\simeq$ 6.5.
      \end{itemize}
    \end{block}

     \begin{block}<3->{Popular terms:}
       \begin{itemize}
       \item<3-> 
         \alertb{The Small World Phenomenon;}
       \item<3->
         \alertb{``Six Degrees of Separation.''}
       \end{itemize}
     \end{block}

  \end{columns}

\end{frame}

\begin{frame}

  \begin{block}{Six Degrees of Kevin Bacon:}
    \begin{columns}
      \column{0.02\textwidth}
      \column{0.3\textwidth}
      \includegraphics[height=0.22\textheight]{kevin-bacon-footloose.jpg}
      \column{0.68\textwidth}
      \begin{itemize}
      \item
        It's a
        \wordwikilink{http://en.wikipedia.org/wiki/Six\_Degrees\_of\_Kevin\_Bacon}{game}:\\
        ``Kevin Bacon is the Center of the Universe''
      \item 
        \wordwikilink{http://oracleofbacon.org}{The Oracle of Bacon}
      \end{itemize}
    \end{columns}
  \end{block}
  
  \begin{block}{Six Degrees of Paul Erd\"{o}s:}
    \begin{columns}
      \column{0.02\textwidth}
      \column{0.3\textwidth}
      \includegraphics[height=0.22\textheight]{paul-erdos.jpg}
      \column{0.68\textwidth}
      \begin{itemize}
      \item 
        Academic papers.
      \item
        \wordwikilink{http://bit.ly/19puO29}{Erd\"{o}s Number}
      \item
        \wordwikilink{http://www.oakland.edu/enp/}{Erd\"{o}s Number Project}
      \end{itemize}
    \end{columns}
  \end{block}

  \begin{block}{}
    \begin{itemize}
    \item<2->
      So naturally we must have the
      \wordwikilink{http://bit.ly/16TyyLf}{Erd\"{o}s-Bacon Number} ...
    \item<3->
      One computational Story Lab team member has EBN $< \infty$.
    \item<4->
      Natalie Hershlag's (Portman's) EBN\# = 5 + 2 = 7.
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  %% include tweets by strogatz ...

  \frametitle{Good Will Hunting:}
  \includegraphics[width=\textwidth]{good_will_hunting_3.jpg}

  \begin{columns}
    \column{0.10\textwidth}
    \column{0.20\textwidth}
    \includegraphics[height=.2\textheight]{Daniel_Kleitman.jpg}
    \column{0.7\textwidth}
    \begin{itemize}
    \item 
      Boardwork by
      \wordwikilink{http://en.wikipedia.org/wiki/Daniel\_Kleitman}{Dan
        Kleitman}, EBN\# = 1 + 2 = 3.
    \item
      See Kleitman's sidebar in \\
      \wordwikilink{http://www.ams.org/notices/199804/review-saul.pdf}{Mark Saul's Movie Review}\\
      (Notices of the AMS, Vol. 45, 1998.)
    \end{itemize}
  \end{columns}
  
\end{frame}


\begin{frame}

  \includegraphics[height=0.8\textheight]{nsa-three-degrees.png}

  \begin{itemize}
  \item 
    \wordwikilink{http://arstechnica.com/information-technology/2013/07/you-may-already-be-a-winner-in-nsas-three-degrees-surveillance-sweepstakes/}{Many people}
    are within three degrees from a random person ...
  \end{itemize}

\end{frame}


%% \begin{frame}
%%   \frametitle{The problem}
%% 
%%   \begin{block}{Stanley Milgram et al., late 1960's:}
%%     \begin{itemize}
%%     \item Target person worked in Boston as a stockbroker.
%%     \item 296 senders from Boston and Omaha.
%%     \item 20\% of senders reached target.
%%     \item average chain length $\simeq$ 6.5.
%%     \end{itemize}
%%   \end{block}
%% 
%% \end{frame}

\begin{frame}
   \frametitle{The problem}

   \begin{columns}
     \column{0.65\textwidth}
     \begin{block}{Lengths of successful chains:}
       \includegraphics[width=\textwidth]{figbasicmilg_noname}
     \end{block}
     \column{0.35\textwidth}
     From Travers and Milgram (1969) in Sociometry:\cite{travers1969a}\\
     \alertb{``An Experimental Study of the Small World Problem.''}
   \end{columns}

\end{frame}

\begin{frame}
  \frametitle{The problem}

  \begin{block}{
      \alertb{Two features}
      characterize a social `Small World':}
    \begin{enumerate}
    \item<2-> Short paths exist, \uncover<4->{\alertb{(= Geometric piece)}}
    \item[]<2-> and 
    \item<3-> People are good at finding them. \uncover<5->{\alertb{(= Algorithmic piece)}}
    \end{enumerate}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Social Search}

  \begin{block}{Milgram's small world experiment with email:}
    \begin{center}
      \includegraphics[width=0.65\textwidth]{window2}
    \end{center}
  \end{block}

{\small
  \begin{block}{}
  \alertg{``An Experimental study of Search in Global Social Networks''}\\
  P.~S.~Dodds, R.~Muhamad, and D.~J.~Watts,\\ 
  \textit{Science}, Vol.~301, pp.~827--829, 2003.\cite{dodds2003b}
  \end{block}
}
\end{frame}

\begin{frame}
  \frametitle{Social search---the Columbia experiment}

  \begin{block}{}
  \begin{itemize}
  \item<1-> 
    60,000+ participants in 166 countries
  \item<2-> 
    18 targets in 13 countries including
    \begin{itemize}
    \item<3-> 
      a professor at an Ivy League university,\\
    \item<4-> 
      an archival inspector in Estonia,\\
    \item<5-> 
      a technology consultant in India,\\
    \item<6-> 
      a policeman in Australia,\\
    \item[]<7-> 
      and 
    \item<7-> 
      a veterinarian in the Norwegian army.
    \end{itemize}
  \item<8->
    24,000+ chains
  \end{itemize}
  \end{block}

  \begin{block}<9->{We were lucky and contagious (more later):}
    \wordwikilink{http://www.nytimes.com/2001/12/20/technology/circuits/20STUD.html}{``Using E-Mail to Count Connections''}, Sarah Milstein, New York Times, Circuits Section (December, 2001)
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{All targets:}

  \begin{block}{}
    \includegraphics[height=0.85\textheight]{dodds2003b_tabS1}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Social search---the Columbia experiment}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Milgram's participation rate was roughly 75\%
  \item<2->
    Email version: Approximately 37\% participation rate.
  \item<3->
    Probability of a chain of length 10 getting through:
    $$.37^{10} \simeq 5 \times 10^{-5}$$
  \item<4->
  $\Rightarrow$ 384 completed chains (1.6\% of all chains).
  \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}
  \frametitle{Social search---the Columbia experiment}

  % results

  \begin{block}{}
  \begin{itemize}
  \item<1->  
    Motivation/Incentives/Perception matter.
  \item<2->  
    If target \textit{seems} reachable\\
    $\Rightarrow$ participation more likely.
  \item<3->  
    Small changes in attrition rates\\
    $\Rightarrow$ large changes in completion rates
  \item<4->  
    e.g., $\searrow$ 15\% in attrition rate \\
    $\Rightarrow$ $\nearrow$ 800\% in completion rate
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Social search---the Columbia experiment}  

  \begin{block}{Comparing successful to unsuccessful chains:}
    \begin{itemize}
    \item 
      Successful chains used relatively weaker ties:
    \end{itemize}
    \includegraphics[width=0.8\textwidth]{figcistrength3.pdf}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{Social search---the Columbia experiment}

  \begin{block}<1->{Successful chains disproportionately used:}
    \begin{itemize}
    \item<2-> 
      Weak ties, Granovetter\cite{granovetter1973a}
    \item<3-> 
      Professional ties (34\% vs.\ 13\%)
    \item<4-> 
      Ties originating at work/college
    \item<5-> 
      Target's work (65\% vs.\ 40\%)
    \end{itemize}
  \end{block}
  
  \begin{block}<6->{\ldots and disproportionately avoided}
    \begin{itemize}
    \item<7-> 
      hubs (8\% vs. 1\%) (+ no evidence of funnels)
    \item<8->
      family/friendship ties (60\% vs. 83\%)
    \end{itemize}
  \end{block}

  \begin{block}<9->{Geography $\rightarrow$ Work}
  \end{block}  

\end{frame}


\begin{frame}
  \frametitle{Social search---the Columbia experiment}


  \begin{block}{}
  Senders of successful messages showed\\
  \alertb{little absolute dependency} on
  \begin{itemize}
  \item<1->
    age, gender
  \item<2->
    country of residence
  \item<3-> 
    income
  \item<4-> 
    religion
  \item<5-> 
    relationship to recipient
  \end{itemize}

  \bigskip

  \uncover<6->{
    Range of completion rates for subpopulations: \\
    \mbox{} \hfill 30\% to 40\%
  }
  \end{block}

\end{frame}


%% \begin{frame}
%%   \frametitle{Social search---the Columbia experiment}
%% 
%% % Age 30-39   39.3%
%% % Australia     40.0%
%% % Graduate level education 41.9%
%% % Gender Male 39.6%
%% % occupation > 20 counts: mass media 47.0%
%% % position 
%% % high school student 31.0%
%% % college student 32.2%
%% % retired 32.9%
%% % nreligion christian 36.2%, buddhism 33.5%, islam 32.3%
%% 
%% 
%% % Age 17 or under 32.8%
%% % Canada 34.7%
%% % Elementary school 28.3%
%% % Gender female 37.1%
%% % occupation > 20 counts: consumer services 29.2%
%% % position 
%% % `other' 40%
%% % specialist/engineer 39.8%
%% % university student 39.8%
%% % religion none 40.5%
%% 
%% % 69 countries
%% % Canada, Italy, France, U.S.
%% % Australia, Germany, Norway, Finland
%% 
%% % uber sender
%% % mass media
%% % > 100k
%% % graduate
%% % male
%% 
%% Nevertheless, some weak discrepencies do exist...
%% 
%% \begin{block}<1->{Contrived hypothetical above average connector:}
%%   Norwegian, secular male, aged 30-39, earning over \$100K, 
%%   with graduate level education working in mass media or science,
%%   who uses relatively weak ties to people
%%   they met in college or at work.
%% \end{block}
%% 
%% \begin{block}<2->{Contrived hypothetical below average connector:}
%%   Italian, religious female earning less than \$2K,
%%   with elementary school education and retired,
%%   who uses strong ties to family members.
%% \end{block}
%% 
%% \end{frame}

\begin{frame}
  \frametitle{Social search---the Columbia experiment}

  \begin{block}{Mildly bad for continuing chain:}
    choosing recipients because 
    \alertg{``they have lots of friends''}
    or because they will 
    \alertg{``likely continue the chain.''}
  \end{block}

  \begin{block}<2->{Why:}
    \begin{itemize}
    \item<2-> 
      Specificity important
    \item<3-> 
      Successful links used relevant information.\\
      (e.g. connecting to someone who shares same profession as target.)
    \end{itemize}
  \end{block}

\end{frame}


% \begin{frame}
%  \frametitle{Social search---the Columbia experiment}
%
%  \includegraphics[height=0.86\textheight]{figsw_2_r_invert_all3_mod_noname}
%
%\end{frame}

\begin{frame}
  \frametitle{Social search---the Columbia experiment}
  \begin{block}{Basic results:}
    
    \begin{itemize}
    \item<1->
      $\avg{L} = 4.05$ for all completed chains
    \item<2->
      $L_\ast$ = Estimated `true' median chain length (zero attrition)
    \item<3->
      Intra-country chains: $L_\ast = 5$ 
    \item<4->
      Inter-country chains:
      $L_\ast = 7$ 
    \item<5->
      All chains:
      $L_\ast = 7$ 
    \item<6->
      Milgram:
      $L_\ast \simeq$ 9
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Usefulness:}

  \begin{block}{Harnessing social search:}
    \begin{itemize}
    \item<2->
      Can distributed social search be used for something big/good? 
    \item<3->
      What about something evil?  (Good idea to check.)
    \item<4->
      What about socio-inspired algorithms for information search?  (More later.)
    \item<5->
      For real social search, we have an incentives problem.
    \item<6->
      Which kind of influence mechanisms/algorithms 
      would help propagate search?
    \item<7->
      Fun, money, prestige, ... ?
    \item<8->
      Must be `non-gameable.'
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Red balloons:}

  \begin{block}<+->{A Grand Challenge:}
    \begin{itemize}
    \item<+->
      1969: The Internet \wordwikilink{http://en.wikipedia.org/wiki/History_of_the_Internet}{is born}\\
      (the \wordwikilink{http://en.wikipedia.org/wiki/ARPANET}{ARPANET}---four nodes!).
    \item<+->
      Originally funded by DARPA who created 
      a grand
      \wordwikilink{http://en.wikipedia.org/wiki/DARPA_Network_Challenge}{Network Challenge}
      for the 40th anniversary.
    \item<+->
      Saturday December 5, 2009: DARPA 
      puts 10 red weather balloons up during the day.
    \item<+->
      Each 8 foot diameter balloon is anchored to the ground
      \alertb{somewhere in the United States}.
    \item<+->
      Challenge: Find the latitude and longitude of each balloon.
    \item<+->
      Prize: \alertg{\$40,000}.
    \end{itemize}
  \end{block}

  \uncover<3->{
    {
      \small
      \mbox{}$^\ast$DARPA = \wordwikilink{http://www.darpa.mil/}{Defense Advanced Research Projects Agency}.
    }
  }

\end{frame}

\begin{frame}
  \frametitle{Where the balloons were:}

  \includegraphics[width=\textwidth]{BalloonMap.jpg}
\end{frame}

\begin{frame}
  \frametitle{Finding red balloons:}

  \begin{block}{The winning team and strategy:}
    \begin{itemize}
    \item<+-> 
      \wordwikilink{http://www.media.mit.edu/}{MIT's Media Lab} 
      won in less than 9 hours.\cite{pickard2011a}
    \item<+-> 
      Pickard et al. ``Time-Critical Social Mobilization,''\cite{pickard2011a}
      Science Magazine, 2011.
    \item<+-> 
      People were virally recruited online to help out.
    \item<+-> 
      Idea: Want people to both
      (1) find the balloons, and
      (2) involve more people.
    \item<+-> 
      Recursive incentive structure with exponentially decaying payout: 
      \begin{itemize}
      \item<+->
        \$2000 for correctly reporting
        the coordinates of a balloon.
      \item<+-> 
        \$1000 for recruiting a person who finds a balloon.
      \item<+-> 
        \$500 for recruiting a person who recruits the balloon finder, \ldots
      \item<+-> 
        (Not a Ponzi scheme.)
      \end{itemize}
    \item<+->
      True victory: 
      \wordwikilink{http://www.colbertnation.com/the-colbert-report-videos/260725/january-05-2010/riley-crane}{Colbert interviews Riley Crane}
    \end{itemize}
  \end{block}

%% no longer around:
%%     \item<3-> 
%%       Sign up: 
%%       {\small 
%%         \{http://balloon.media.mit.edu/your\_humorous\_name}
%%       }
  
\end{frame}

\begin{frame}
  \frametitle{Finding balloons:}
  
  \begin{block}{Clever scheme:}
    \begin{itemize}
    \item<1-> 
      Max payout = \$4000 per balloon.
    \item<2-> 
      Individuals have clear incentives to both 
      \begin{enumerate}
      \item 
        \alertg{involve/source more people} (spread), and
      \item
        \alertg{find balloons} (goal action).
      \end{enumerate}
    \item<3-> Gameable?
    \item<4-> Limit to how much money a set of bad actors can extract.
    \end{itemize}
  \end{block}

  \begin{block}<5->{Extra notes:}
    \begin{itemize}
    \item<5-> 
      MIT's brand helped greatly.
    \item<6-> 
      MIT group first heard about the competition a few days before.
      \visible<7->{\alertg{Ouch.}}
    \item<8-> 
      A number of other teams 
      \wordwikilink{https://networkchallenge.darpa.mil/FinalStandings.pdf}{did well}.
    \item<9-> 
      Worthwhile looking at these competing strategies.\cite{pickard2011a}
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}

  \begin{block}{Collective Detective:}
    \begin{itemize}
    \item<+->
      \wordwikilink{http://www.nytimes.com/2013/06/25/us/a-parallel-search-for-a-missing-panda.html}{Finding an errant panda}
    \item<+-> 
      Nature News:
      \wordwikilink{http://www.nature.com/news/crowdsourcing-in-manhunts-can-work-1.12867}
      {``Crowdsourcing in manhunts can work:
        Despite mistakes over the Boston bombers, social media can help to find people quickly''}
      by Philip Ball
      (April 26, 2013)
    \end{itemize}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{The social world appears to be small...  why?}
  
  \begin{block}{Theory: how do we understand the small world property?}
    \begin{itemize}
    \item<1->
      Connected \alertb{random networks}
      have short average path lengths:
      $$\tavg{d_{AB}} \sim \log(N)$$
    \item[]<1->
      $N$ = population size,
    \item[]<1->
      $d_{AB}$ = distance between nodes $A$ and $B$.
    \item<2->
      \alertg{But: social networks aren't random...}
    \end{itemize}
  \end{block}

\end{frame}




\begin{frame}
  \frametitle{Simple socialness in a network:}

  \begin{columns}
    \column{0.6\textwidth}
    \includegraphics[width=\textwidth]{clustering}
    \column{0.4\textwidth}
    Need \alertg{``clustering''} (your friends are likely to know each other):
  \end{columns}

\end{frame}


\begin{frame}
  \frametitle{Non-randomness gives clustering:}

  \begin{center}
    \includegraphics[height=0.65\textheight]{lattice3}
  \end{center}

  $d_{AB}=10$ $\rightarrow$ too many long paths.

\end{frame}

\begin{frame}
  \frametitle{Randomness + regularity}

  \begin{center}
    \includegraphics[height=0.65\textheight]{latticeshortcut3}
  \end{center}

  \alertg{Now have $d_{AB}=3$}
  \hfill $\tavg{d}$ decreases overall
\end{frame}

\begin{frame}
  \frametitle{Small-world networks}

  \begin{block}{}
  Introduced by
  Watts and Strogatz (Nature, 1998)\cite{watts1998a}\\
  \alertg{``Collective dynamics of `small-world' networks.''}
  \end{block}

  \begin{block}<2->{Small-world networks were found everywhere:}
    \begin{itemize}
    \item<2-> neural network of C. elegans,
    \item<3-> semantic networks of languages,
    \item<4-> actor collaboration graph,
    \item<5-> food webs,
    \item<6-> social networks of comic book characters,...
    \end{itemize}
  \end{block}

  \begin{block}<7->{Very weak requirements:}
    \begin{itemize}
    \item<7-> \alertg{local regularity}
      \uncover<8->{+ random \alertb{short cuts}}
    \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}

  \begin{block}{Papers should be apps:}
      \begin{center}
        \includegraphics[height=0.6\textheight]{bretvictor-smallworlds.png}
      \end{center}
      \begin{itemize}
      \item 
        Bret Victor's \wordwikilink{http://worrydream.com/ScientificCommunicationAsSequentialArt/}{Scientific Communication As Sequential Art}
      \item 
        Interactive figures and tables = windows into large data sets
        (empirical or simulated).
    \end{itemize}
  \end{block}

\end{frame}

%% \begin{frame}
%%   \frametitle{Toy model:}
%% 
%%   \begin{block}{}
%%     \includegraphics[width=\textwidth]{watts1998a_fig1.pdf}    
%%   \end{block}
%% 
%% \end{frame}

\begin{frame}
  \frametitle{The structural small-world property:}

  \begin{block}{}
    \includegraphics[width=0.9\textwidth]{watts1998a_fig2.pdf}

    \small
    \begin{itemize}
    \item 
      $L(p)$ = average shortest path length as a function of $p$
    \item 
      $C(p)$ = average clustring as a function of $p$
    \end{itemize}
  \end{block}

\end{frame}



\begin{frame}
  \frametitle{Previous work---finding short paths}

  \begin{block}{}
  But are these short cuts findable?

  \bigskip

  \uncover<2->{\alertg{Nope.}\cite{kleinberg2000a}}

  \bigskip

  \uncover<3->{
  Nodes \alertb{cannot} find each other quickly\\ 
  with \alertb{any local search method}.
  }

  \bigskip

  \uncover<4->{Need a more sophisticated model...}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Previous work---finding short paths}

  \begin{block}{}
    \begin{itemize}
    \item<1-> What can a local search method reasonably use?
    \item<2->  How to find things without a map?
    \item<3-> \alertb{Need some measure of distance between friends
        and the target.}
    \end{itemize}
  \end{block}
  
  \bigskip

  \begin{block}<4->{Some possible knowledge:}
    \begin{itemize}
    \item<1-> Target's identity
    \item<1-> Friends' popularity 
    \item<1-> Friends' identities 
    \item<1-> Where message has been 
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Previous work---finding short paths}

  \begin{block}{}
    Jon Kleinberg (Nature, 2000)\cite{kleinberg2000a}\\
    ``Navigation in a small world.''
  \end{block}

   \bigskip
   
   \begin{block}<2->{Allowed to vary:}
     \begin{enumerate}
     \item<2-> local search algorithm
     \item[]<3-> and
     \item<3-> network structure.
     \end{enumerate}
   \end{block}

\end{frame}

\begin{frame}
  \frametitle{Previous work---finding short paths}

  \begin{block}<1->{Kleinberg's Network:}
    \begin{enumerate}
    \item<2->
      Start with
      regular d-dimensional cubic lattice.
    \item<3-> 
      Add local links so 
      nodes know all nodes within a distance $q$.
    \item<4->
      Add $m$ short cuts per node.
    \item<5->  
      Connect $i$ to $j$ with probability 
      $$ p_{ij} \propto {x_{ij}}^{-\alpha}. $$
    \end{enumerate}
  \end{block}

  \begin{block}{}
    \begin{itemize}
    \item<6-> 
      \alertg{$\alpha=0$}: random connections.
    \item<6->  
      \alertg{$\alpha$ large}: reinforce local connections.
    \item<6-> 
      \alertg{$\alpha=d$}: connections grow logarithmically in space.
    \end{itemize}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{Previous work---finding short paths}

  \begin{block}{Theoretical optimal search:}
    \begin{itemize}
    \item<1-> 
      ``Greedy'' algorithm.
    \item<2-> 
      Number of connections grow logarithmically (slowly)
      in space: $\alpha=d$.
    \item<3-> 
      Social golf.
    \end{itemize}

    \bigskip
    \visible<4->{
      Search time grows slowly with system size (like $\log^2N$).
      }

 %  For $\alpha \ne d$, polynomial factor $N^\beta$ appears.

    \bigskip
    \visible<5->{
      \alertg{But: social networks aren't lattices plus links.}
    }
    
  \end{block}
  
\end{frame}

\begin{frame}

  \begin{block}{Advances for understanding Kleinberg's model}

    \displaypaper{roberson2006a}{1}

    \displaypaper{carmi2009a}{2}

    %% must fix bracket matching
    \displaypaper{cartoza2009a}{2}
    
  \end{block}

  %% bagrow
  %% The tl;dr key takeaways are the 3rd-6th paragraphs of the first paper
  %% (which all boils down to Eq 1) and Fig 2 in the second paper.

\end{frame}

\begin{frame}
  \frametitle{Previous work---finding short paths}

  \begin{block}{}
    \begin{itemize}
    \item<1-> 
      If networks have \alertb{hubs} can 
      also search well: Adamic et al. (2001)\cite{adamic2001a}
      $$ P(k_i) \propto k_i^{-\gamma}$$
      where $k$ = degree of node $i$ (number of friends).
    \item<2->
      Basic idea: get to hubs first\\
      (airline networks).
    \item<3->   
      \alertg{But: hubs in social networks are limited.}
    \end{itemize}
  \end{block}
  
\end{frame}

\subsection{Generalized\ affiliation\ networks}

\begin{frame}
  \frametitle{The problem}

  \begin{block}{}
  If there are no hubs and no underlying lattice,
  how can search be efficient?

  \includegraphics[width=0.45\textwidth]{barenetwork}%
  \raisebox{8ex}{\begin{tabular}{l}
      \\
      Which friend of \alertb{a} is closest \\
      to the target \alertb{b}?\\
      \\
      What does `closest' mean?\\
      \\
      What is
      `social distance'?  \\
      \end{tabular}}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{Models}

  \begin{block}{}
  One approach: incorporate \alertb{identity}.

  \bigskip

  \visible<2->{
    \alertb{Identity is formed from attributes such as:}
    \begin{itemize}
    \item<1-> 
      Geographic location
    \item<1-> 
      Type of employment
    \item<1-> 
      Religious beliefs
    \item<1-> 
      Recreational activities.
    \end{itemize}
  }

  \bigskip

  \visible<3->{
    \alertb{Groups} are formed by people with at least one similar attribute.
  }

  \bigskip

  \visible<4->{
    Attributes $\Leftrightarrow$ 
    Contexts $\Leftrightarrow$ 
    Interactions $\Leftrightarrow$ 
    Networks.
    }
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Social distance---Bipartite affiliation networks}

  \begin{block}{}
  \centering
  \includegraphics[height=0.75\textheight]{bipartite}
  \begin{itemize}
  \item 
    Bipartite affiliation networks: boards and directors, movies and actors.
  \end{itemize}
  \end{block}

  %% boards of directors
  %% movies
  %% transportation

\end{frame}



\begin{frame}
  \frametitle{Social distance---Context distance}

  \begin{block}{}
    \centering
    \includegraphics[width=\textwidth]{bipartite2}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Models}

  \begin{block}{}
    Distance between two individuals $x_{ij}$ 
    is the height of lowest common ancestor.

    \begin{center}
      \includegraphics[width=0.8\textwidth]{fig01_hierarchy_againA}
    \end{center}

    \alertb{$x_{ij}=3$, $x_{ik}=1$, $x_{iv}=4$.}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Models}

  \begin{block}{}
    \begin{itemize}
    \item<1-> 
      Individuals are more
      likely to know each other the closer they are
      within a hierarchy.
    \item<2-> 
      Construct $z$ connections for each node
      using
      \alertb{$$p_{ij} =c\exp\{-\alpha x_{ij}\}.$$}
    \item<3-> 
      \alertg{$\alpha=0$}: random connections.
    \item<4-> 
      \alertg{$\alpha$ large}: local connections.
    \end{itemize}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{Models}

  \begin{block}{Generalized affiliation networks}

    \medskip

    \includegraphics[width=1\textwidth]{generalcontext2}  
    \begin{itemize}
    \item Blau \& Schwartz\cite{blau1984a}, Simmel\cite{simmel1902a},
      Breiger\cite{breiger1974a}, Watts \etal\cite{watts2002b}; 
      see also Google+ Circles.
    \end{itemize}
  \end{block}


\end{frame}

%% \begin{frame}
%%   \frametitle{Models}
%% 
%%   \begin{block}<1->{3. small-world networks}
%%     \begin{itemize}
%%     \item Introduced by Watts and Strogatz\cite{watts1998a}
%%     \end{itemize}
%%     \medskip
%%     \visible<2->{Two scales:}
%%     \begin{itemize}
%%     \item<3-> \alertg{local regularity} (an individual's friends know each other)
%%     \item<4-> \alertg{global randomness} (shortcuts).
%%     \end{itemize}
%% 
%%     \begin{columns}
%%       \column{0.65\textwidth}
%%       \begin{itemize}
%%       \item<5-> Shortcuts allow disease to jump
%%       \item<6-> Number of infectives increases exponentially in time
%%       \end{itemize}
%%       \column{0.35\textwidth}
%%       \begin{overprint}
%%         \onslide<1-2| handout:0| trans:0>
%%         \onslide<3| handout:0| trans:0>
%%         \includegraphics[height=0.3\textheight]{lattice4}
%%         \onslide<4-| handout:1| trans:1>
%%         \includegraphics[height=0.3\textheight]{latticeshortcut4}
%%       \end{overprint}
%%     \end{columns}
%%   \end{block}
%%   
%% \end{frame}


% % \begin{frame}
% %   \frametitle{The model}
% % 
% %   A Geographic example: The United States.
% % 
% %   \alertb{Level 1:} The country.
% % 
% %   \alertb{Level 3:} Regions: South, North East, Midwest, West coast, South West, Alaska.
% % 
% %   \alertb{Level 4:} States within regions\\ (New York, Connecticut, Massachusetts,\ldots).
% % 
% %   \alertb{Level 5:} Cities/areas within States\\ (New York city, Boston, the Berkshires).
% % 
% %   \alertb{Level 6:} Suburbs/towns/smaller cities\\ (Brooklyn, Cambridge).
% %   
% %   \alertb{Level 7:} Neighborhoods\\ (the Village, Harvard Square).
% % \end{frame}
% 

 
% \begin{frame}
%   \frametitle{The model}
% 
%   \alertg{P4:}  Each attribute
%   of identity $\equiv$ hierarchy.
% \end{frame}


\begin{frame}
  \frametitle{The model}

  \begin{center}
    \includegraphics[width=\textwidth]{fig01_hierarchy_againD}
  \end{center}

  \begin{center}

    $\vec{v}_i = [ 1 \  1 \ 1 ]^T$, $\vec{v}_j = [ 8 \ 4 \ 1]^T$ \hfill
    Social distance:\\
    \alertb{$x^1_{ij} = 4$, \ $x^2_{ij} = 3$, \ $x^3_{ij} = 1$.}
    \hfill
    $ \boxed{y_{ij} = \min_h x^h_{ij}.} $

  \end{center}

\end{frame}

% \begin{frame}
%   \frametitle{The model}
% 
%   \alertg{P5:}   ``Social distance'' is the minimum distance
%   between two nodes in all hierarchies.
% 
%   $$ \boxed{y_{ij} = \min_h x^h_{ij}.} $$
% 
% \vfill
% 
%   Previous slide:
%   \begin{center}
%     
%     \alertb{$x^1_{ij} = 4$, \ $x^2_{ij} = 3$, \ $x^3_{ij} = 1$.}
% 
%     $\Rightarrow  y_{ij} = 1$.
% 
%   \end{center}
% 
% \end{frame}


\begin{frame}
  \frametitle{The model}

  \begin{block}{Triangle inequality doesn't hold:}
    \begin{center}
      \includegraphics[width=1\textwidth]{fig01_hierarchy_againE}
    \end{center}

    \begin{center}
      \alertg{$y_{ik} = 4 > y_{ij} + y_{jk} = 1 + 1 = 2.$}
    \end{center}
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{The model}

  \begin{block}{}
  \begin{itemize}
  \item<1-> 
    Individuals know the identity
    vectors of
    \begin{enumerate}
    \item<2-> 
      themselves,
    \item<3->  
      their friends,
    \item[]<4->  
      and
    \item<4->  
      the target.
    \end{enumerate}
  \item<5->
    Individuals can estimate the social distance
    between their friends and the target.
  \item<6->
    Use a greedy algorithm + allow searches to fail randomly.
  \end{itemize}
  \end{block}
  
\end{frame}


\begin{frame}
   \frametitle{The model-results---searchable networks}
 
   $\alpha=0$ versus $\alpha=2$ for $N \simeq 10^5$:
   \centering
   \includegraphics[height=0.4\textheight]{figHalphavar02ultp_talk2_noname}%
 \raisebox{12ex}{
   \begin{tabular}{l}
   \alertb{$q \ge r$} \\
   \alertg{$q<r$} \\
   $r= 0.05$
 \end{tabular}}

\begin{block}{}
  $q$ = probability an arbitrary message
  chain reaches a target.

  \begin{itemize}
  \item<1-> A few dimensions help.\\
  \item<1-> Searchability decreases as population increases.\\
  \item<1-> Precise form of hierarchy largely doesn't matter.
  \end{itemize}
\end{block}

\end{frame}

 
\begin{frame}
  \frametitle{The model-results}

  Milgram's Nebraska-Boston data:

  \begin{columns}
    \column{0.6\textwidth}
    \includegraphics[width=\textwidth]{figmilgram_talk_noname}%
    \column{0.4\textwidth}
    \begin{block}{Model parameters:}
      \begin{itemize}
      \item<1->
        $N=10^8$, 
      \item<1->
        $z=300$, $g=100$,
      \item<1->
        $b=10$,  
      \item<1->
        $\alpha=1$, $H=2$; 
      \item[]<1->
      \item<1->
        $\tavg{L_{\textrm{model}}} \simeq 6.7$
      \item<1->
        $L_{\textrm{data}} \simeq 6.5$
      \end{itemize}
    \end{block}
  \end{columns}

\end{frame}

\begin{frame}
  \frametitle{Social search---Data}

  \begin{block}{Adamic and Adar (2003)}
    \begin{itemize}
    \item<1->
      For HP Labs, found probability of connection
      as function of organization distance
      well fit by exponential distribution.
    \item<2->
      Probability of connection as function of
      real distance $\propto 1/r$.
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Social Search---Real world uses}

  \begin{block}{}
  \begin{itemize}
  \item 
  Tags create identities for objects
  \item 
  Website tagging:
  \wordwikilink{http://bitly.com}{bitly.com}
  \item 
  (e.g., Wikipedia)
  \item 
  Photo tagging:
  \wordwikilink{http://www.flickr.com}{flickr.com}
  \item 
  Dynamic creation of metadata
  plus links between information objects.
  \item 
  Folksonomy: collaborative creation of metadata
  \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}
  \frametitle{Social Search---Real world uses}

  \begin{block}{Recommender systems:}
    \begin{itemize}
    \item<1->
      Amazon uses people's actions to build
      effective connections between books.  
    \item<2->
      Conflict between `expert judgments' and\\
      tagging of the hoi polloi.
    \end{itemize}
  \end{block}

  %  Q: Does tagging lead to a flat structure or 
  %  can we identify categories?  (Community detection.)

  % some information scientists decry tagging
  % as poorly directed

\end{frame}

\subsection{Nutshell}

\begin{frame}

  \begin{block}{Nutshell for Small-World Networks:}
    \begin{itemize}
    \item<+->
      Bare networks are typically unsearchable.
    \item<+-> 
      Paths are findable if nodes understand how network is formed.
    \item<+-> 
      Importance of identity (interaction contexts).
    \item<+-> 
      Improved social network models.
    \item<+-> 
      Construction of peer-to-peer networks.
    \item<+-> 
      Construction of searchable information databases.
    \end{itemize}
  \end{block}

\end{frame}

%% Aleksander Gamme
\neuralreboot{vC8gJ0_9o4M}{}{}{Food-induced happiness}

\section{Scale-free\ networks}

\subsection{Main\ story}

\begin{frame}[label=]
 \frametitle{Scale-free networks}
 
 \begin{block}{}
 \begin{itemize}
 \item<1-> 
   Networks with power-law degree distributions
   have become known as \alertg{scale-free} networks.
 \item<2->
   Scale-free refers specifically to the \alertg{degree distribution}
   having a \alertg{power-law decay} in its tail:
   $$
   \uncover<3->{
     P_k \sim k^{-\gamma} 
     \mbox{\ for `large' $k$}
   }
   $$
 \item<4->
   One of the seminal works in complex networks:\\
   Laszlo Barab\'{a}si and Reka Albert, Science, 1999:\\
   \alertb{``Emergence of scaling in random networks''}\cite{barabasi1999a}\\
   \wordwikilink{http://scholar.google.com/citations?view\_op=view\_citation\&hl=en\&user=vsj2slIAAAAJ\&citation\_for\_view=vsj2slIAAAAJ:u5HHmVD\_uO8C}
   {Times cited: \uncover<3->{\alert{$\sim 20,734$}}}
   {\tiny(as of September 23, 2014)}
 \item<5->
   Somewhat misleading nomenclature...
 \end{itemize}
 \end{block}

\end{frame}

\begin{frame}[label=]
  \frametitle{Scale-free networks}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Scale-free networks are \alertg{not fractal} in any sense.
  \item<2->
    Usually talking about networks whose links are
    \alertb{abstract}, 
    \alertg{relational}, 
    \alertb{informational}, 
    \ldots (non-physical)
  \item<3->
    Primary example: hyperlink network of the Web
  \item<4->
    Much arguing about whether or networks are `scale-free' or not\ldots
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[label=]
 \frametitle{Some real data (we are feeling brave):}

 \begin{block}{From Barab\'{a}si and Albert's original paper\cite{barabasi1999a}:}
   \includegraphics[width=\textwidth]{barabasi1999a_fig1}   
 \end{block}
 
\end{frame}

\input{nw_powerlaw_graphviz03giants}

\begin{frame}[label=]
 \frametitle{Scale-free networks}

 \begin{block}<1->{The big deal:}
   \begin{itemize}
   \item<1->
     We move beyond describing networks
     to finding \alertg{mechanisms} for why
     certain networks are the way they are.
   \end{itemize}
 \end{block}

 \begin{block}<2->{A big deal for scale-free networks:}
   \begin{itemize}
   \item<2->
     How does the exponent $\gamma$ depend on the mechanism?
   \item<3->
     Do the mechanism details matter?
   \end{itemize}
 \end{block}
 
\end{frame}

%% \begin{frame}
%%   \frametitle{Models}
%% 
%%   \begin{block}{2. `scale-free networks':}
%%     \begin{itemize}
%%     \item<2-> Introduced by Barabasi and Albert\cite{barabasi1999a}
%%     \item<2-> Generative model
%%     \item<3-> Preferential attachment model with growth:
%%         $P[\textrm{\mbox{attachment to node}\ i] \propto k_{i}^\alpha$.
%%     \item<4-> Produces \alertg{$P_k \sim k^{-3}$} when $\alpha=1$.
%%     \item<5-> Trickiness: other models generate skewed degree distributions.
%%     \end{itemize}
%%   \end{block}
%% 
%% \end{frame}




%% something i completely agree with from socnet (mason porter):
%%

%% The term 'scale free' is a very unfortunate (and misleading) term that was spread based on the terminology choice of some individuals.  (Some people on this mailing
%% list who know me are probably laughing their heads off and expecting me to go into one of my usual rants.  I'll spare you this time.)  It is used to refer to networks
%% with degree distributions given by power laws (or, for real data, have power law tails) because a commonality of these sorts of scaling laws (which is different from
%% scales, by the way, despite the similar terminology) one finds in fractal structures.  The thing is that networks with those degree distributions _can_ have scales,
%% as discussed in gory detail by John Doyle and collaborators.  It's better to just state the power law aspect of it when one's network has that feature and not comment
%% on whether or not there are scales unless one goes beyond the degree distribution.

%% 
%% One of Doyle's papers that discusses this is here: http://arxiv.org/abs/cond-mat/0501169
%% 
%% I hope this helps.


%% \begin{frame}
%%   \frametitle{Heritage}
%% 
%%   \begin{block}{Work that presaged scale-free networks}
%%     \begin{itemize}
%%     \item <1->
%%       1924: \alertg{G. Udny Yule}\cite{yule1924a}:\\ \# Species per Genus
%%     \item <2->
%%       1926: \alertg{Lotka}\cite{lotka1926a}:\\ \# Scientific papers per author
%%     \item <3-> 
%%       1953: \alertg{Mandelbrot}\cite{mandelbrot1953a}):\\
%%       Zipf's law for word frequency through optimization
%%     \item <4->
%%       1955: \alertg{Herbert Simon}\cite{simon1955a,zipf1949a}:\\ Zipf's law, 
%%       city size, income, publications, and species per genus
%%     \item <5->
%%       1965/1976: \alertg{Derek de Solla Price}\cite{price1965a,price1976a}:\\ Network of Scientific Citations
%%     \end{itemize}
%%   \end{block}
%% 
%% \end{frame}

%% \subsection{Model details}

\begin{frame}[label=]
  \frametitle{BA model}
 
  \begin{block}{}
    \begin{itemize}
    \item<1-> 
      Barab\'{a}si-Albert model = BA model.
    \item<2-> 
      Key ingredients:\\
      \alertg{Growth} and \alertb{Preferential Attachment} (PA).
    \item<3-> 
      \alertg{Step 1}: start with $m_0$ disconnected nodes.
    \item<4-> 
      \alertg{Step 2}: 
      \begin{enumerate}
      \item<5->  
        \alertb{Growth}---a new node appears at each time step $t=0,1,2, \ldots$.
      \item<6-> 
        Each new node makes $m$ links to nodes already present.
      \item<7-> 
        \alertb{Preferential attachment}---Probability 
        of connecting to $i$th node is $\propto k_i$.
      \end{enumerate}
    \item<8->
      In essence, we have a \alertg{rich-gets-richer} scheme.
    \item<9->
      Yes, we've seen this all before in Simon's model.
    \end{itemize}
  \end{block}

\end{frame}

%% \subsection{Analysis}

\begin{frame}[label=]
 \frametitle{BA model}  

 \begin{block}{}
   \begin{itemize}
   \item<1->
     \alertg{Definition:} $A_k$ is the \alertb{attachment kernel}
     for a node with degree $k$.
   \item<2->
     For the original model:
     $$ A_k = k$$
   \item<3->
     \alertg{Definition:} $P_{\textrm{attach}}(k,t)$ 
     is the attachment probability.
   \item<4->
     For the original model:
     $$
     P_{\textrm{attach}}(\mbox{node $i$},t)
     =
     \frac{k_i(t)}
     {
       \sum_{j=1}^{N(t)} k_j(t)
     }
     \uncover<6->{
       =
       \frac{k_i(t)}
       {
         \sum_{k=0}^{k_{\textrm{max}}(t)} k N_k(t)
       }
     }
     $$
     \uncover<5->{
       where $N(t) = m_0 + t$ is \# nodes at time $t$\\
     }
     \uncover<7->{
       \ \ and $N_k(t)$ is \# degree $k$ nodes at time $t$.
     }
   \end{itemize}
 \end{block}

\end{frame}

\begin{frame}[label=]
 \frametitle{Approximate analysis}  

 \begin{block}{}
 \begin{itemize}
 \item<1->
   When $(N+1)$th node is added, 
   the expected increase in the degree of node $i$ is 
   $$
   E(k_{i,N+1} - k_{i,N}) 
   \simeq 
   m
   \frac{k_{i,N}}
   {
     \sum_{j=1}^{N(t)} k_j(t)
   }.
   $$
 \item<2->
   Assumes probability of being connected to is \alertg{small}.
 \item<3->
   Dispense with Expectation by assuming (hoping)
   that over longer time frames, degree growth will
   be smooth and stable.
 \item<4->
   Approximate 
   $k_{i,N+1} - k_{i,N}$ with $\diff{}{t} k_{i,t}$:
   \uncover<5->{
     $$
     \alertb{
       \diff{}{t} k_{i,t}
       =
       m
       \frac{k_{i}(t)}
       {
         \sum_{j=1}^{N(t)} k_j(t)
       }
     }
   $$
   where $t = N(t) - m_0$.
   }
 \end{itemize}
 \end{block}


\end{frame}

\begin{frame}[label=]
 \frametitle{Approximate analysis}  

 \begin{block}{}
 \begin{itemize}
 \item<1->
   Deal with denominator: each added node brings $m$ new edges.
   $$
   \uncover<2->{
     \therefore 
     \sum_{j=1}^{N(t)} k_j(t)
     =
     2 t m
   }
   $$
 \item<3->
   The node degree equation now simplifies:
   $$
   \diff{}{t} k_{i,t}
   =
   m
   \frac{k_{i}(t)}
   {
     \sum_{j=1}^{N(t)} k_j(t)
   }
   \uncover<4->{
     =
     m
     \frac{k_{i}(t)}
     {
       2 m t
     }
   }
   \uncover<5->{
     =
     \frac{1}
     {
       2 t
     }
     k_{i}(t)
   }
   $$
 \item<6->
   Rearrange and solve:
   $$
   \frac{\dee{k_{i}(t)}}
   {k_{i}(t)}
   =
   \frac{\dee{t}}
   {2t}
   \uncover<7->{
     \Rightarrow
     \boxed{\alertg{k_i(t) = c_i \,  t^{1/2}.}}
   }
   $$
 \item<8->
   Next find $c_i$ \ldots
 \end{itemize}
 \end{block}

\end{frame}

\begin{frame}[label=]
 \frametitle{Approximate analysis}  

 \begin{block}{}
 \begin{itemize}
 \item<1->
   Know $i$th node appears at time 
   $$
   t_{i,\textrm{start}} = \left\{
     \begin{array}{ll}
       i-m_0 & \mbox{for $i>m_0$} \\
       0 & \mbox{for $i \le m_0$}
     \end{array}
   \right.
   $$
 \item<2->
   So for $i>m_0$ (exclude initial nodes),
   we must have
   $$
   k_i(t) 
   = 
   m 
   \left(
   \frac{t}{t_{i,\textrm{start}}}
   \right)^{1/2}
   \
   \mbox{for $t \ge t_{i,\textrm{start}}$}.
   $$
 \item<3-> 
   All node degrees grow as \alertg{$t^{1/2}$}
   \uncover<4->{but
   later nodes have larger $t_{i,\textrm{start}}$ which
   \alertb{flattens out} growth curve.}
 \item<5-> 
   First-mover advantage: Early nodes do \alertg{best}.
 \item<6-> 
   Clearly, a \wordwikilink{http://en.wikipedia.org/wiki/Ponzi\_scheme}{Ponzi scheme}.
 \end{itemize}
 \end{block}
 
\end{frame}

\begin{frame}[label=]
 \frametitle{Approximate analysis}  

 \begin{block}{}
 \begin{columns}
   \column{0.6\textwidth}
   \includegraphics[width=\textwidth]{figsfn_deggrowth_noname}
   \column{0.4\textwidth}
   \begin{itemize}
   \item $m = 3$
   \item $t_{i,\textrm{start}} = 1, 2, 5, \mbox{\ and\ } 10$.
   \end{itemize}
 \end{columns}
 \end{block}

\end{frame}

\begin{frame}[label=]
 \frametitle{Degree distribution}

 \begin{block}{}
 \begin{itemize}
 \item<1-> 
   So what's the \alertb{degree distribution} at time $t$?
 \item<2-> 
   Use fact that birth time for added nodes
   is distributed uniformly between time 0 and t:
   $$
   \Prob(t_{i,\textrm{start}}) \dee{t_{i,\textrm{start}}}
   \simeq
   \frac{\dee{t_{i,\textrm{start}}}}
   {t}
%%   {t+m_0}
   $$
 \item<3->
   Also use
   $$
   k_i(t) 
   = 
   m 
   \left(
   \frac{t}{t_{i,\textrm{start}}}
   \right)^{1/2}
   \alertg{\Rightarrow}
   t_{i,\textrm{start}}
   = \frac{m^2 t}{k_i(t)^2}.
   $$
   \uncover<4->{
     Transform variables---Jacobian:
     $$
     \diff{t_{i,\textrm{start}}}{k_i}  = -2 \frac{m^2 t}{k_i(t)^3}.
     $$
   }
 \end{itemize}
 \end{block}

\end{frame}

\begin{frame}[label=]
  \frametitle{Degree distribution}

  \begin{block}{}
  \begin{itemize}
  \item<1-> 
    $$
    \Prob(k_i) \dee{k_i} =  \Prob(t_{i,\textrm{start}}) \dee{t_{i,\textrm{start}}}
    $$
  \item<2-> 
    $$
    = \Prob(t_{i,\textrm{start}}) \dee{k_i} \left| \diff{t_{i,\textrm{start}}}{k_i} \right|
    $$
  \item<3-> 
    $$
    = \frac{1}{t} \dee{k_i} 2 \frac{m^2 t}{k_i(t)^3}
    $$
  \item<4-> 
    $$
    = 2 \frac{m^2 }{k_i(t)^3}  \dee{k_i}
    $$
  \item<5-> 
    $$
    \alertg{\propto k_i^{-3}} \dee{k_i}.
    $$
  \end{itemize}
  \end{block}

\end{frame}

%%     =
%%     \Prob(t_{i,\textrm{start}} > \frac{m^2 t}{k^2})
%%     \simeq
%%     \frac{\alertg{t-\frac{m^2 t}{k^2}}}{\alertb{t+m_0}}.
%%     $$
%%   \item<2-> 
%%     Differentiate to find $\Prob(k)$:
%%     $$
%%     \Prob(k)
%%     = 
%%     \diff{}{k} \Prob(k_i < k)
%%     \uncover<3->{
%%       =
%%       \frac{2m^2 t }{(t+m_0) k^3}
%%     }
%%     $$
%%     $$
%%     \uncover<4->{
%%       \sim 
%%       \alertg{2m^2} \alertb{k^{-3}}
%%       \mbox{\ as $m\rightarrow \infty$}.
%%     }
%%     $$
%%   \end{itemize}
%% 
%% \end{frame}

\begin{frame}[label=]
 \frametitle{Degree distribution}

 \begin{block}{}
 \begin{itemize}
  \item<1-> 
    We thus have a very specific prediction of 
    \alertb{$\Prob(k) \sim k^{-\gamma}$} with \alertg{$\gamma=3$}.
  \item<2-> 
    Typical for real networks: \alertg{$2 < \gamma < 3$}.
  \item<3-> 
    Range true more generally for events with size
    distributions that have power-law tails.
  \item<4->
    \alertg{$2 < \gamma < 3$}: finite mean and `infinite' variance
    \uncover<7->{\alertg{(wild)}}
  \item<5->
    In practice, $\gamma < 3$ means variance is governed
    by upper cutoff.
  \item<6->
    \alertg{$\gamma > 3$}: finite mean and variance 
    \uncover<8->{\alertg{(mild)}}
 \end{itemize}
 \end{block}

\end{frame}

% power law tail degree distribution

\begin{frame}[label=]
 \frametitle{Back to that real data:}

 \begin{block}{From Barab\'{a}si and Albert's original paper\cite{barabasi1999a}:}
   \includegraphics[width=\textwidth]{barabasi1999a_fig1}   
 \end{block}
 
\end{frame}

\begin{frame}[label=]
 \frametitle{Examples}  

%  \rowcolors[]{1}{blue!20}{blue!10} 

 \begin{block}{}
   \rowcolors[]{1}{blue!20}{blue!10} 
  \begin{center}
    \begin{tabular}{rl}
      Web & $\gamma \simeq 2.1$ for in-degree \\
      Web & $\gamma \simeq 2.45$ for out-degree \\
      Movie actors & $\gamma \simeq 2.3$ \\
      Words (synonyms) & $\gamma \simeq 2.8$ \\
    \end{tabular}
  \end{center}

  \uncover<2->{
    The Internet\alertg{s} is a different business...
  }
 \end{block}

\end{frame}


\begin{frame}[label=]
 \frametitle{Things to do and questions}  

 \begin{block}{}
 \begin{itemize}
 \item<1-> Vary attachment kernel.
 \item<1-> Vary mechanisms:
   \begin{enumerate}
   \item<1-> Add edge deletion
   \item<1-> Add node deletion
   \item<1-> Add edge rewiring
   \end{enumerate}
 \item<+-> Deal with directed versus undirected networks.
 \item<+-> \alertg{Important Q.}:
   Are there distinct universality classes for these networks?  
 \item<+->
   \alertg{Q.}:
   How does changing the model affect $\gamma$?
 \item<+->
   \alertg{Q.}:
   Do we need preferential attachment and growth?
 \item<+->
   \alertg{Q.}:
   Do model details matter?
   \uncover<+->{Maybe \ldots}
 \end{itemize}
 \end{block}

\end{frame}

\subsection{A\ more\ plausible\ mechanism}

\begin{frame}[label=]
 \frametitle{Preferential attachment}  

 \begin{block}{}
 \begin{itemize}
 \item<1-> 
   Let's look at preferential attachment \alertg{(PA)}
   a little more closely.
 \item<2-> 
   PA implies arriving nodes have \alertg{complete knowledge}
   of the existing network's degree distribution.
 \item<3->
   For example: If \alertb{$P_{\textrm{attach}}(k) \propto k$}, we need to determine
   the constant of proportionality.
 \item<4->
   We need to know what everyone's degree is...
 \item<5->
   PA is $\therefore$ an \alertb{outrageous} assumption of
   node capability.
 \item<6->
   But a \alertg{very simple mechanism} saves the day\ldots
 \end{itemize}
 \end{block}

\end{frame}

\begin{frame}[label=]
 \frametitle{Preferential attachment through randomness}  

 \begin{block}{}
 \begin{itemize}
 \item<1-> Instead of attaching preferentially, allow
   new nodes to attach randomly.
 \item<2-> Now add an \alertg{extra step}: new nodes
   then connect to some of their friends' friends.
 \item<3-> Can also do this \alertg{at random}.
 \item<4-> Assuming the existing network is random,
   we know probability of a \alertg{random friend} having
   degree \alertb{$k$} is 
   $$Q_k \propto kP_k$$
 \item<5-> 
   So \alertg{rich-gets-richer} scheme can
   now be seen to work in a natural way.
 \item<6-> 
   Later: we'll see that the nature of $Q_{k}$
   means your friends have more friends than you.
   \uncover<7->{\alertg{\#disappointing}}
 \end{itemize}
 \end{block}

\end{frame}

\subsection{Robustness}

\begin{frame}[label=]
 \frametitle{Robustness}  

 \begin{block}{}
 \begin{itemize}
 \item
   Albert et al., Nature, 2000:\\
   \alertb{``Error and attack tolerance of complex networks''}\cite{albert2000a}
 \item 
   \alertb{Standard random networks} (\erdosrenyi)\\
   versus \alertb{Scale-free networks}:
 \end{itemize}

 \includegraphics[width=\textwidth]{albert2000a_fig1-tp-3}\\
 {\tiny from Albert et al., 2000}
 \end{block}

\end{frame}

\begin{frame}[label=]
 \frametitle{Robustness}  

 \begin{block}{}
 \begin{columns}
   \column{0.5\textwidth}
   \includegraphics[width=\textwidth]{albert2000a_fig2}\\
   {\tiny from Albert et al., 2000}
   \column{0.5\textwidth}
   \begin{itemize}
   \item 
     Plots of network diameter as a function of fraction of nodes removed
   \item 
     \erdosrenyi\ versus scale-free networks
   \item
   \alertb{blue symbols} = \\
   {random} removal
   \item
   \alertg{red symbols} = \\
   {targeted} removal \\
   (most connected first)
   \end{itemize}

 \end{columns}
 \end{block}

\end{frame}


\begin{frame}[label=]
  \frametitle{Robustness}  

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Scale-free networks are thus \alertb{robust to random failures}
    yet \alertg{fragile to targeted ones}.
  \item<2-> 
    All very reasonable: \alertb{Hubs} are a big deal.
  \item<3-> 
    \alertg{But}: next issue is whether hubs are vulnerable or not.
  \item<4-> 
    Representing all webpages as the same size node is obviously
    a stretch (e.g., google vs. a random person's webpage)
  \item<5-> 
    Most connected nodes are either:
    \begin{enumerate}
    \item<6->
      Physically larger nodes that may be harder to `target'
    \item<7->
      or subnetworks of smaller, normal-sized nodes.
    \end{enumerate}
  \item<8-> 
    Need to explore cost of various targeting schemes.
  \end{itemize}
  \end{block}
  
\end{frame}


%% {``These go to eleven'':}
%% \neuralreboot{NrVCjnRdB_k}{}{}{A different, better way?}

\subsection{Krapivisky\ \&\ Redner's\ model}

%% \subsection{Generalized model}

\begin{frame}[label=]
  \frametitle{Generalized model}  

  \begin{block}<1->{Fooling with the mechanism:}
    \begin{itemize}
    \item<1->
      2001: Krapivsky \& Redner (KR)\cite{krapivsky2001a}
      explored the \alertr{general attachment kernel}:
      \uncover<2->{
      $$
      \Prob(\mbox{attach to node $i$}) \propto A_k  = k_{i}^\nu
      $$
      where $A_k$ is the attachment kernel and $\nu>0$.
      }
    \item<3->
      KR also looked at changing the details of
      the attachment kernel.
    \item<4->
      We'll follow KR's approach using
      \wordwikilink{http://en.wikipedia.org/wiki/Rate_equation}{rate equations}.
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Generalized model}

  \begin{block}{}
    \begin{itemize}
    \item<+->
      Here's the set up:
      $$
      \diff{N_k}{t}
      =
      \frac{1}{A}
      \left[
        A_{k-1} N_{k-1} - A_{k} N_{k}
      \right]
      + \delta_{k1}
      $$
      where $N_k$ is the number of nodes of degree $k$.
      \begin{enumerate}
      \item<+->
        One node with one link is added per unit time.
      \item<+-> 
        The \alertb{first term} corresponds to 
        degree $k-1$ nodes becoming degree $k$ nodes.
      \item<+-> 
        The \alertb{second term} corresponds to 
        degree $k$ nodes becoming degree $k-1$ nodes.
      \item<+->
        $A$ is the correct normalization (coming up).
      \item<+-> 
        Seed with some initial network\\
        \uncover<+->{(e.g., a connected pair)}
      \item<+-> 
        Detail: $A_0=0$
      \end{enumerate}
    \end{itemize}
  \end{block}

\end{frame}

%% \subsection{Analysis}

\begin{frame}[label=]
  \frametitle{Generalized model}  
  
  \begin{block}{}
  \begin{itemize}
  \item<1->
    In general, probability of attaching to a \alertr{specific node}
    of degree $k$ at time $t$ is
    \uncover<2->{
    $$
    \Prob(\mbox{attach to node $i$}) 
    = 
    \frac{A_k}{A(t)}
    $$
    }
    \uncover<3->{
    where
    $ A(t) = \sum_{k=1}^{\infty} A_k N_k(t).$
    }
  \item<4->
    E.g., for BA model, $A_k = k$ and $A = \sum_{k=1}^{\infty} k N_k(t)$.
  \item<5-> For $A_k=k$, we have
    $$
    \uncover<6->{
    A(t) 
    = 
    \sum_{k'=1}^{\infty}
    k' N_{k'}(t)
    }
    \uncover<7->{
    = 2t     
    }
    $$
    \uncover<8->{
      since one edge is being added per unit time.
    }
  \item<9->
      Detail: we are ignoring initial seed network's edges.
  \end{itemize}
  \end{block}

\end{frame}


\begin{frame}[label=]
  \frametitle{Generalized model}  

  \begin{block}{}
  \begin{itemize}
  \item<1->
    So now
    $$
    \diff{N_k}{t}
    =
    \frac{1}{A}
    \left[
      A_{k-1} N_{k-1} - A_{k} N_{k}
    \right]
    + \delta_{k1}
    $$
    becomes
    $$
    \diff{N_k}{t}
    =
    \frac{1}{2t}
    \left[
      (k-1) N_{k-1} - k N_{k}
    \right]
    + \delta_{k1}
    $$
  \item<2->
    As for BA method, look for steady-state growing solution:
    \uncover<3->{
      $
      \alertr{N_k = n_k t}.
      $
    }
  \item<4->
    We replace $\tdiff{N_k}{t}$ with $\tdiff{n_k t}{t} = n_k$.
  \item<5->
    We arrive at a difference equation:
    $$
    n_k
    =
    \frac{1}{2\cancel{\alertr{t}}}
    \left[
      (k-1) n_{k-1}\cancel{\alertr{t}} - k n_{k}\cancel{\alertr{t}}
    \right]
    + \delta_{k1}
    $$
  \end{itemize}
  \end{block}

\end{frame}


%% \subsection{Universality?}

\begin{frame}
  \frametitle{Universality?}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    \insertassignmentquestionsoft{08}{8}\\
    As expected, we have the same result as for the BA model:
    $$
    N_k(t) = n_k(t) t \propto k^{-3} t \mbox{\ for large $k$}.
    $$
  \item<2->
    Now: what happens if we start playing around with 
    the attachment kernel $A_k$?
  \item<3->
    Again, we're asking if the result $\gamma=3$
    \wordwikilink{http://en.wikipedia.org/w/index.php?title=Universality_\%28dynamical_systems\%29&oldid=204738455}{universal}?
  \item<4->
    KR's natural modification: $A_k = k^\nu$ with $\nu \ne 1$.
  \item<5->
    But we'll first explore a more subtle modification of $A_k$ made by Krapivsky/Redner\cite{krapivsky2001a}
  \item<6->
    Keep $A_k$ \alertb{linear in $k$} but tweak details.
  \item<7->
    \alertr{Idea:} Relax from $A_k = k$ to $A_k \sim k$ as $k \rightarrow \infty$.
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Universality?}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Recall we used the normalization:
    $$
    A(t) 
    = 
    \sum_{k'=1}^{\infty}
    k' N_{k'}(t)
    \simeq 2t
    \mbox{\ for large $t$.}
    $$
  \item<2->
    We now have 
    $$
    A(t) 
    = 
    \sum_{k'=1}^{\infty}
    A_{k'} N_{k'}(t)
    $$
    \uncover<3->{
    where we only know the asymptotic behavior of $A_k$.
    }
  \item<4->
    We assume that \alertr{$A = \mu t$}
  \item<5->
    We'll find $\mu$ later and make sure that our
    assumption is consistent.
  \item<6->
    As before, also assume $N_k(t) = n_k t$.
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Universality?}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    For $A_k = k$ we had 
    $$
    n_k
    =
    \frac{1}{2}
    \left[
      (k-1) n_{k-1} - k n_{k}
    \right]
    + \delta_{k1}
    $$
  \item<2->
    This now becomes
        $$
        n_k
    =
    \frac{1}{\mu}
    \left[
      A_{k-1} n_{k-1} - A_k n_{k}
    \right]
    + \delta_{k1}
    $$
    \uncover<3->{
      $$
      \Rightarrow
      (A_k+\mu)n_k
      =
      A_{k-1} n_{k-1} 
      + \mu \delta_{k1}
      $$
    }
  \item<4->
    Again two cases:
    $$
    \uncover<4->{
      \alertr{k=1:}
      n_1 = \frac{\mu}{\mu+A_1};
    }
    \qquad
    \uncover<5->{
      \alertr{k>1:}
      n_k
      =
      n_{k-1}\frac{A_{k-1}}{\mu + A_k}.
    }
    $$

  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Universality?}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Time for pure excitement: Find \alertr{asymptotic behavior} 
    of $n_k$ given $A_k \rightarrow k$ as $k \rightarrow \infty$.
  \item<2->
    \insertassignmentquestionsoft{08}{8}\\
    For large $k$, we find:
    $$
    n_k = 
    \frac{\mu}{A_k}
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    \propto
    \alertr{ k^{-\mu-1}}
    $$
  \item<3->
    Since $\mu$ depends on $A_k$, \alertr{details matter...}
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Universality?}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Now we need to find $\mu$.
  \item<2->
    Our assumption again:
    $
    \alertb{A = \mu t = \sum_{k=1}^{\infty} N_k(t) A_k}
    $
  \item<3->
    Since $N_k = n_k t$, we have the simplification
    $
    \alertb{
    \mu = \sum_{k=1}^{\infty} n_k A_k}
    $
  \item<4->
    Now subsitute in our expression for $n_k$:
    \begin{overprint}
      \onslide<5| handout: 0 | trans: 0>
    $$
    \mu = 
    \sum_{k=1}^{\infty} 
    \alertr{
      \frac{\mu}{A_k}
      \prod_{j=1}^{k}
      \frac{1}{1 + \frac{\mu}{A_j}}
    }
    A_k
    $$
    \onslide<6| handout: 0 | trans: 0>
    $$
    \mu = 
    \sum_{k=1}^{\infty} 
    \frac{\mu}{\alertr{\cancel{A_k}}}
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    \alertr{\cancel{A_k}}
    $$
    \onslide<7-| handout: 1 | trans: 1>      
    $$
    \alertb{1 \cancel{\mu}} = 
    \sum_{k=1}^{\infty} 
    \frac{\alertb{\cancel{\mu}}}{\alertr{\cancel{A_k}}}
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    \alertr{\cancel{A_k}}
    $$
    \end{overprint}
    \item<8->
      Closed form expression for $\mu$.
    \item<9->
      We can solve for $\mu$ in some cases.
    \item<10->
      Our assumption that $A = \mu t$ looks to be not too horrible.
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Universality?}

  \begin{block}{}
    \begin{itemize}
    \item<+->
      Consider tunable $A_{1} = \alpha$ and $A_{k} = k$ for $k \ge 2$.
    \item<+-> 
      Again, we can find $\gamma = \mu + 1$ by finding $\mu$.
    \item<+->
      \insertassignmentquestionsoft{08}{8}\\
      Closed form expression for $\mu$: 
      $$
      \frac{\mu}{\alpha}
      =
      \sum_{k=2}^{\infty} 
      \frac{\Gamma(k+1)\Gamma(2+\mu)}{\Gamma(k+\mu+1)}
      $$
      \alertg{\#mathisfun}
    \item<+->
      $$
      \mu(\mu-1) = 2\alpha \Rightarrow \mu = \frac{1+\sqrt{1+8\alpha}}{2}.
      $$
    \item<+-> 
      Since $\gamma = \mu+1$, we have
      $$
      0 \le \alpha < \infty \Rightarrow 2 \le \gamma < \infty
      $$
    \item<+-> 
      Craziness...    
    \end{itemize}
  \end{block}

\end{frame}

%% \subsection{Sublinear attachment kernels}

\begin{frame}
  \frametitle{Sublinear attachment kernels}

  \begin{block}{}
  \begin{itemize}
  \item<1-> 
    Rich-get-somewhat-richer:
    $$
    A_k \sim k^\nu
    \mbox{\ with $0 < \nu < 1$.}
    $$
  \item<2->
    General finding by Krapivsky and Redner:\cite{krapivsky2001a}
    $$
    n_k 
    \sim 
    k^{-\nu}
    e^{-c_1 k^{1-\nu} + \mbox{\scriptsize correction terms}}.
    $$
  \item<3->
    Stretched exponentials (truncated power laws).
  \item<4->
    aka Weibull distributions.
  \item<5->
    \alertr{Universality}: now details of kernel \alertb{do not} matter.
  \item<6->
    Distribution of degree is universal providing $\nu<1$.
  \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Sublinear attachment kernels}

  \begin{block}{Details:}
    \begin{itemize}
    \item<1->
      For $1/2 < \nu < 1$:
      $$
      n_k \sim k^{-\nu} e^{-\mu \left(\frac{k^{1-\nu} - 2^{1-\nu}}{1-\nu} \right)}
      $$
    \item<2->
      For $1/3 < \nu < 1/2$:
      $$
      n_k \sim k^{-\nu} e^{-\mu \frac{k^{1-\nu}}{1-\nu} + \frac{\mu^2}{2}\frac{k^{1-2\nu}}{1-2\nu}}
      $$
    \item<3->
      And for $1/(r+1) < \nu < 1/r$, we have $r$ pieces in exponential.
    \end{itemize}
  \end{block}

\end{frame}

%% \subsection{Superlinear attachment kernels}

\begin{frame}
  \frametitle{Superlinear attachment kernels}

  \begin{block}{}
  \begin{itemize}
  \item<1->
    Rich-get-much-richer:
    $$
    A_k \sim k^\nu
    \mbox{\ with $\nu > 1$.}
    $$
  \item<2->
    Now a \alertr{winner-take-all} mechanism.
  \item<3->
    One single node ends up being connected to
    almost all other nodes.
  \item<4->
    For $\nu>2$, all but a finite \# of nodes connect
    to one node.
  \end{itemize}
  \end{block}
  
\end{frame}

%% todo

%% cites for exponents

%% more exponents

%% history

%% \section{Appendix}


\subsection{Nutshell}


\begin{frame}[label=]
  \frametitle{Nutshell:}

  \begin{block}{Overview Key Points for Models of Networks:}
    \begin{itemize}
    \item<1->
      Obvious connections with the vast
      extant field of graph theory.
    \item<2->
      But focus on dynamics is more of a physics/stat-mech/comp-sci
      flavor.
    \item<3->
      Two main areas of focus:
      \begin{enumerate}
      \item 
        \alertb{Description:} Characterizing very large networks
      \item
        \alertb{Explanation:} Micro story $\Rightarrow$ Macro features
      \end{enumerate}
    \item<4->
      Some essential structural aspects are understood: degree distribution, clustering,
      assortativity, group structure, overall structure,...
    \item<5->
      Still much work to be done, especially with respect to dynamics...
      \uncover<6->{\alertg{\#excitement}}
    \end{itemize}
    
  \end{block}

\end{frame}

%% hand feeding a friendly platypus
\neuralreboot{a6QHzIJO5a8}{}{}{Monotrematic Love}

% biham paper, 1998
% generic emergence of power laws

% \begin{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% very clear summary slide 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \textbf{Definitions}

General observation:

Systems (complex or not) \\
that cross many spatial and temporal scales\\
often exhibit some form of \tc{blue}{scaling}.


  \textbf{Outline}

  {Scaling---A plenitude of power laws:}

  \inv

  \tc{white}{\ding{182}} Allometry

  \tc{white}{\ding{183}} Size distributions

  \tc{white}{\ding{184}} Critical phenomena {\small (statistical mechanics)}

  \tc{white}{\ding{185}} Geometry {\small (okay, fractals...)}

  \tc{white}{\ding{186}} Solutions to Equations


  \textbf{Outline}

  {Scaling---A plenitude of power laws:}

  \ding{182} Allometry

  \inv

  \tc{white}{\ding{183}} Size distributions

  \tc{white}{\ding{184}} Critical phenomena {\small (statistical mechanics)}

  \tc{white}{\ding{185}} Geometry {\small (okay, fractals...)}

  \tc{white}{\ding{186}} Solutions to Equations


  \textbf{Outline}

  {Scaling---A plenitude of power laws:}

  \ding{182} Allometry

  \ding{183} Size distributions

  \inv

  \tc{white}{\ding{184}} Critical phenomena {\small (statistical mechanics)}

  \tc{white}{\ding{185}} Geometry {\small (okay, fractals...)}

  \tc{white}{\ding{186}} Solutions to Equations


  \textbf{Outline}

  {Scaling---A plenitude of power laws:}

  \ding{182} Allometry

  \ding{183} Size distributions

  \ding{184} Critical phenomena {\small (statistical mechanics)}

  \inv

  \tc{white}{\ding{185}} Geometry {\small (okay, fractals...)}


  \tc{white}{\ding{186}} Solutions to Equations


  \textbf{Outline}

  {Scaling---A plenitude of power laws:}

  \ding{182} Allometry

  \ding{183} Size distributions

  \ding{184} Critical phenomena {\small (statistical mechanics)}

  \ding{185} Geometry {\small (okay, fractals...)}

  \inv

  \tc{white}{\ding{186}} Solutions to Equations


  \textbf{Outline}

  {Scaling---A plenitude of power laws:}

  \ding{182} Allometry

  \ding{183} Size distributions

  \ding{184} Critical phenomena {\small (statistical mechanics)}

  \ding{185} Geometry {\small (okay, fractals...)}

  \ding{186} Solutions to Equations


  \textbf{Outline}

\tc{blue}{All about scaling:}

1. Definitions

2. Examples

3. How to measure your power-law relationship

4. Mechanisms giving rise to your power-laws 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% definitions and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


  \textbf{Definitions}

A \tc{blue}{power law} relates two
variables $x$ and $y$ as follows:
{\Large
$$ y = c x^\alpha $$
}

\inv

$\alpha$ is the \tc{red}{scaling exponent} (or just exponent)

($\alpha$ can be any number in principle but we will
find various restrictions.)

$c$ is the \tc{red}{prefactor} (which can be important)


  \textbf{Definitions}

A \tc{blue}{power law} relates two
variables $x$ and $y$ as follows:
{\Large
$$ y = c x^\alpha $$
}

$\alpha$ is the \tc{red}{scaling exponent} (or just exponent)

\inv 

($\alpha$ can be any number in principle but we will
find various restrictions.)

$c$ is the \tc{red}{prefactor} (which can be important!)


  \textbf{Definitions}

A \tc{blue}{power law} relates two
variables $x$ and $y$ as follows:
{\Large
$$ y = c x^\alpha $$
}

$\alpha$ is the \tc{red}{scaling exponent} (or just exponent)

($\alpha$ can be any number in principle but we will
find various restrictions.)

\inv

$c$ is the \tc{red}{prefactor} (which can be important!)


  \textbf{Definitions}

A \tc{blue}{power law} relates two
variables $x$ and $y$ as follows:
{\Large
$$ y = c x^\alpha $$
}

$\alpha$ is the \tc{red}{scaling exponent} (or just exponent)

($\alpha$ can be any number in principle but we will
find various restrictions.)

$c$ is the \tc{red}{prefactor} (which can be important!)


  \textbf{Definitions}

The \tc{blue}{prefactor} $c$ must \tc{blue}{balance dimensions}.

\inv

Ex. length $l$ and volume $v$ of common nails
are related as:\
$$l = c v^{1/4}$$

Using $[ \cdot ]$ to indicate dimension, then 
$$[c] = [l]/[V^{1/4}] = L/L^{3/4} = L^{1/4}.$$


  \textbf{Definitions}

The \tc{blue}{prefactor} $c$ must \tc{blue}{balance dimensions}.

Ex. length $l$ and volume $v$ of common nails
are related as:\
$$l = c v^{1/4}$$

\inv

Using $[ \cdot ]$ to indicate dimension, then 
$$[c] = [l]/[V^{1/4}] = L/L^{3/4} = L^{1/4}.$$


  \textbf{Definitions}

The \tc{blue}{prefactor} $c$ must \tc{blue}{balance dimensions}.

Ex. length $l$ and volume $v$ of common nails
are related as:\
$$l = c v^{1/4}$$

Using $[ \cdot ]$ to indicate dimension, then 
$$[c] = [l]/[V^{1/4}] = L/L^{3/4} = L^{1/4}.$$


  \textbf{Looking at data}

Power-law relationships
are linear in log-log space:
$$y = c x^\alpha $$
$$ \Rightarrow \log_{b} y = \alpha \log_{b} x + \log_{b} c$$

with slope equal to $\alpha$, the scaling exponent.

\tc{white}{\ding{43} Lots of double-logarithmic plots.}

\tc{white}{\ding{43} Good practice: Always, always, always use base 10.}


  \textbf{Looking at data}

Power-law relationships
are linear in log-log space:
$$y = c x^\alpha $$
$$ \Rightarrow \log_{b} y = \alpha \log_{b} x + \log_{b} c$$

with slope equal to $\alpha$, the scaling exponent.

\ding{43} Lots of \tc{blue}{double-logarithmic plots}.

\tc{white}{\ding{43} Good practice: Always, always, always use base 10.}


  \textbf{Looking at data}

Power-law relationships
are linear in log-log space:
$$y = c x^\alpha $$
$$ \Rightarrow \log_{b} y = \alpha \log_{b} x + \log_{b} c$$

with slope equal to $\alpha$, the scaling exponent.

\ding{43} Lots of double-logarithmic plots.

\ding{43} Good practice: \tc{blue}{Always, always, always use base 10.}


  \textbf{An excellent example}

\includegraphics[height=0.82\textheight]{zhang2000fig2.jpg}  
 \raisebox{17ex}{
    \parbox{.3\textwidth}{
      \tc{blue}{$\alpha \simeq 1.23$}

      {      
        \small
        gray matter:\\ `computing elements'
        
        white matter:\\ `wiring'

      }
    }
  }

\hfill{\tiny(from Zhang \& Sejnowski, PNAS, 2000)}


  \textbf{Good scaling}

General rules of thumb:

\textit{High quality:}\\ scaling persists 
over \tc{blue}{three or more orders of magnitude}\\
for \tc{red}{each variable}.

\inv

\textit{Medium quality:}\\ scaling persists
over \tc{blue}{three or more orders of magnitude}\\
for \tc{red}{only one variable} and 
\tc{blue}{at least one} for \tc{red}{the other}.

\textit{Very dubious:}\\ scaling `persists'
over \tc{blue}{less than an order of magnitude}\\
for \tc{red}{both variables}.


  \textbf{Good scaling}

General rules of thumb:

\textit{High quality:}\\ scaling persists 
over \tc{blue}{three or more orders of magnitude}\\
for \tc{red}{each variable}.

\textit{Medium quality:}\\ scaling persists
over \tc{blue}{three or more orders of magnitude}\\
for \tc{red}{only one variable} and 
\tc{blue}{at least one} for \tc{red}{the other}.

\inv

\textit{Very dubious:}\\ scaling `persists'
over \tc{blue}{less than an order of magnitude}\\
for \tc{red}{both variables}.



  \textbf{Good scaling}

General rules of thumb:

\textit{High quality:}\\ scaling persists 
over \tc{blue}{three or more orders of magnitude}\\
for \tc{red}{each variable}.

\textit{Medium quality:}\\ scaling persists
over \tc{blue}{three or more orders of magnitude}\\
for \tc{red}{only one variable} and 
\tc{blue}{at least one} for \tc{red}{the other}.

\textit{Very dubious:}\\ scaling `persists'
over \tc{blue}{less than an order of magnitude}\\
for \tc{red}{both variables}.




  \textbf{Bad scaling}

Average walking speed as a function of city population:

\includegraphics[width=0.4\textwidth]{bettencourt2007a_fig.pdf}
\raisebox{10ex}{
  \parbox{0.4\textwidth}{
    Two problems: \\
    \tc{blue}{(1)} use of natural log, and \\
    \tc{blue}{(2)} minute varation in\\ \mbox{} \quad dependent variable.
  }
}

{\small from Bettencourt et al., 2007; otherwise interesting...}


  \textbf{Definitions}

Power laws are the signature of \tc{red}{scale invariance}:

Scale invariant \tc{blue}{`objects'}\\
look the \tc{blue}{`same'}\\ 
when they are 
appropriately \tc{blue}{rescaled.}

\inv
{\small $\bullet$ \tc{blue}{Objects} = geometric shapes, time series, functions, relationships, distributions,...}

{\small $\bullet$ \tc{blue}{`Same'} might be \tc{blue}{`statistically the same'}}

{\small $\bullet$ To \tc{blue}{rescale} means to change the units
of measurement for the relevant variables}


  \textbf{Definitions}

Power laws are the signature of \tc{red}{scale invariance}:

Scale invariant \tc{blue}{`objects'}\\
look the \tc{blue}{`same'}\\ 
when they are 
appropriately \tc{blue}{rescaled.}

{\small $\bullet$ \tc{blue}{Objects} = geometric shapes, time series, functions, relationships, distributions,...}

\inv
{\small $\bullet$ \tc{blue}{`Same'} might be \tc{blue}{`statistically the same'}}

{\small $\bullet$ To \tc{blue}{rescale} means to change the units
of measurement for the relevant variables}


  \textbf{Definitions}

Power laws are the signature of \tc{red}{scale invariance}:

Scale invariant \tc{blue}{`objects'}\\
look the \tc{blue}{`same'}\\ 
when they are 
appropriately \tc{blue}{rescaled.}

{\small $\bullet$ \tc{blue}{Objects} = geometric shapes, time series, functions, relationships, distributions,...}

{\small $\bullet$ \tc{blue}{`Same'} might be \tc{blue}{`statistically the same'}}

\inv
{\small $\bullet$ To \tc{blue}{rescale} means to change the units
of measurement for the relevant variables}


  \textbf{Definitions}

Power laws are the signature of \tc{red}{scale invariance}:

Scale invariant \tc{blue}{`objects'}\\
look the \tc{blue}{`same'}\\ 
when they are 
appropriately \tc{blue}{rescaled.}

{\small $\bullet$ \tc{blue}{Objects} = geometric shapes, time series, functions, relationships, distributions,...}

{\small $\bullet$ \tc{blue}{`Same'} might be \tc{blue}{`statistically the same'}}

{\small $\bullet$ To \tc{blue}{rescale} means to change the units
of measurement for the relevant variables}



  \textbf{Scale invariance}

  \textbf{Our friend \tc{blue}{$y=cx^{\alpha}$}:}
    
    
    If we rescale $x$ as $x = rx'$ and $y$ as $y' = r^\alpha y$,
    
    then
    $$r^\alpha y' = c (rx')^{\alpha}$$
    
    $$\Rightarrow y' = c r^{\alpha} {x'}^{\alpha}r^{-\alpha}$$
    
    $$\Rightarrow y' = c {x'}^{\alpha}$$
    
  


  \textbf{Scale invariance}
Compare with \tc{blue}{$y=c e^{-\lambda x}$}:

\inv
If we rescale $x$ as $x = rx'$, then
$$ y = c e^{-\lambda rx'} $$

Original form cannot be recovered.

$\Rightarrow$ scale matters for the exponential.


  \textbf{Scale invariance}

Compare with \tc{blue}{$y=c e^{-\lambda x}$}:

If we rescale $x$ as $x = rx'$, then
$$ y = c e^{-\lambda rx'} $$

\inv
Original form cannot be recovered.

$\Rightarrow$ scale matters for the exponential.


  \textbf{Scale invariance}

Compare with \tc{blue}{$y=c e^{-\lambda x}$}:

If we rescale $x$ as $x = rx'$, then
$$ y = c e^{-\lambda rx'} $$

Original form cannot be recovered.

\inv

$\Rightarrow$ scale matters for the exponential.


  \textbf{Scale invariance}

Compare with \tc{blue}{$y=c e^{-\lambda x}$}:

If we rescale $x$ as $x = rx'$, then
$$ y = c e^{-\lambda rx'} $$

Original form cannot be recovered.

$\Rightarrow$ scale matters for the exponential.


  \textbf{Scale invariance}

More on \tc{blue}{$y=c e^{-\lambda x}$}:

\inv

Say $x_0 = 1/\lambda$ is the \tc{blue}{characteristic scale}.

For $x \gg x_0$, $y$ is small,\\
while for $x \ll x_0$, $y$ is large.

$\Rightarrow$ More on this later with size distributions.


  \textbf{Scale invariance}

More on \tc{blue}{$y=c e^{-\lambda x}$}:

Say $x_0 = 1/\lambda$ is the \tc{blue}{characteristic scale}.

\inv 

For $x \gg x_0$, $y$ is small,\\
while for $x \ll x_0$, $y$ is large.

$\Rightarrow$ More on this later with size distributions.


  \textbf{Scale invariance}

More on \tc{blue}{$y=c e^{-\lambda x}$}:

Say $x_0 = 1/\lambda$ is the \tc{blue}{characteristic scale}.

For $x \gg x_0$, $y$ is small,\\
while for $x \ll x_0$, $y$ is large.

\inv

$\Rightarrow$ More on this later with size distributions.


  \textbf{Scale invariance}

More on \tc{blue}{$y=c e^{-\lambda x}$}:

Say $x_0 = 1/\lambda$ is the \tc{blue}{characteristic scale}.

For $x \gg x_0$, $y$ is small,\\
while for $x \ll x_0$, $y$ is large.

$\Rightarrow$ More on this later with size distributions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% allometry
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \newslide{Definitions}
%% 
%% Allometry 
%% \hfill
%% \includegraphics[width=.07\textwidth]{wikipedia.jpg}\\
%% 
%% [refers to] differential growth rates of the parts \\
%% of a living organism's body part or process.


  \textbf{Definitions}

\tc{blue}{Isometry:} dimensions scale linearly with each other.

\begin{center}
\includegraphics[height=0.3\textheight]{iso_tree.jpg}
\hfill
\includegraphics[height=0.3\textheight]{allo_tree.jpg}
\end{center}

\mbox{} \hfill \tc{blue}{Allometry:} dimensions scale nonlinearly.


  \textbf{Definitions}

Isometry = `same measure'

Allometry = `other measure'

\inv

Confusingly, we use allometry to mean both\\
\tc{blue}{(1)} nonlinear scaling (e.g., $x \propto y^{1/3}$)\\
and\\
\tc{blue}{(2)} the relative scaling of different measures\\
(e.g., resting heart rate as a function of body size)


  \textbf{Definitions}

Isometry = `same measure'

Allometry = `other measure'

Confusingly, we use allometry to mean both\\
\tc{blue}{(1)} nonlinear scaling (e.g., $x \propto y^{1/3}$)\\
and\\
\tc{blue}{(2)} the relative scaling of different measures\\
(e.g., resting heart rate as a function of body size)


  \textbf{Examples}

\tc{blue}{Allometric scaling abounds in nature:}

% http://www.nature.com/news/2006/060703/full/060703-17.html

Weight lifting: Weight of world record lift  $\propto M^{2/3}.$

Nails: $l \propto M^{1/4}$ and $r \propto M^{3/8}$.

Rowing: Speed $\propto $ (number of rowers)$^{1/9}$.

{\tiny (see ``On Size and Life'' by McMahon and Bonner)}


  \textbf{Examples}

Species-area law: $N_{\textrm{species} \propto A^{\beta}$

Allegedly (data is messy): 

On islands: $\beta \approx 1/4$.

On continuous land: $\beta \approx 1/8$.


  \textbf{Examples}

  Average lifespan $\propto M^{\beta}$

  Average heart rate $\propto M^{-\beta}$

  

  Average number of heart beats in a lifespan
  \begin{center}
  $\simeq$ (Average lifespan) $\times$ (Average heart rate)

  $ \propto M^{\beta - \beta} $

  $ \propto M^{0} $
  \end{center}

  


  \textbf{Examples}

  Average lifespan $\propto M^{\beta}$

  Average heart rate $\propto M^{-\beta}$

  Average number of heart beats in a lifespan
  \begin{center}
  $\simeq$ (Average lifespan) $\times$ (Average heart rate)

  $ \propto M^{\beta - \beta} $

  $ = $ constant.
  \end{center}


  \textbf{Next Tuesday}

Next Tuesday's lecture will
be supplanted by this one:

V.S. Ramachandran

\tc{blue}{``The Neurology of Human Nature''}

\tc{red}{Tuesday, September 18, 4:00 PM\\
Sugar Maple Room, Davis Center}



  \textbf{Examples}

One of the most important constraints in biology and ecology:
  $$B = \mbox{basal metabolic rate}$$
  $$M = \mbox{organismal body mass}$$
  $$B = c M^\alpha$$

%%  Mammals, poikilotherms, birds, trees,\\
%%  bacteria, rocks,\ldots\\

  \includegraphics[height=0.3\textheight]{shrew.jpg}
  \hfill
  {
    \includegraphics[height=0.3\textheight]{singlepixel}
  }
  \hfill
  \includegraphics[height=0.3\textheight]{250px-Re-exposure_of_elephant_-_lahugala_park1.jpg}


  \textbf{Example}

One of the most important constraints in biology and ecology:
  $$B = \mbox{basal metabolic rate}$$
  $$M = \mbox{organismal body mass}$$
  $$B = c M^\alpha$$

%%  Mammals, poikilotherms, birds, trees,\\
%%  bacteria, rocks,\ldots\\

  \includegraphics[height=0.3\textheight]{shrew.jpg}
  \hfill
  \includegraphics[height=0.3\textheight]{shrew-elephant.png}
  \hfill
  \includegraphics[height=0.3\textheight]{250px-Re-exposure_of_elephant_-_lahugala_park1.jpg}


  \textbf{Examples}

  Prefactor $c$ depends on 
  body plan and body temperature.

  $c$ for Birds (body temperature: 39--\tempc{41})\\
  $> c$ for Eutherian Mammals (36--\tempc{38})\\
  $> c$ for Marsupials  (34--\tempc{36})\\
  $> c$ for Monotremes (30--\tempc{31})

  \begin{center}
    \includegraphics[height=0.3\textheight]{Platypus.jpg}
    \quad
    \includegraphics[height=0.3\textheight]{Echidna.jpg}
  \end{center}

%% \newslide{Metabolism}
%% 
%% But it's a controversial business:
%% 
%% Some claim $\alpha=3/4$, others claim $2/3$,
%% and yet others say it's neither.
%% 
%% \mbox{}\hfill long story...




  \textbf{What we might expect} 

%% \begin{center}
%% \tc{blue}{$\alpha=2/3$}  because \ldots 
%% 
%% 
%% 
%% Dimensional analysis suggests\\ 
%% an energy balance surface law:\\
%% $$ B \propto S \propto V^{2/3} \propto M^{2/3}$$
%% 
%% 
%% \end{center}


  \textbf{Metabolism}

What we might expect:

\begin{center}
\tc{blue}{$\alpha=2/3$} because \ldots

Dimensional analysis suggests\\ 
an energy balance surface law:\\
$$ B \propto S \propto V^{2/3} \propto M^{2/3}$$
\end{center}

%% \newslide{What we might also expect} 
%% 
%% $\bullet$  Lognormal fluctuations:\\
%% \quad Gaussian fluctuations in $\log{B}$ around $\log{cM^\alpha}$.
%% 
%% $\bullet$  Stefan-Boltzmann relation for radiated energy:\\
%% \tc{black}{$$\diff{E}{t} = \sigma S T^4$$}


  \textbf{What we might also expect} 

$\bullet$  Lognormal fluctuations:\\
\quad Gaussian fluctuations in $\log{B}$ around $\log{cM^\alpha}$.



$\bullet$  Stefan-Boltzmann relation for radiated energy:\\
\tc{white}{$$\diff{E}{t} = \sigma S T^4$$}




  \textbf{What we might also expect} 

$\bullet$  Lognormal fluctuations:\\
\quad Gaussian fluctuations in $\log{B}$ around $\log{cM^\alpha}$.

$\bullet$  Stefan-Boltzmann relation for radiated energy:\\
\tc{black}{$$\diff{E}{t} = \sigma S T^4$$}


  \textbf{The prevailing belief}

\vspace{2mm}


\tc{blue}{$$ {\alpha = 3/4} $$}
$$ \mbox{i.e.,} \ \ B \propto M^{3/4}$$

 

Huh?




  \textbf{The prevailing belief}

\vspace{2mm}

\tc{blue}{$$ {\alpha = 3/4} $$}
$$ \mbox{i.e.,} \ \ B \propto M^{3/4}$$

\tc{blue}{Hmmm...}


  \textbf{Related putative scalings}

number of capillaries $\propto M^{3/4}$

time to reproductive maturity $\propto M^{1/4}$

heart rate $\propto M^{-1/4}$

cross-sectional area of aorta $\propto M^{3/4}$

population density $\propto M^{-3/4}$


  \textbf{History}

1840's: Sarrus and Rameaux first suggested $\alpha=2/3$.

\begin{center}
  \includegraphics[height=0.3\textheight]{Smokestack_small.jpg}
\end{center}


  \textbf{History}

1883: Rubner studied dogs.

Found $\alpha \simeq 2/3$.

\begin{center}
  \includegraphics[height=0.3\textheight]{bio_dogs.jpg}
\end{center}


  \textbf{History}

1930's: Brody, Benedict study mammals.

Found $\alpha \simeq 0.73 $ (standard).

\begin{center}
  \includegraphics[height=0.3\textheight]{haybale_small}
\end{center}


  \textbf{History}

1932: Kleiber analyzed 13 mammals.

Found $\alpha=0.76$ and suggested $\alpha=3/4$.

\begin{center}
  \includegraphics[height=0.3\textheight]{sliderule_small}
\end{center}


  \textbf{History}

  1950/1960: Hemmingsen

  Extension to unicellular organisms, $\alpha=3/4$ assumed true.

\begin{center}
  \includegraphics[height=0.3\textheight]{slimemold}
\end{center}


  \textbf{History}

  1964: Troon, Scotland:\\
  3rd symposium on energy metabolism.

  $\alpha=3/4$ made official \ldots \hfill \ldots 29 to zip.

\begin{center}
%%  \includegraphics[height=0.3\textheight]{poll_card06_front}
  \includegraphics[height=0.3\textheight]{ballotbox}
\end{center}


  \textbf{Today}

3/4 is held by many to be the one true exponent.

\begin{tabular}{lp{18cm}}
\raisebox{-3cm}{
\includegraphics[height=.28\textheight]{inthebeatofaheart.jpg}
}
&
\textit{In the Beat of a Heart: Life, Energy, and the Unity of Nature}---by John Whitfield
\end{tabular}

Still---much controversy...

% ???
% \newslide{Even in physics \ldots}
%
%   Physics:
%
%  Ratio of electron mass and charge.
% 



  \textbf{Heusner's data, 1991 (391 Mammals)}

  \begin{center}
    \includegraphics[width=0.55\textwidth]{figheusner391_v2}    
  \end{center}

  \tc{blue}{blue line}: 2/3, \tc{red}{red line}: 3/4.



  \textbf{Linear regression}

\tc{blue}{Important:}

Ordinary Least Squares (OLS) Linear regression 
is only appropriate for analyzing
a dataset $\{(x_i,y_i)\}$
when we know the $x_i$ are measured without error.

\inv

Here we assume that measurements of mass $M$
have less error than measurements of metabolic rate $B$.

Linear regression assumes Gaussian errors.

\vis


  \textbf{Linear regression}

\tc{blue}{Important:}

Ordinary Least Squares (OLS) Linear regression 
is only appropriate for analyzing
a dataset $\{(x_i,y_i)\}$
when we know the $x_i$ are measured without error.

Here we assume that measurements of mass $M$
have much less error than measurements of metabolic rate $B$.

\inv

Linear regression assumes Gaussian errors.

\vis


  \textbf{Linear regression}

\tc{blue}{Important:}

Ordinary Least Squares (OLS) Linear regression 
is only appropriate for analyzing
a dataset $\{(x_i,y_i)\}$
when we know the $x_i$ are measured without error.

Here we assume that measurements of mass $M$
have much less error than measurements of metabolic rate $B$.

Linear regression assumes Gaussian errors.


  \textbf{Measuring exponents}

More or regression:

If \tc{blue}{(a)} we don't know what the errors of either variable are,\\
or \tc{blue}{(b)} no variable can be considered independent,

\inv

then we need to use Standardized Major Axis Linear Regression.

{\small (aka Reduced Major Axis)}

\vis 


  \textbf{Measuring exponents}

More or regression:

If \tc{blue}{(a)} we don't know what the errors of either variable are,\\
or \tc{blue}{(b)} no variable can be considered independent,

then we need to use Standardized Major Axis Linear Regression.

{\small (aka Reduced Major Axis)}


  \textbf{Measuring exponents}

For Standardized Major Axis Linear Regression:

$$
\mbox{slope}_{\mbox{\tiny SMA}}
=
\frac{
\mbox{standard deviation of $y$ data}
}
{
\mbox{standard deviation of $x$ data}
}
$$

Very simple!


  \textbf{Measuring exponents}

Relationship to ordinary least squares regression is simple:
\begin{eqnarray*}
\mbox{slope}_{\mbox{\tiny SMA}} & = & r^{-1} \times 
\mbox{slope}_{\mbox{\tiny OLS $y$ on $x$}} \\ 
& = & r \times \mbox{slope}_{\mbox{\tiny OLS $x$ on $y$}}
\end{eqnarray*} \inv
where $r$ = standard correlation coefficient:
$$\tc{white}{
r = \frac{
  \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
}
{
  \sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2}
  \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}
}
}
$$
\vis


  \textbf{Measuring exponents}

Relationship to ordinary least squares regression is simple:
\begin{eqnarray*}
\mbox{slope}_{\mbox{\tiny SMA}} & = & r^{-1} \times 
\mbox{slope}_{\mbox{\tiny OLS $y$ on $x$}} \\ 
& = & r \times \mbox{slope}_{\mbox{\tiny OLS $x$ on $y$}}
\end{eqnarray*}
where $r$ = standard correlation coefficient:
$$
r = \frac{
  \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
}
{
  \sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2}
  \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}
}
$$






  \textbf{Heusner's data, 1991 (391 Mammals)}

  \begin{center}
     \begin{tabular}{c|c|c}
       range of $M$ & $N$ & $\nalpha$ \\ \hline
       & & \\
       $\leq 0.1$ kg    &  167          & $0.678 \pm 0.038$ \\
       & & \\
       $\leq 1$  kg     &  276          & $0.662 \pm 0.032$ \\
       & & \\
       $\leq 10$ kg     &  357          & $0.668 \pm 0.019$ \\
       & & \\
       $\leq 25$ kg     &  366          & $0.669 \pm 0.018$ \\
       & & \\
       $\leq 35$ kg     &  371          & $0.675 \pm 0.018$ \\
       & & \\
       $\leq 350$ kg    &  389          & $0.706 \pm 0.016$ \\
       & & \\
       $\leq 3670$ kg   &  391          & $0.710 \pm 0.021$ \\
     \end{tabular}
  \end{center}


%% \newslide{Data for 391 mammals}
%% 
%%   \begin{center}
%%     \begin{tabular}{c|c|c|ccc}
%%       $M$ range   & $N$ & $r$ & $\nalpha$  &  $\alphak$ & $\alpharma$ \\ \hline
%%        & & & & & \\
%%       $\leq 0.1$     & 167  & 0.810 & 0.678 & 0.693 & 0.837  \\
%%        & & & & & \\                   
%%       $\leq 1$       & 276  & 0.926 & 0.662 & 0.667 & 0.715  \\
%%        & & & & & \\                   
%%       $\leq 10$      & 357  & 0.965 & 0.668 & 0.666 & 0.692  \\
%%        & & & & & \\                   
%%       $\leq 32$      & 371  & 0.968 & 0.675 & 0.671 & 0.697  \\
%%        & & & & & \\                   
%%       $\leq 100$     & 381  & 0.971 & 0.698 & 0.682 & 0.719  \\
%%        & & & & & \\                   
%%       $\leq 1000$    & 390  & 0.975 & 0.707 & 0.691 & 0.725  \\
%%        & & & & & \\                   
%%       $\leq 3670$    & 391  & 0.976 & 0.710 & 0.692 & 0.728  \\
%%     \end{tabular}
%%   \end{center}


%%\newslide{Bartels' data, 1982}
%%
%%  \begin{center}
%%    \begin{tabular}{cc|c|c|c}
%%      $\Mmin$       & $\Mmax$    &  $N$              & $\nalpha$      & $r$ \\ \hline
%%      & & & & \\
%%      $2.4\times10^{-3}$ & 3800  & $\simeq 85$      & 0.66          & 0.99 \\
%%      & & & & \\
%%      $2.4\times10^{-3}$ & 0.26  & $\simeq 40$      & 0.42          & 0.76 \\
%%      & & & & \\
%%      0.26               & 3800  & $\simeq 45$      & 0.76          & 0.99 \\
%%    \end{tabular}
%%  \end{center}
%%
%%\newslide{Brody's data, 1945}
%%
%%  \begin{center}
%%    \begin{tabular}{cc|c|c}
%%      $\Mmin$ & $\Mmax$ & $N$ & $\nalpha$ \\ \hline
%%      & & & \\
%%      0.016 &    1 &  19 & $0.673 \pm 0.061$ \\
%%      & & & \\
%%      0.016 &   10 &  26 & $0.709 \pm 0.041$ \\
%%      & & & \\
%%      10 & 920     &   9 & $0.760 \pm 0.085$ \\
%%      & & & \\
%%      0.016 & 920  &  35 & $0.718 \pm 0.022$ \\
%%    \end{tabular}
%%
%%  \end{center}



  \textbf{Bennett and Harvey, 1987 (398 birds)}

  \begin{center}
    \includegraphics[width=0.55\textwidth]{figbennettbirds398_v2_noname}
  \end{center}

  Passerine vs. non-passerine


  \textbf{Bennett and Harvey, 1987 (398 birds)}

  \begin{center}
    \begin{tabular}{c|c|c}
      $\Mmax$   & $N$ & $\nalpha$  \\ \hline
      & & \\
      $\leq 0.032$   & 162 & $0.636 \pm 0.103$ \\
      & & \\
      $\leq  0.1$    & 236 & $0.602 \pm 0.060$ \\
      & & \\
      $\leq 0.32$    & 290 & $0.607 \pm 0.039$ \\
      & & \\
      $\leq    1$    & 334 & $0.652 \pm 0.030$ \\
      & & \\
      $\leq  3.2$    & 371 & $0.655 \pm 0.023$ \\
      & & \\
      $\leq   10$    & 391 & $0.664 \pm 0.020$ \\
      & & \\
      $\leq   32$    & 396 & $0.665 \pm 0.019$ \\
      & & \\
      $\leq  100$    & 398 & $0.664 \pm 0.019$ \\
    \end{tabular}
  \end{center}



  \textbf{Hypothesis testing}

\tc{blue}{Test to see if $\alpha'$ is consistent
with our data $\{(M_i,B_i)\}$:}

\inv

$H_0: \alpha = \alpha'$ and $H_1: \alpha \ne \alpha'.$

\tc{white}{Assume each $\mathbf{B}_i$ (now a random variable) }
is normally distributed
about $\alpha' \log_{10} M_i + \log_{10} c$.

Follows that the measured $\alpha$ for
one realization obeys
a $t$ distribution with $N-2$ degrees of freedom.

Calculate a $p$-value: probability that the measured
$\alpha$ is as least as different to our hypothesized
$\alpha'$ as we observe.

{\tiny (see, for example, DeGroot and Scherish, ``Probability and Statistics'')}

\vis


  \textbf{Hypothesis testing}

Test to see if $\alpha'$ is consistent
with our data $\{(M_i,B_i)\}$:

\tc{blue}{$H_0: \alpha = \alpha'$ and $H_1: \alpha \ne \alpha'.$}

\inv

\tc{white}{Assume each $\mathbf{B}_i$ (now a random variable) }
is normally distributed
about $\alpha' \log_{10} M_i + \log_{10} c$.

Follows that the measured $\alpha$ for
one realization obeys
a $t$ distribution with $N-2$ degrees of freedom.

Calculate a $p$-value: probability that the measured
$\alpha$ is as least as different to our hypothesized
$\alpha'$ as we observe.

{\tiny (see, for example, DeGroot and Scherish, ``Probability and Statistics'')}

\vis



  \textbf{Hypothesis testing}

Test to see if $\alpha'$ is consistent
with our data $\{(M_i,B_i)\}$:

$H_0: \alpha = \alpha'$ and $H_1: \alpha \ne \alpha'.$

\tc{blue}{Assume each $\mathbf{B}_i$ (now a random variable) 
is normally distributed
about $\alpha' \log_{10} M_i + \log_{10} c$.}

\inv 

Follows that the measured $\alpha$ for
one realization obeys
a $t$ distribution with $N-2$ degrees of freedom.

Calculate a $p$-value: probability that the measured
$\alpha$ is as least as different to our hypothesized
$\alpha'$ as we observe.

{\tiny (see, for example, DeGroot and Scherish, ``Probability and Statistics'')}

\vis


  \textbf{Hypothesis testing}

Test to see if $\alpha'$ is consistent
with our data $\{(M_i,B_i)\}$:

$H_0: \alpha = \alpha'$ and $H_1: \alpha \ne \alpha'.$

Assume each $\mathbf{B}_i$ (now a random variable) 
is normally distributed
about $\alpha' \log_{10} M_i + \log_{10} c$.

\tc{blue}{Follows that the measured $\alpha$ for
one realization obeys
a $t$ distribution with $N-2$ degrees of freedom.}

\inv

\tc{blue}{Calculate a $p$-value: probability that the measured
$\alpha$ is as least as different to our hypothesized
$\alpha'$ as we observe.}

{\tiny (see, for example, DeGroot and Scherish, ``Probability and Statistics'')}

\vis


  \textbf{Hypothesis testing}

Test to see if $\alpha'$ is consistent
with our data $\{(M_i,B_i)\}$:

$H_0: \alpha = \alpha'$ and $H_1: \alpha \ne \alpha'.$

Assume each $\mathbf{B}_i$ (now a random variable) 
is normally distributed
about $\alpha' \log_{10} M_i + \log_{10} c$.

Follows that the measured $\alpha$ for
one realization obeys
a $t$ distribution with $N-2$ degrees of freedom.

\tc{blue}{Calculate a $p$-value: probability that the measured
$\alpha$ is as least as different to our hypothesized
$\alpha'$ as we observe.}

\inv 

{\tiny \tc{blue}{(see, for example, DeGroot and Scherish, ``Probability and Statistics'')}}

\vis


  \textbf{Hypothesis testing}

Test to see if $\alpha'$ is consistent
with our data $\{(M_i,B_i)\}$:

$H_0: \alpha = \alpha'$ and $H_1: \alpha \ne \alpha'.$

Assume each $\mathbf{B}_i$ (now a random variable) 
is normally distributed
about $\alpha' \log_{10} M_i + \log_{10} c$.

Follows that the measured $\alpha$ for
one realization obeys
a $t$ distribution with $N-2$ degrees of freedom.

Calculate a $p$-value: probability that the measured
$\alpha$ is as least as different to our hypothesized
$\alpha'$ as we observe.

{\tiny \tc{blue}{(see, for example, DeGroot and Scherish, ``Probability and Statistics'')}}


  \textbf{Revisiting the past---mammals}

    Full mass range:

    \begin{tabular}{ccccccc}
             & $N$ & $\nalpha$ & $p_{2/3}$ & $p_{3/4}$ \\ \hline
              & & & & & & \\
      Kleiber             &  13 & 0.738 & $<10^{-6}$ & 0.11 \\
              & & & & & & \\
      Brody               &  35 & 0.718 & $<10^{-4}$ & $<10^{-2}$ \\
              & & & & & & \\
      Heusner             & 391 & 0.710 & $<10^{-6}$ & $<10^{-5}$ \\
              & & & & & & \\
      Bennett   & 398 & 0.664 & 0.69 &     $<10^{-15}$ \\
      and Harvey & & & & & & \\
    \end{tabular}


  \textbf{Revisiting the past---mammals}

  $M \leq 10$ kg:
   \begin{tabular}{ccccccc}
              & $N$ & $\nalpha$ & $p_{2/3}$ & $p_{3/4}$ \\ \hline
              & & & & & & \\
      Kleiber        &   5 & 0.667 &  0.99 & 0.088 \\
              & & & & & & \\
      Brody          &  26 & 0.709   & $<10^{-3}$ & $<10^{-3}$ \\
              & & & & & & \\
      Heusner        & 357 & 0.668 &   0.91 &   $<10^{-15}$ \\
    \end{tabular}



  $M \geq 10$ kg:
   \begin{tabular}{ccccccc}
              & $N$ & $\nalpha$ &  $p_{2/3}$ & $p_{3/4}$ \\ \hline
              & & & & & & \\
      Kleiber        &   8 & 0.754 & $<10^{-4}$ & 0.66 \\
              & & & & & & \\
      Brody          &   9 & 0.760 & $<10^{-3}$ & 0.56 \\
              & & & & & & \\
      Heusner        &  34 & 0.877 & $< 10^{-12}$ & $<10^{-7}$ \\
    \end{tabular}




%% \newslide{Correlations in residuals}
%%
%%  \vspace{5mm}
%%  \begin{center}
%%    \includegraphics[angle=90,height=0.9\textheight]{figheusner_stats}
%%  \end{center}


% The individual plots correspond
% to the following ranges: 
% (a) $M<3.2$ kg, (b) $M<10$ kg, (c) $M<32$ kg, (d) all mammals.
% For all mass ranges considered, $p_{2/3}>0.05$ and $p_{3/4} \ll 10^{-4}$.  


  \textbf{\small Fluctuations---Kolmogorov-Smirnov test}

  \begin{center}
    \includegraphics[angle=90,height=0.7\textheight]{figmetascalingfn2}

     $  P(B\, |M) = 1/M^{2/3} f(B/M^{2/3})$
  \end{center}


  \textbf{Analysis of residuals}

\tc{blue}{(1) Presume an exponent of your choice: 2/3 or 3/4.}

\inv

\tc{black}{(2) Fit the prefactor ($\log_{10} c$) and then
examine the residuals:
$$ 
r_i = \log_{10} B_i - (\alpha' \log_{10} M_i - \log_{10} c).
$$}


(3) $H_0$: residuals are uncorrelated\\
\ \ \quad $H_1$: residuals are correlated.

(4) Measure the correlations in the residuals
and compute a $p$-value.

\vis


  \textbf{Analysis of residuals}

(1) Presume an exponent of your choice: 2/3 or 3/4.


\tc{blue}{(2) Fit the prefactor ($\log_{10} c$) and then
examine the residuals:
$$ 
r_i = \log_{10} B_i - (\alpha' \log_{10} M_i - \log_{10} c).
$$}

\inv

(3) $H_0$: residuals are uncorrelated\\
\ \ \quad $H_1$: residuals are correlated.

(4) Measure the correlations in the residuals
and compute a $p$-value.

\vis


  \textbf{Analysis of residuals}

(1) Presume an exponent of your choice: 2/3 or 3/4.

\tc{black}{(2) Fit the prefactor ($\log_{10} c$) and then
examine the residuals:
$$ 
r_i = \log_{10} B_i - (\alpha' \log_{10} M_i - \log_{10} c).
$$}


\tc{blue}{(3) $H_0$: residuals are uncorrelated\\
\ \ \quad $H_1$: residuals are correlated.}

\inv

(4) Measure the correlations in the residuals
and compute a $p$-value.


  \textbf{Analysis of residuals}

(1) Presume an exponent of your choice: 2/3 or 3/4.

\tc{black}{(2) Fit the prefactor ($\log_{10} c$) and then
examine the residuals:
$$ 
r_i = \log_{10} B_i - (\alpha' \log_{10} M_i - \log_{10} c).
$$}


(3) $H_0$: residuals are uncorrelated\\
\ \ \quad $H_1$: residuals are correlated.


\tc{blue}{(4) Measure the correlations in the residuals
and compute a $p$-value.}

\inv

\vis


  \textbf{Analysis of residuals}

\tc{blue}{We use the spiffing \\
Spearman Rank-Order Correlation Cofficient.}

\inv 

Basic idea:  Given $\{(x_i,y_i)\}$, rank 
the $\{x_i\}$ and $\{y_i\}$ separately from
smallest to largest.  Call these ranks $R_i$ and $S_i$.

Now calculate correlation coefficient for ranks, $r_s$:

{\small
\tc{white}{
$$ r_s 
= 
\frac{
  \sum_{i=1}^{n} (R_i - \bar{R})(S_i - \bar{S})
}
{
  \sqrt{\sum_{i=1}^{n} (R_i - \bar{R})^2}
  \sqrt{\sum_{i=1}^{n} (S_i - \bar{S})^2}
}
$$
}
}

Perfect correlation: $x_i$'s and $y_i$'s both
increase monotonically.

\vis



  \textbf{Analysis of residuals}

We use the spiffing \\
Spearman Rank-Order Correlation Cofficient.

\tc{blue}{Basic idea:  Given $\{(x_i,y_i)\}$, rank 
the $\{x_i\}$ and $\{y_i\}$ separately from
smallest to largest.  Call these ranks $R_i$ and $S_i$.}

\inv 

Now calculate correlation coefficient for ranks, $r_s$:

{\small
\tc{white}{
$$ r_s 
= 
\frac{
  \sum_{i=1}^{n} (R_i - \bar{R})(S_i - \bar{S})
}
{
  \sqrt{\sum_{i=1}^{n} (R_i - \bar{R})^2}
  \sqrt{\sum_{i=1}^{n} (S_i - \bar{S})^2}
}
$$
}
}

Perfect correlation: $x_i$'s and $y_i$'s both
increase monotonically.

\vis



  \textbf{Analysis of residuals}

We use the spiffing \\
Spearman Rank-Order Correlation Cofficient.

Basic idea:  Given $\{(x_i,y_i)\}$, rank 
the $\{x_i\}$ and $\{y_i\}$ separately from
smallest to largest.  Call these ranks $R_i$ and $S_i$.

Now calculate correlation coefficient for ranks, $r_s$:

{\small
\tc{blue}{$$ r_s 
= 
\frac{
  \sum_{i=1}^{n} (R_i - \bar{R})(S_i - \bar{S})
}
{
  \sqrt{\sum_{i=1}^{n} (R_i - \bar{R})^2}
  \sqrt{\sum_{i=1}^{n} (S_i - \bar{S})^2}
}
$$
}
}

\inv

Perfect correlation: $x_i$'s and $y_i$'s both
increase monotonically.

\vis



  \textbf{Analysis of residuals}

We use the spiffing \\
Spearman Rank-Order Correlation Cofficient.

Basic idea:  Given $\{(x_i,y_i)\}$, rank 
the $\{x_i\}$ and $\{y_i\}$ separately from
smallest to largest.  Call these ranks $R_i$ and $S_i$.

Now calculate correlation coefficient for ranks, $r_s$:

{\small
$$ r_s 
= 
\frac{
  \sum_{i=1}^{n} (R_i - \bar{R})(S_i - \bar{S})
}
{
  \sqrt{\sum_{i=1}^{n} (R_i - \bar{R})^2}
  \sqrt{\sum_{i=1}^{n} (S_i - \bar{S})^2}
}
$$
}

\tc{blue}{
Perfect correlation: $x_i$'s and $y_i$'s both
increase monotonically.
}

\vis



  \textbf{Analysis of residuals}

\tc{blue}{We assume all rank orderings are equally likely.}

\inv

$r_s$ is distributed according to a Student's distribution
with $N-2$ degrees of freedom.

Excellent feature: Non-parametric---real distribution
of $x$'s and $y$'s doesn't matter.

Bonus: works for non-linear monotonic relationships as well.

{\tiny (See ``Numerical Recipes in C/Fortran'' which contains many good things)}

\vis


  \textbf{Analysis of residuals}

We assume all rank orderings are equally likely.

\tc{blue}{$r_s$ is distributed according to a Student's distribution
with $N-2$ degrees of freedom.}

\inv

Excellent feature: Non-parametric---real distribution
of $x$'s and $y$'s doesn't matter.

Bonus: works for non-linear monotonic relationships as well.

{\tiny (See ``Numerical Recipes in C/Fortran'' which contains many good things)}

\vis


  \textbf{Analysis of residuals}

We assume all rank orderings are equally likely.

$r_s$ is distributed according to a Student's distribution
with $N-2$ degrees of freedom.

\tc{blue}{Excellent feature: Non-parametric---real distribution
of $x$'s and $y$'s doesn't matter.}

\inv

Bonus: works for non-linear monotonic relationships as well.

{\tiny (See ``Numerical Recipes in C/Fortran'' which contains many good things)}

\vis


  \textbf{Analysis of residuals}

We assume all rank orderings are equally likely.

$r_s$ is distributed according to a Student's distribution
with $N-2$ degrees of freedom.

Excellent feature: Non-parametric---real distribution
of $x$'s and $y$'s doesn't matter.

\tc{blue}{Bonus: works for non-linear monotonic relationships as well.}

\inv

{\tiny \tc{blue}{(See ``Numerical Recipes in C/Fortran'' which contains many good things)}}


\vis


  \textbf{Analysis of residuals}

We assume all rank orderings are equally likely.

$r_s$ is distributed according to a Student's distribution
with $N-2$ degrees of freedom.

Excellent feature: Non-parametric---real distribution
of $x$'s and $y$'s doesn't matter.

Bonus: works for non-linear monotonic relationships as well.

{\tiny \tc{blue}{(See ``Numerical Recipes in C/Fortran'' which contains many good things)}}

\inv

\vis


  \textbf{Analysis of residuals---mammals}

  \vspace{5mm}
  \begin{center}
    \includegraphics[width=0.55\textwidth]{figmammals_pv_log_noname}
  \end{center}
{\small (a) $M<3.2$ kg, (b) $M<10$ kg, (c) $M<32$ kg, (d) all mammals.}

% The individual plots correspond
% to the following ranges: 
% (a) $M<3.2$ kg, (b) $M<10$ kg, (c) $M<32$ kg, (d) all mammals.
% For all mass ranges considered, $p_{2/3}>0.05$ and $p_{3/4} \ll 10^{-4}$.  



  \textbf{Analysis of residuals---birds}

  \vspace{5mm}
  \begin{center}
    \includegraphics[width=0.55\textwidth]{figbirds_pv_log_noname}
  \end{center}
{\small (a) $M<.1$ kg, (b) $M<1$ kg, (c) $M<10$ kg, (d) all birds.}

% The individual plots correspond
% to the following ranges: 
% (a) $M<3.2$ kg, (b) $M<10$ kg, (c) $M<32$ kg, (d) all mammals.
% For all mass ranges considered, $p_{2/3}>0.05$ and $p_{3/4} \ll 10^{-4}$.  


%% \newslide{\small Fluctuations}
%% 
%%   \vspace{5mm}
%%   \begin{center}
%%     \begin{tabular}{l|ccccc}
%%       & range  & $\sigma$ &    $p$  & $\sigma^{\ast}$ & $p^{\ast}$ \\ \hline
%%       & & & & & \\
%%       mammals & $M < 1$  & 0.153    & 0.232   & 0.120 & 0.307 \\
%%       & & & & & \\
%%       mammals & $M < 10$ & 0.153    & 0.093   & 0.120 & 0.135 \\
%%       & & & & & \\
%%       birds   & all    & 0.132    & 0.032   & 0.115 & 0.573 \\ 
%%     \end{tabular}
%% 
%%     $$  P(B\, |M) = 1/M^{2/3} f(B/M^{2/3})$$
%%   \end{center}

% \end{comment}


  \textbf{Quarterology}

Theories for how metabolic rate should scale with size.


  \textbf{Earlier theories}

Building on the surface area idea...

\tc{blue}{Blum (1977) speculates on four-dimensional biology:}

$$B \propto M^{(d-1)/d}$$

\inv

$d=3$ gives $\alpha = 2/3$

$d=4$ gives $\alpha = 3/4$, so we need another dimension...

\vis


  \textbf{Earlier theories}

Building on the surface area idea...

\tc{black}{Blum (1977) speculates on four-dimensional biology:}

$$B \propto M^{(d-1)/d}$$

\tc{blue}{$d=3$ gives $\alpha = 2/3$}

\inv

$d=4$ gives $\alpha = 3/4$, so we need another dimension...

\vis


  \textbf{Earlier theories}

Building on the surface area idea...

\tc{black}{Blum (1977) speculates on four-dimensional biology:}

$$B \propto M^{(d-1)/d}$$


$d=3$ gives $\alpha = 2/3$

\tc{blue}{$d=4$ gives $\alpha = 3/4$, so we need another dimension...}

\inv

\vis



  \textbf{Earlier theories}

Building on the surface area idea:

McMahon (70's, 80's): Elastic Similarity

\ding{228} Idea is that organismal shapes scale allometrically
with 1/4 powers {\small (like nails and trees...)}

\ding{228} Appears to be true for ungulate legs.

\ding{228} Metabolism and shape never properly connected.


  \textbf{Networks {\tiny (West \etal, Science, 1997)}}

  \textbf{1.} Nutrient delivering networks 

  \tc{blue}{Assumptions:\\
  $\bullet$ hierarchical network.\\
  $\bullet$ capillaries (delivery units) invariant.\\
  $\bullet$ network impedance is minimized via evolution}

  \inv

  \tc{blue}{Claims:\\
  $\bullet$ $B \propto M^{3/4}$.\\
  $\bullet$ networks are fractal.\\
  $\bullet$ quarter powers everywhere.\\}

  \vis


  \textbf{Networks {\tiny (West \etal, Science, 1997)}}

  \textbf{1.} Nutrient delivering networks 

  Assumptions:\\
  $\bullet$ hierarchical network.\\
  $\bullet$ capillaries (delivery units) invariant.\\
  $\bullet$ network impedance is minimized via evolution


  \tc{blue}{Claims:\\
  $\bullet$ $B \propto M^{3/4}$.\\
  $\bullet$ networks are fractal.\\
  $\bullet$ quarter powers everywhere.\\}




  \textbf{Networks {\tiny (West \etal, Science, 1997)}}

\begin{center}
  \includegraphics[angle=90,height=0.9\textheight]{west97figure1}
\end{center}


  \textbf{Networks {\tiny (West \etal, Science, 1997)}}

  $$ N_{k+1}/N_k = n_k, 
  \qquad 
  l_{k+1}/l_k = \gamma_k, 
  \qquad
  r_{k+1}/r_k = \beta_k $$

  Poiseuille flow (outer branches):
  $$ Z = \frac{8\mu}{\pi} \sum_{k=0}^N \frac{l_k}{r_k^4 N_k} $$

  Pulsatile flow (main branches):
  $$ Z \propto \sum_{k=0}^N \frac{h_k^{1/2}}{r_k^{5/2} N_k} $$



  \textbf{Not so fast \ldots}

  Actually, model shows:\\
  $\bullet$ $B \propto M^{3/4}$ does not follow for pulsatile flow

  $\bullet$ networks are not necessarily fractal.

  \inv

  Do find:\\
  $\bullet$ Murray's law for outer branches: $r^{3} = r_1^{3} + r_2^{3}$.

  $\bullet$ Impedance is distributed evenly.
  
  $\bullet$ Can still assume networks are fractal.


  \textbf{Not so fast \ldots}

  Actually, model shows:\\
  $\bullet$ $B \propto M^{3/4}$ does not follow for pulsatile flow

  $\bullet$ networks are not necessarily fractal.


  Do find:\\
  $\bullet$ Murray's law for outer branches: $r^{3} = r_1^{3} + r_2^{3}$.

  $\bullet$ Impedance is distributed evenly.
  
  $\bullet$ Can still assume networks are fractal.



  \textbf{Connecting network structure to $\alpha$}

  (1) Can estimate $V_{\textrm{blood} \propto (\beta^2 \gamma)^{-N} \propto M$\\
  (also problematic due to  prefactor issues)

  (2) Number of capillaries $\simeq n^N \propto B \propto M^\alpha$.
  
  $$ \Rightarrow \ \ \alpha = -\frac{\ln{n}}{\ln{\beta^2\gamma}}$$

  
  West \etal\ claim $\beta = n^{-1/2}$ (area-preservingness)\\
  and $\gamma = n^{-1/3}$ (space-fillingness).

  $\Rightarrow$ $\alpha=3/4$.

  


  \textbf{ Connecting network structure to $\alpha$}

  (1) Can estimate $V_{\textrm{blood} \propto (\beta^2 \gamma)^{-N} \propto M$.\\
  (also problematic due to  prefactor issues)


  (2) Number of capillaries $\simeq n^N \propto B \propto M^\alpha$.
  
  $$ \Rightarrow \ \ \alpha = -\frac{\ln{n}}{\ln{\beta^2\gamma}}$$

  West \etal\ claim $\beta = n^{-1/2}$ (area-preservingness)\\
  and $\gamma = n^{-1/3}$ (space-fillingness).

  $\Rightarrow$ $\alpha=3/4$.


  \textbf{Data from real networks}

%% ignore vein data
  {\small
    \begin{center}
    \begin{tabular}{c|ccc|cc|c}
      Network & $n$ & $\beta^{-1}$ & $\gamma^{-1}$ & $-\frac{\ln\beta}{\ln{n}}$ & $-\frac{\ln\gamma}{\ln{n}}$  & $-\frac{\ln{n}}{\ln{\beta^2\gamma}}$  \\ 
      \hline
      & & & & & & \\
      West \etal\      & --   & --   & --   & 1/2  & 1/3  & 3/4   \\
      & & & & & & \\
%      Dimensional analysis               & --   & --   & --   & 1/2  & 1/2  & 2/3   \\
      rat (PAT)           & 2.76 & 1.58 & 1.60 & 0.45 & 0.46 & 0.73  \\
      & & & & & & \\
      cat (PAT)           & 3.67 & 1.71 & 1.78 & 0.41 & 0.44 & 0.79  \\
      & & & & & & \\
      dog (PAT)           & 3.69 & 1.67 & 1.52 & 0.39 & 0.32 & 0.90  \\
%%      dog (PVT)           & 3.76 & 1.70 & 1.56 & 0.40 & 0.34 & 0.88  \\
      & & & & & & \\
      pig (LCX)           & 3.57 & 1.89 & 2.20 & 0.50 & 0.62 & 0.62  \\
      pig (RCA)           & 3.50 & 1.81 & 2.12 & 0.47 & 0.60 & 0.65  \\
      pig (LAD)           & 3.51 & 1.84 & 2.02 & 0.49 & 0.56 & 0.65  \\
%%      pig (TV)            & 3.05 & 1.73 & 1.61 & 0.49 & 0.43 & 0.71  \\
%%      pig (SV)            & 3.37 & 1.72 & 1.77 & 0.45 & 0.47 & 0.73  \\ 
      & & & & & & \\
      human (PAT)         & 3.03 & 1.60 & 1.49 & 0.42 & 0.36 & 0.83  \\
%%      human (PVT)         & 3.30 & 1.68 & 1.68 & 0.43 & 0.43 & 0.77  \\
      human (PAT)         & 3.36 & 1.56 & 1.49 & 0.37 & 0.33 & 0.94  \\
%%      human (PVT)         & 3.33 & 1.58 & 1.50 & 0.38 & 0.34 & 0.91  \\ 
    \end{tabular}
    \end{center}
    }




  \textbf{Simpler networks}

  \begin{center}
    \includegraphics[height=0.7\textheight]{banavar1999fig1.png}
  \end{center}
  What is the most efficient transportation network?\\
  \hfill{\tiny (Banavar \etal, Nature, 1999)}


  \textbf{Simpler networks}

  Banavar \etal\ find `most efficient' networks with
  $$B \propto M^{d/(d+1)}$$

  
  but also find 
  $$V_{\textrm{blood} \propto M^{(d+1)/d}$$

3 g shrew with $V_{\textrm{blood}$ = $0.1V_{\textrm{body}$

$\Rightarrow$
3000 kg elephant with $V_{\textrm{blood}$ = $10V_{\textrm{body}$ (oops)



  \textbf{Simpler networks}

  Banavar \etal\ find `most efficient' networks with
  $$B \propto M^{d/(d+1)}$$

  but also find 
  $$V_{\textrm{blood} \propto M^{(d+1)/d}$$

  
3 g shrew with $V_{\textrm{blood}$ = $0.1V_{\textrm{body}$ (oops)

$\Rightarrow$
3000 kg elephant with $V_{\textrm{blood}$ = $10V_{\textrm{body}$




  \textbf{Simpler networks}

  Banavar \etal\ find `most efficient' networks with
  $$B \propto M^{d/(d+1)}$$

  but also find 
  $$V_{\textrm{blood} \propto M^{(d+1)/d}$$

3 g shrew with $V_{\textrm{blood}$ = $0.1V_{\textrm{body}$

$\Rightarrow$
3000 kg elephant with $V_{\textrm{blood}$ = $10V_{\textrm{body}$ \inv (oops)


  \textbf{Simpler networks}

  Banavar \etal\ find `most efficient' networks with
  $$B \propto M^{d/(d+1)}$$

  but also find 
  $$V_{\textrm{blood} \propto M^{(d+1)/d}$$

3 g shrew with $V_{\textrm{blood}$ = $0.1V_{\textrm{body}$

$\Rightarrow$
3000 kg elephant with $V_{\textrm{blood}$ = $10V_{\textrm{body}$ (oops)


  \textbf{Geometric argument}

\tc{blue}{An even simpler argument for $d$-dimensional regions:}

Let's consider a class of $d$-dimensional 
scalable spatial regions in $D \ge d$ dimensions.

Denote such a region with volume $V$ as
$\volume{V}$.


  \textbf{Geometric argument}

Overall shape of region may scale allometrically:
\begin{center}
  \includegraphics[angle=-90,width=0.8\textwidth]{shapescaling}      
\end{center}


  \textbf{Geometric argument}

Consider network as a bundle of virtual vessels:
\begin{center}
  \includegraphics[angle=-90,width=0.8\textwidth]{virtualvessels4.pdf}
\end{center}

Material drawn by sinks per unit time is invariant.



  \textbf{Geometric argument}

\begin{center}
  \includegraphics[angle=-90,width=0.8\textwidth]{efficientnetworks5.pdf}
\end{center}


% shapescaling
%  \includegraphics[angle=-90,width=0.48\textwidth]{}  
%  \includegraphics[angle=-90,width=0.48\textwidth]{efficientnetworks5.pdf}


Best case: lengths of virtual vessels $\propto r$\\
where $r$ = distance to the source.

Worst case: lengths of virtual vessels $\propto L^d$.




  \textbf{Geometric argument}

Our minimal networks satisfy:

\begin{eqnarray*}
  \label{eq:supnet.optVn1}
  \lefteqn{ \min V_{\textrm{net}  \propto \int_{\volume{V}} \rho \, ||\vec{x}|| \, \dee{\vec{x}},} \nonumber \\
  & = & \rho \int_{\volume{V}} (x_1^2 + x_2^2 + \ldots + x_d^2)^{1/2} \dee{\vec{x}}. \nonumber \\
\end{eqnarray*}


  \textbf{Geometric argument}

Substituting $x_i = L_i u_i$, we have
\begin{eqnarray*}
  \label{eq:supnet.optVn2}
  \lefteqn{\min V_{\textrm{net} 
   \propto   
  \rho L_1 L_2 \cdots L_d \times} \nonumber \\ 
  &  &  
  \int_{\volume{c}} (L_1^2 u_1^2 + L_2^2 u_2^2 + \ldots + L_d^2 u_d^2)^{1/2} \dee{\vec{u}}, \nonumber \\
  & \propto  &
  \rho V
  \int_{\volume{c}} (L_1^2 u_1^2 + L_2^2 u_2^2 + \ldots + L_d^2 u_d^2)^{1/2} \dee{\vec{u}},
\end{eqnarray*}

\inv

Integrand is dominated by the $L_i$ that scale strongest with $V$.

\tc{blue}{Assume the first $k \le d$ lengths scale with equal strength:
$$L_i = c_i V^{\gamma}$$}


  \textbf{Geometric argument}

Substituting $x_i = L_i u_i$, we have
\begin{eqnarray*}
  \label{eq:supnet.optVn2}
  \lefteqn{\min V_{\textrm{net} 
   \propto   
  \rho L_1 L_2 \cdots L_d \times} \nonumber \\ 
  &  &  
  \int_{\volume{c}} (L_1^2 u_1^2 + L_2^2 u_2^2 + \ldots + L_d^2 u_d^2)^{1/2} \dee{\vec{u}}, \nonumber \\
  & \propto  &
  \rho V
  \int_{\volume{c}} (L_1^2 u_1^2 + L_2^2 u_2^2 + \ldots + L_d^2 u_d^2)^{1/2} \dee{\vec{u}},
\end{eqnarray*}

\tc{blue}{Integrand is dominated by the $L_i$ that scale strongest with $V$.}

\inv

\tc{blue}{Assume the first $k \le d$ lengths scale with equal strength:
$$L_i = c_i V^{\gamma}$$}



  \textbf{Geometric argument}

Substituting $x_i = L_i u_i$, we have
\begin{eqnarray*}
  \label{eq:supnet.optVn2}
  \lefteqn{\min V_{\textrm{net} 
   \propto   
  \rho L_1 L_2 \cdots L_d \times} \nonumber \\ 
  &  &  
  \int_{\volume{c}} (L_1^2 u_1^2 + L_2^2 u_2^2 + \ldots + L_d^2 u_d^2)^{1/2} \dee{\vec{u}}, \nonumber \\
  & \propto  &
  \rho V
  \int_{\volume{c}} (L_1^2 u_1^2 + L_2^2 u_2^2 + \ldots + L_d^2 u_d^2)^{1/2} \dee{\vec{u}},
\end{eqnarray*}

Integrand is dominated by the $L_i$ that scale strongest with $V$.

\tc{blue}{Assume the first $k \le d$ lengths scale with equal strength:
$$L_i = c_i V^{\gamma}$$}



  \textbf{Geometric argument}

\begin{eqnarray*}
  \label{eq:supnet.optVn3}
  \min V_{\textrm{net} 
  & \rightarrow & 
  \rho V^{1+\gamma}
  \int_{\volume{c}} (c_1^{2} u_1^2 + c_2^{2} u_2^2 + \ldots + c_k^{2} u_k^2 
  )^{1/2} \dee{\vec{u}}
  \nonumber \\
  & \propto & 
  \rho V^{1+\gamma}
\end{eqnarray*}





  \textbf{Geometric argument}

If we allow $\rho$ to vary, then we find
$$\tc{blue}{ V_{\textrm{network} \propto \rho L^{d+1}} $$

 

\tc{black}{Since $ V_{\textrm{network} \propto V_{\textrm{body} \propto L^d $, we must have
$\rho \propto L^{-1}$.}


\tc{black}{$\Rightarrow$ capillary density must decrease as $M$ increases
(observed).}




  \textbf{Geometric argument}

If we allow $\rho$ to vary, then we find
$$ V_{\textrm{network} \propto \rho L^{d+1} $$

\tc{blue}{Since $ V_{\textrm{network} \propto L^d $, we must have
$\rho \propto L^{-1}$.  }

 

\tc{black}{$\Rightarrow$ capillary density must decrease as $M$ increases
(observed).}




  \textbf{Geometric argument}

If we allow $\rho$ to vary, then we find
$$ V_{\textrm{network} \propto \rho L^{d+1} $$

\tc{black}{Since $ V_{\textrm{network} \propto L^d $, we must have
$\rho \propto L^{-1}$.}

\tc{blue}{$\Rightarrow$ capillary density must decrease as $M$ increases
(observed).}


  \textbf{Geometric argument}

  \begin{eqnarray*}
    B & \propto & \tc{blue}{\rho L^{d}} \nonumber \\ 
    & \tc{white}{\propto} & \tc{white}{L^{-1} L^{d}} \nonumber \\
    & \tc{white}{\propto} & \tc{white}{M^{(d-1)/d}}  \nonumber \\
  \end{eqnarray*}
  {
    \qquad \qquad and for $d=3$, we have $\alpha=2/3$.}

  
   If body shapes change allometrically, the
    exponent \tc{blue}{decreases}.
   Less efficient networks have \tc{blue}{lower} exponents too\\
    (b/c they must have lower densities of sinks).
  


  \textbf{Summary}

  
   
    The exponent $\alpha = 2/3$ works for all birds and
    mammals up to 10--30 kg
   For mammals $>$ 10--30 kg, maybe we have a new scaling regime
   Economos: limb length break in scaling around 20 kg
   White and Seymour, 2005: unhappy with large herbivore measurements.
Find $\alpha \simeq 0.686 \pm 0.014$
  


  \textbf{Prefactor}

  \textbf{Stefan-Boltzmann law:}
    
    
      $$\diff{E}{t} = \sigma S T^4$$
      where $S$ is surface and $T$ is temperature.
     
      Very rough estimate of prefactor based on scaling
      of normal mammalian body temperature and surface
      area $S$:
      $$B \simeq 10^5M^{2/3} \mbox{erg/sec}.$$
    
      Measured for $M \leq 10$ kg:
      $$B=2.57\times 10^5M^{2/3} \mbox{erg/sec}.$$
    
  


  \textbf{Supply networks}

Other implications:

If region is embedded in a higher dimensional space,
network volume can scale faster than region volume.

\inv

Example: river networks = aggregation networks.

Now $\rho =$ constant (roughly, rain falls uniformly over time).

Okay because network volume expands into third dimension.


  \textbf{Supply networks}

Other implications:

If region is embedded in a higher dimensional space,
network volume can scale faster than region volume.

\tc{blue}{Example: river networks = aggregation networks.}

\inv

Now $\rho =$ constant (roughly, rain falls uniformly over time).

Okay because network volume expands into third dimension.


  \textbf{Supply networks}

Other implications:

If region is embedded in a higher dimensional space,
network volume can scale faster than region volume.

Example: river networks = aggregation networks.

\tc{blue}{Now $\rho =$ constant (roughly, rain falls uniformly over time).}

\inv

Okay because network volume expands into third dimension.


  \textbf{Supply networks}

Other implications:

If region is embedded in a higher dimensional space,
network volume can scale faster than region volume.


Example: river networks = aggregation networks.

Now $\rho =$ constant (roughly, rain falls uniformly over time).

\tc{blue}{Okay because network volume expands into third dimension.}




  \textbf{Supply networks}

In some areas, small basins seem to scale allometrically

$\Rightarrow$ typically lengthen faster than
they widen

Hack's law (1957): $l \sim a^h$

\inv

But find largest basins do scale isometrically

{\small (Montgomery and Dietrich, 1992)}

Evidence of a basic kind of optimality.


  \textbf{Supply networks}

In some areas, small basins seem to scale allometrically

$\Rightarrow$ typically lengthen faster than
they widen

Hack's law (1957): $l \sim a^h$

But find largest basins do scale isometrically

{\small (Montgomery and Dietrich, 1992)}

Evidence of a basic kind of optimality.


  \textbf{Power laws}

{\Large
  Next: Power-law size distributions...
}



  \textbf{Size distributions}

The sizes of many systems' elements
appear to obey an\\
\tc{blue}{inverse power-law 
size distribution}:
$$P(\mbox{size}=x) \sim  c\, x^{-\gamma}$$
$$\mbox{where} \quad x_{\textrm{min} < x < x_{\textrm{max}$$
$$\mbox{and} \quad \gamma > 1 $$
\inv
  Typically, $2 < \gamma < 3$.\\
  $x_{\textrm{min}$ is called the lower cutoff\\
  $x_{\textrm{max}$ is the upper cutoff\\



  \textbf{Size distributions}

The sizes of many systems' elements
appear to obey an\\
\tc{blue}{inverse power-law 
size distribution}:
$$P(\mbox{size}=x) \sim  c\, x^{-\gamma}$$
$$\mbox{where} \quad x_{\textrm{min} < x < x_{\textrm{max}$$
$$\mbox{and} \quad \gamma > 1 $$
\tc{blue}{Typically, $2 < \gamma < 3$.}\\
\inv
  $x_{\textrm{min}$ is called the lower cutoff\\
  $x_{\textrm{max}$ is the upper cutoff\\


  \textbf{Size distributions}

The sizes of many systems' elements
appear to obey an\\
\tc{blue}{inverse power-law 
size distribution}:
$$P(\mbox{size}=x) \sim  c\, x^{-\gamma}$$
$$\mbox{where} \quad x_{\textrm{min} < x < x_{\textrm{max}$$
$$\mbox{and} \quad \gamma > 1 $$
  Typically, $2 < \gamma < 3$.\\
  \tc{blue}{$x_{\textrm{min}$ is called the lower cutoff}\\
  \tc{blue}{$x_{\textrm{max}$ is the upper cutoff}\\



  \textbf{Size distributions}

Usually, only the tail of the distribution
obeys a power law:

\tc{blue}{$$P(x) \sim  c\, x^{-\gamma} \ \mbox{as} \ x \rightarrow \infty.$$}

\ding{228} Still use term `power law distribution'


  \textbf{Size distributions}

Many systems have discrete sizes $k$:\\
Word frequency, number of hyperlinks, number of citations, etc.
$$P(k) \sim  c\, k^{-\gamma}$$
$$\mbox{where} \ \  k_{\textrm{min} \le k \le k_{\textrm{max}$$

\ding{228} Work with sums instead
of integrals in calculations.


  \textbf{Size distributions}

Power law size distributions 
are sometimes called\\ \tc{blue}{Pareto distributions}.

(especially by economists)

\ding{228} Italian scholar Vilfredo Pareto.

\ding{228} Pareto noted wealth in Italy was distributed unevenly\\
\quad 80-20 rule.


  \textbf{Size distributions}

Negative linear relationships in log space:

$$ \log P(x) = \log c - \gamma \log x $$

%% \newslide{Size distributions}
%% 
%% Basal metabolic rate per unit mass:
%% 
%% \includegraphics[width=0.45\textwidth]{figmetabcc_noname.pdf}
%% \includegraphics[width=0.45\textwidth]{figmetabcc2_noname.pdf}
%% 
%% We like log-log plots...


  \textbf{Size distributions}

{

Earthquake magnitude (Gutenberg Richter law): $P(M) \propto M^{-3}$

Number of war deaths: $P(d) \propto d^{-1.8}$

Sizes of forest fires

Sizes of cities: $P(S) \propto k^{-2.1}$

Number of links to and from websites
}
{\tiny

(Note: Exponents range in error; see M.E.J. Newman arXiv:cond-mat/0412004v3)
}


  \textbf{Size distributions}

{

Number of citations to papers: $P(k) \propto k^{-3}$.

Individual wealth (maybe): $P(W) \propto W^{-2}$.

Distributions of tree trunk diameters: $P(d) \propto d^{-2}$.

The gravitational force at a random point in the universe: $P(F) \propto F^{-5/2}$.

Diameter of moon craters: $P(d) \propto d^{-3}$.

Word freqency: $P(k) \propto k^{-2.2}$.

}
{\tiny
(Note: Exponents range in error; see M.E.J. Newman arXiv:cond-mat/0412004v3)
}


% ??? add exponents


  \textbf{Size distributions}

Such distributions are often called `heavy-tailed'\\
or said to have `fat tails'

\inv

Inverse power laws aren't the only ones:

lognormals, stretched exponentials, ...


  \textbf{Size distributions}

Such distributions are often called `heavy-tailed'\\
or said to have `fat tails'


Inverse power laws aren't the only ones:

lognormals, stretched exponentials, ...


  \textbf{Power law distributions}

\tc{blue}{Gaussians versus power-law distributions.}

\inv

Height versus wealth.

Mild versus Wild (Mandelbrot)

Mediocristan versus Extremistan {\small (See ``The Black Swan'' by Taleb)}



  \textbf{Power law distributions}

Gaussians versus power-law distributions.

\tc{blue}{Height versus wealth.}

\inv

Mild versus Wild (Mandelbrot)

Mediocristan versus Extremistan {\small (See ``The Black Swan'' by Taleb)}


  \textbf{Power law distributions}

Gaussians versus power-law distributions.

Height versus wealth.

\tc{blue}{Mild versus Wild (Mandelbrot)}

\inv

Mediocristan versus Extremistan {\small (See ``The Black Swan'' by Taleb)}



  \textbf{Power law distributions}

Gaussians versus power-law distributions.

Height versus wealth.

Mild versus Wild (Mandelbrot)

\tc{blue}{Mediocristan versus Extremistan {\small (See ``The Black Swan'' by Taleb)}}



  \textbf{Turkeys}

\begin{center}
  \includegraphics{taleb_turkey.pdf}
\end{center}


  \textbf{Taleb's table}

{\small
  \textbf{Mediocristan}/\textbf{Extremistan}
  
  \tc{blue}{Most typical member is mediocre/Most typical is either giant or tiny}

  \inv
  Winners get a small segment/Winner take almost all effects 

  When you observe for a while, you know what's going on/
  It takes a very long time to figure out what's going on
  
  Prediction is easy/Prediction is hard

  History crawls/History makes jumps

  Tyranny of the collective/Tyranny of the accidental
}


  \textbf{Taleb's table}

{\small
  \textbf{Mediocristan}/\textbf{Extremistan}
  
  Most typical member is mediocre/Most typical is either giant or tiny

  \tc{blue}{Winners get a small segment/Winner take almost all effects }

  \inv

  When you observe for a while, you know what's going on/
  It takes a very long time to figure out what's going on
  
  Prediction is easy/Prediction is hard

  History crawls/History makes jumps

  Tyranny of the collective/Tyranny of the accidental
}


  \textbf{Taleb's table}

{\small
  \textbf{Mediocristan}/\textbf{Extremistan}
  
  Most typical member is mediocre/Most typical is either giant or tiny

  Winners get a small segment/Winner take almost all effects 

  \tc{blue}{When you observe for a while, you know what's going on/
  It takes a very long time to figure out what's going on}

  \inv

  Prediction is easy/Prediction is hard

  History crawls/History makes jumps

  Tyranny of the collective/Tyranny of the accidental
}



  \textbf{Taleb's table}

{\small
  \textbf{Mediocristan}/\textbf{Extremistan}
  
  Most typical member is mediocre/Most typical is either giant or tiny

  Winners get a small segment/Winner take almost all effects 

  When you observe for a while, you know what's going on/
  It takes a very long time to figure out what's going on
  
  \tc{blue}{Prediction is easy/Prediction is hard}

  \inv

  History crawls/History makes jumps

  Tyranny of the collective/Tyranny of the accidental
}


  \textbf{Taleb's table}

{\small
  \textbf{Mediocristan}/\textbf{Extremistan}
  
  Most typical member is mediocre/Most typical is either giant or tiny

  Winners get a small segment/Winner take almost all effects 

  When you observe for a while, you know what's going on/
  It takes a very long time to figure out what's going on
  
  Prediction is easy/Prediction is hard

  \tc{blue}{History crawls/History makes jumps}
  
  \inv

  Tyranny of the collective/Tyranny of the accidental
}




  \textbf{Taleb's table}

{\small
  \textbf{Mediocristan}/\textbf{Extremistan}
  
  Most typical member is mediocre/Most typical is either giant or tiny

  Winners get a small segment/Winner take almost all effects 

  When you observe for a while, you know what's going on/
  It takes a very long time to figure out what's going on
  
  Prediction is easy/Prediction is hard

  History crawls/History makes jumps

  \tc{blue}{Tyranny of the collective/Tyranny of the accidental}
}




  \textbf{How samples grow}



  \textbf{Size distributions}

  \textbf{Complementary Cumulative Distribution Function (CCDF):}
    
    
      $$ P_{\ge}(x) = P(x' \ge x)  = 1 - P(x'<x) $$
    
      $$ = \int_{x' = x}^{\infty} P(x') \dee{x'}  $$
    
      $$ \propto \int_{x' = x}^{\infty} (x')^{-\gamma} \dee{x'}  $$
    
      $$\tc{white}{ = \left. \frac{1}{-\gamma+1}(x')^{-\gamma+1} \right|_{x' = x}^{\infty}} $$
    
      $$ \propto x^{-\gamma+1} $$
    
  


  \textbf{Size distributions}

Complementary Cumulative Distribution Function (CCDF):
$$ P_{\ge}(x) \propto x^{-\gamma+1} $$

$\bullet$ Use when tail of $P$ follows a power law.

$\bullet$ Increases exponent by one.

$\bullet$ Useful in cleaning up data.

% \newslide{Size distributions}

% Brown Corpus (1,015,945 words)
% Example

%% ??? insert example of how it does clean up data


  \textbf{Size distributions}

Complementary Cumulative Distribution Function (CCDF):

Discrete variables:
$$ P_\ge(k) = P(k' \ge k) $$
$$ = \sum_{k'=k}^{\infty} P(k) $$
$$ \propto k^{-\gamma+1} $$

Use integrals to approximate sums.


  \textbf{Size distributions}

Connection between CCDF and Zipf distributions:

\tc{blue}{George Kingsley Zipf}
noted various rank distributions\\ 
followed power laws, often with exponent -1\\
(word frequency, city cizes...)

``Human Behaviour and the Principle of Least-Effort''
{\small Addison-Wesley, Cambridge MA, 1949.}


  \textbf{Size distributions}

$x_i$ = the size of the $i$th ranked object.

\inv
$i=1$ corresponds to the largest size.

$s_1$ could be the frequency of occurrence of
the most common word in a text.

Zipf:
$$ x_i \propto i^{-\alpha} $$


  \textbf{Size distributions}

$x_i$ = the size of the $i$th ranked object.

\tc{blue}{$i=1$ corresponds to the largest size.}

\inv

$s_1$ could be the frequency of occurrence of
the most common word in a text.

Zipf:
$$ x_i \propto i^{-\alpha} $$


  \textbf{Size distributions}

$x_i$ = the size of the $i$th ranked object.

$i=1$ corresponds to the largest size.

\tc{blue}{$s_1$ could be the frequency of occurrence of
the most common word in a text.}

\inv
Zipf:
$$ x_i \propto i^{-\alpha} $$



  \textbf{Size distributions}

$x_i$ = the size of the $i$th ranked object.

$i=1$ corresponds to the largest size.

$s_1$ could be the frequency of occurrence of
the most common word in a text.

\tc{blue}{Zipf:}
$$ \tc{blue}{x_i \propto i^{-\alpha}} $$


  \textbf{Size distributions}

Brown Corpus (1,015,945 words)

\includegraphics[width=0.45\textwidth]{figwords1_noname}
\includegraphics[width=0.45\textwidth]{figwords2_noname}

The, of, and, to, a, ...  \#2000 = components


  \textbf{Size distributions}

  \textbf{Observe:}
    
     $ NP_\ge(x) = $ the number of objects with size at least $x$
     If an object has size $x_i$, then $NP_\ge(x_i)$ is its rank $i$.
     So 
      $$\tc{blue}{x_i \propto i^{-\alpha} = (NP_\ge(x_i))^{-\alpha}}$$
     
      $$\tc{blue}{ \propto x_i^{(-\gamma+1)(-\alpha)}}$$
    
  


  \textbf{Size distributions}

Since $P_\ge(x) \sim x^{-\gamma+1}$,
\tc{blue}{$$\alpha = \frac{1}{\gamma-1}$$}

\tc{white}{A rank distribution exponent of $\alpha = -1$ 
corresponds to a size distribution exponent $\gamma=2$.}


  \textbf{Size distributions}

Since $P_\ge(x) \sim x^{-\gamma+1}$,
\tc{blue}{$$\alpha = \frac{1}{\gamma-1}$$}

\tc{black}{A rank distribution exponent of $\alpha = -1$ 
corresponds to a size distribution exponent $\gamma=2$.}


%% \newslide{Moments}
%% 
%% Given an inverse power law distribution:
%% 
%% $$P(x) \sim  c\, x^{-(1+\gamma)}  \ \mbox{for $\xmin < x < \xmax$} $$
%% 
%% let's calculate some basic quantities.
%% 
%\multislide[3]{moments}
%{a}
%{b}
%{c}


  \textbf{Normalization}

  
   
    $$ 1 = \int_{x=\xmin}^{\xmax} P(x) \dee{x} $$
   
    $$ \sim c \int_{x=\xmin}^{\xmax} x^{-\gamma} \dee{x} $$
   
    $$ = \tc{white}{\frac{c}{-\gamma+1}} \left.
      x^{-\gamma+1}
    \right|_{\xmin}^{\xmax}
    $$
   
    $$ = \tc{white}{\frac{c}{-\gamma+1}} \left(
      \xmax^{-\gamma+1}
      -
      \xmin^{-\gamma+1}
    \right).
    $$
   
    \mbox{}\hfill Need $\gamma>1$\\
   
    \mbox{}\hfill Depends only on cutoffs.
  


  \textbf{Moments}

  \textbf{Moments of order $n$:}
    
     
$$ \avg{x^n} = \int_{x=\xmin}^{\xmax} x^n P(x) \dee{x} $$
     
$$ \sim c \int_{x=\xmin}^{\xmax} x^n x^{-\gamma} \dee{x} $$
     
$$ = \tc{white}{\frac{c}{n-\gamma+1}} \left(
  \xmax^{n-\gamma+1} - \xmin^{n-\gamma+1} \right).
$$
    
  


  \textbf{Moments}

$$ \avg{x^n} \sim \frac{c}{n-\gamma+1} \left( \xmax^{n-\gamma+1} - \xmin^{n-\gamma+1} \right). $$

\inv

Mean ($n=1$) blows up with upper cutoff if $\gamma < 2$.

Variance ($n=1,2$) blows up with upper cutoff if $\gamma < 3$.

$n$th moment blows up with upper cutoff if $\gamma < n + 1$.


  \textbf{Moments}

$$ \avg{x^n} \sim \frac{c}{n-\gamma+1} \left( \xmax^{n-\gamma+1} - \xmin^{n-\gamma+1} \right). $$

\tc{blue}{Mean ($n=1$) blows up with upper cutoff if $\gamma < 2$.}

\inv

Variance ($n=1,2$) blows up with upper cutoff if $\gamma < 3$.

$n$th moment blows up with upper cutoff if $\gamma < n + 1$.


  \textbf{Moments}

$$ \avg{x^n} \sim \frac{c}{n-\gamma+1} \left( \xmax^{n-\gamma+1} - \xmin^{n-\gamma+1} \right). $$

Mean ($n=1$) blows up with upper cutoff if $\gamma < 2$.

\tc{blue}{Variance ($n=1,2$) blows up with upper cutoff if $\gamma < 3$.}

\inv

$n$th moment blows up with upper cutoff if $\gamma < n + 1$.


  \textbf{Moments}

$$ \avg{x^n} \sim \frac{c}{n-\gamma+1} \left( \xmax^{n-\gamma+1} - \xmin^{n-\gamma+1} \right). $$

Mean ($n=1$) blows up with upper cutoff if $\gamma < 2$.

Variance ($n=1,2$) blows up with upper cutoff if $\gamma < 3$.

\tc{blue}{$n$th moment blows up with upper cutoff if $\gamma < n + 1$.}


  \textbf{Moments}

All moments depend only on cutoffs, the limits of $x$.

\tc{black}{\ding{228} No internal scale dominates}

\tc{black}{\ding{228} Compare to a Gaussian, exponential, etc.}


  \textbf{Moments}

For many real size distributions:
$$ 2 < \gamma < 3 $$

\tc{black}{\ding{228} mean is finite (depends on lower cutoff)}

\tc{black}{\ding{228} $\sigma^2$ = variance is `infinite' (depends on upper cutoff)}

Width of distribution is `infinite'


  \textbf{Moments}

The variance:

$$ \sigma^2 = \avg{(x-\avg{x})^2} $$
$$ = \int_{\xmin}^{\xmax} (x-\avg{x})^2 P(x) \dee{x} $$
$$ = \tavg{x^2} - \tavg{x}^2 $$



  \textbf{Moments}

\tc{black}{Variance is a social construction!}

\inv
\tc{black}{Variance is nice analytically}

\tc{black}{Another measure of distribution width:}

\tc{black}{Mean average deviation (MAD):}
\tc{black}{$$\avg{\left| x - \avg{x} \right|}$$}


  \textbf{Moments}

\tc{black}{Variance is a social construction!}

\tc{blue}{Variance is nice analytically}

\inv
\tc{black}{Another measure of distribution width:}

\tc{black}{Mean average deviation (MAD):}
\tc{black}{$$\avg{\left| x - \avg{x} \right|}$$}


  \textbf{Moments}

\tc{black}{Variance is a social construction!}

\tc{black}{Variance is nice analytically}

\tc{blue}{Another measure of distribution width:}

\tc{black}{Mean average deviation (MAD):}
\tc{black}{$$\avg{\left| x - \avg{x} \right|}$$}



  \textbf{Moments}

For a pure power law with $2 < \gamma < 3$:
$$\avg{\left| x - \avg{x} \right|} \ \mbox{is finite.}$$

\inv

Still, we say such a distribution has infinite `width'


  \textbf{Moments}

For a pure power law with $2 < \gamma < 3$:
$$\avg{\left| x - \avg{x} \right|} \ \mbox{is finite.}$$


\tc{blue}{Still, we say such a distribution has infinite `width'}


% \newslide{Mechanisms}
% include measure of displacement from mean


  \textbf{Mechanisms}

A powerful theme in complex systems: \\
\tc{blue}{structure arises out of randomness.}

\inv

\tc{red}{Exhibit A:} Random walks...


  \textbf{Mechanisms}

A powerful theme in complex systems: \\
\tc{blue}{structure arises out of randomness.}

\tc{red}{Exhibit A:} Random walks...

  

  \textbf{Random walks}

The essential random walk:

\inv
\tc{white}{\ding{228} One dimension.}

\tc{white}{\ding{228} Time and space are discrete.}

Random walker (e.g., a drunk) starts at origin $x=0$.

Step at time $t$ is $\epsilon_t$:
$$
\epsilon_t = 
\left\{
    \begin{array}{ll}
      +1 & \mbox{with probability 1/2} \\
      -1 & \mbox{with probability 1/2} \\
    \end{array}
\right.
$$


  \textbf{Random walks}

The essential random walk:

\tc{blue}{\ding{228} One dimension.}

\inv

\tc{white}{\ding{228}} Time and space are discrete.

Random walker (e.g., a drunk) starts at origin $x=0$.

Step at time $t$ is $\epsilon_t$:
$$
\epsilon_t = 
\left\{
    \begin{array}{ll}
      +1 & \mbox{with probability 1/2} \\
      -1 & \mbox{with probability 1/2} \\
    \end{array}
\right.
$$


  \textbf{Random walks}

The essential random walk:

\tc{black}{\ding{228}} One dimension.

\tc{blue}{\ding{228} Time and space are discrete.}

\inv

Random walker (e.g., a drunk) starts at origin $x=0$.

Step at time $t$ is $\epsilon_t$:
$$
\epsilon_t = 
\left\{
    \begin{array}{ll}
      +1 & \mbox{with probability 1/2} \\
      -1 & \mbox{with probability 1/2} \\
    \end{array}
\right.
$$


  \textbf{Random walks}

The essential random walk:

\ding{228} One dimension.

\ding{228} Time and space are discrete.

\tc{blue}{Random walker (e.g., a drunk) starts at origin $x=0$.}

\inv

Step at time $t$ is $\epsilon_t$:
$$
\epsilon_t = 
\left\{
    \begin{array}{ll}
      +1 & \mbox{with probability 1/2} \\
      -1 & \mbox{with probability 1/2} \\
    \end{array}
\right.
$$



  \textbf{Random walks}

The essential random walk:

\tc{black}{\ding{228}} One dimension.

\tc{black}{\ding{228}} Time and space are discrete.

Random walker (e.g., a drunk) starts at origin $x=0$.

\tc{blue}{Step at time $t$ is $\epsilon_t$:}
$$
\epsilon_t = 
\left\{
    \begin{array}{ll}
      +1 & \mbox{with probability 1/2} \\
      -1 & \mbox{with probability 1/2} \\
    \end{array}
\right.
$$



  \textbf{Random walks}

\tc{blue}{Displacement after $t$ steps:}
$$x_t = \sum_{i=1}^{t} \epsilon_t$$

\inv

Expected displacement:
\tc{black}{$$\avg{x_t} = \avg{\sum_{i=1}^{t} \epsilon_t}$$}
\tc{black}{$$= \sum_{i=1}^{t} \avg{\epsilon_t}$$}
\tc{black}{$$ = 0$$}


  \textbf{Random walks}

Displacement after $t$ steps:
$$x_t = \sum_{i=1}^{t} \epsilon_t$$

\tc{blue}{Expected displacement:}
\tc{black}{$$\avg{x_t} = \avg{\sum_{i=1}^{t} \epsilon_t}$$}
\tc{black}{$$= \sum_{i=1}^{t} \avg{\epsilon_t}$$}
\tc{black}{$$ = 0$$}


  \textbf{Random walks}

\tc{black}{Variances sum:}
\tc{blue}{$$ \var(x_t) = \var\left( \sum_{i=1}^{t} \epsilon_t \right) $$}
\inv
\tc{black}{$$ = \sum_{i=1}^{t} \var\left(  \epsilon_t \right) $$}
\tc{black}{$$ = \sum_{i=1}^{t} 1 $$}
\tc{black}{$$ = t $$}


  \textbf{Random walks}

\tc{black}{Variances sum:}
\tc{black}{$$ \var(x_t) = \var\left( \sum_{i=1}^{t} \epsilon_t \right) $$}
\tc{blue}{$$ = \sum_{i=1}^{t} \var\left(  \epsilon_t \right) $$}
\inv
\tc{black}{$$ = \sum_{i=1}^{t} 1 $$}
\tc{black}{$$ = t $$}


  \textbf{Random walks}

\tc{black}{Variances sum:}
\tc{black}{$$ \var(x_t) = \var\left( \sum_{i=1}^{t} \epsilon_t \right) $$}
\tc{black}{$$ = \sum_{i=1}^{t} \var\left(  \epsilon_t \right) $$}
\tc{blue}{$$ = \sum_{i=1}^{t} 1 $$}
\inv
\tc{black}{$$ = t $$}


  \textbf{Random walks}

\tc{black}{Variances sum:}
\tc{black}{$$ \var(x_t) = \var\left( \sum_{i=1}^{t} \epsilon_t \right) $$}
\tc{black}{$$ = \sum_{i=1}^{t} \var\left(  \epsilon_t \right) $$}
\tc{black}{$$ = \sum_{i=1}^{t} 1 $$}
\tc{blue}{$$ = t $$}


  \textbf{Random walks}

So the typical displacement from the origin
scales as 
$$\sigma = t^{1/2}$$

\inv

$\Rightarrow$ A non-trivial power-law arises out
of \tc{blue}{additive aggregation} or \tc{blue}{accumulation}


  \textbf{Random walks}

So the typical displacement from the origin
scales as 
$$\sigma = t^{1/2}$$

$\Rightarrow$ A non-trivial power-law arises out
of \tc{blue}{additive aggregation} or \tc{blue}{accumulation}


  \textbf{Random walks}

Random walks are weirder than you might think...

\inv

For example:

$\xi_{r,t}$ = the probability that by time step $t$,
a random walk has crossed the origin $r$ times.

Think of a coin flip game with ten thousand tosses.

If you are behind early on, what are the chances you
will make a comeback?

The most likely number of lead changes is...  
0.

% {\tiny See Feller, Intro to Probability Theory, Volume I } 


  \textbf{Random walks}

Random walks are weirder than you might think...

For example:

$\xi_{r,t}$ = the probability that by time step $t$,
a random walk has crossed the origin $r$ times.

\inv

Think of a coin flip game with ten thousand tosses.

If you are behind early on, what are the chances you
will make a comeback?

The most likely number of lead changes is...  
0.

% {\tiny See Feller, Intro to Probability Theory, Volume I } 



  \textbf{Random walks}

Random walks are weirder than you might think...

For example:

$\xi_{r,t}$ = the probability that by time step $t$,
a random walk has crossed the origin $r$ times.

Think of a coin flip game with ten thousand tosses.

If you are behind early on, what are the chances you
will make a comeback?

\inv 

The most likely number of lead changes is...  
0.

% {\tiny See Feller, Intro to Probability Theory, Volume I } 


  \textbf{Random walks}

Random walks are weirder than you might think...

For example:

$\xi_{r,t}$ = the probability that by time step $t$,
a random walk has crossed the origin $r$ times.

Think of a coin flip game with ten thousand tosses.

If you are behind early on, what are the chances you
will make a comeback?

The most likely number of lead changes is...  
\inv  0.

% {\tiny See Feller, Intro to Probability Theory, Volume I } 


  \textbf{Random walks}

Random walks are weirder than you might think...

For example:

$\xi_{r,t}$ = the probability that by time step $t$,
a random walk has crossed the origin $r$ times.

Think of a coin flip game with ten thousand tosses.

If you are behind early on, what are the chances you
will make a comeback?

The most likely number of lead changes is...  
\tc{blue}{0}.

% {\tiny See Feller, Intro to Probability Theory, Volume I } 


  \textbf{Random walks}

In fact,
$$\xi_{0,t} > \xi_{1,t} > \xi_{2,t} > \cdots $$

\inv 

Even crazier:

The expected time between tied scores = $\infty$!


  \textbf{Random walks}

In fact,
$$\xi_{0,t} > \xi_{1,t} > \xi_{2,t} > \cdots $$

Even crazier:

The expected time between tied scores = $\infty$!


  \textbf{Random walks}

\includegraphics[width=\textwidth]{figrandwalk1_noname.pdf}
\includegraphics[width=\textwidth]{figrandwalk2_noname.pdf}
\includegraphics[width=\textwidth]{figrandwalk3_noname.pdf}


  \textbf{Random walks}

\includegraphics[width=\textwidth]{figrandwalk4_noname.pdf}
\includegraphics[width=\textwidth]{figrandwalk5_noname.pdf}
\includegraphics[width=\textwidth]{figrandwalk6_noname.pdf}

%%%%%%%%%%%%%%%%%%%
% gaussians
% proof of central limit theorem?
% renormalization group approach
% random walks
%%%%%%%%%%%%%%%%%%%


  \textbf{The Don}

  An aside---statistical outliers in sport:

  \begin{center}
    \includegraphics[width=0.7\textwidth]{CricketBattingAverageHistogram.png}
    \includegraphics[width=.07\textwidth]{wikipedia.jpg}
  \end{center}

  Don Bradman's average score in test cricket.


  \textbf{First returns}

  What is the probability that a random walker
  in one dimension returns to the origin
  for the first time after $t$ steps?

  Will our drunkard always return to the origin?

  What about higher dimensions?


  \textbf{First returns}

  Reasons for caring:

  (1) We will find a power-law size distribution
  with an interesting exponent

  (2) Some physical structures may result from random walks

  (3) We'll start to see how different scalings relate to each other


\begin{comment}

  \textbf{First returns}



  \textbf{Normality}

derive using characteristic functions

The normal (or Gaussian) distribution is
$$
P(x) 
=
\frac{1}{\sqrt{2\pi}\sigma}
e^{-\frac{()}{2\sigma^2}}
$$


  \textbf{Normality}

Random walk/binomial derivation



  \textbf{Normality}

Scaling of random walks


  \textbf{Normality}

Deriving the normal distribution using 
the renormalization group approach.

Hierarchical approach


  \textbf{Normality}

Fourier transforms and
characteristic equations


  \textbf{Normality}

Cumulants


  \textbf{RG}

The RG approach works when


  \textbf{Normality}

The RG approach fails when


  \textbf{Normality}

Anderson's 1972 paper `More is Different'
does not suggest RG should work 
very generally!

%% opposite to what sornette says


  \textbf{Percolation}

Percolation example

Real space renormalization


  \textbf{Normality}

Normalization:

%%%%%%%%%%%%%%%%%%%
% levy distributions
% stable distributions
%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%
% mechanisms and reasons for power law distributions
%%%%%%%%%%%%%%%%%%%


  \textbf{Mechanisms}

Generally, heavy-tailed distributions of
part size appears when there is a low cost in combining or growing
of parts.



  \textbf{Mechanisms}

Beware of PLIPLO...

Power law in, power law out.


  \textbf{Mechanisms}

Change of variable:


  \textbf{Mechanisms}

Distribution of the gravitational force:

$$ P(F) \propto F^{-5/2} $$



  \textbf{Mechanisms}

\textbf{Random copying with innovation}

Fundamental type of growth or agglomeration.

First described by G. Udny Yule in 
``a mathematical theory of evolution, based on the 
conclusions of Dr. J. C. Willis, F.R.S.
\textit{Phil. Trans. B.}, \textbf{Vol.} 213, pp. 21--, 1924.

``On a class of skew distribution functions''\\
Herbert Simon, \textit{Biometrika}, \textbf{Vol.} 42, pp. 425--440, 1955.


  \textbf{Measuring exponents}

River network data from my thesis

Show problems with measurement




  \textbf{Measuring exponents}

$\chi^2$ test


  \textbf{Measuring exponents}

Kolmogorov-Smirnov test



  \textbf{Measuring exponents}

Number of sexual partners



  \textbf{Statistical models}

$$y = c (x + x_0)^\alpha$$

$$y = c x^\alpha e^{-x/x_c}$$


  \textbf{Measuring exponents}

Finite size scaling


  \textbf{Measuring exponents}

Breaks in scaling



  \textbf{Scaling in nature}

Stefan-Boltzmann relation for radiated energy:

$$\diff{E}{t} = \sigma S T^4$$



  \textbf{Physics}

Critical Phenomena in physics



  \textbf{Examples}

Random walks in one dimension:\\
typical displacement $x \propto$ (time)$^{1/2}$


(Essence of the central limit theorem)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% scaling in math
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


  \textbf{Random walks}

Scaling from randomness:



$$\tavg{x} \propto t^{1/2}$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% mechanisms and explanations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


  \textbf{Dimensional Analysis}

Dimensional analysis:


  \textbf{Cleverness}

Turbulence

Atomic bomb


  \textbf{$\pi$ theorem}

Buckingham's $\pi$ theorem (1914).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fractals
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



  \textbf{Geometry}

Okay, okay, okay: `fractals.'



  \textbf{Benford's law}


  \textbf{Turbulence}

% http://www.efluids.com/efluids/gallery/gallery_pages/jet_cfd_page.jsp

\includegraphics[width=0.48\textwidth]{jet_cfd.jpg}
\raisebox{17ex}{
    \parbox{.48\textwidth}{
      \small
      Big whirls have little whirls\\
      That heed on their velocity, \\
      And little whirls have littler whirls \\
      And so on to viscosity.

      \hfill---Lewis Richardson
      
      ??? laws
    }
  }

%% [A play on Jonathan Swift's "Great fleas have little fleas upon their backs to bite 'em, And little fleas have lesser fleas, and so ad infinitum." (1733)].


  \textbf{Examples}

Scaling in elementary laws of physics:

Inverse-square law of gravity
and Coulomb's law: 
$$F \propto \frac{m_1 m_2}{r^{2}}
\quad \mbox{and} \quad
F \propto \frac{q_1 q_2}{r^{2}}$$

$\Rightarrow$ Force is diminished by expansion of
space away from source.  

(The square is $d-1=3-1=2$, the dimension of
a sphere's surface.)


% check out http://complexsystems.lri.fr/Main/tiki-index.php


%% \newslide{Measuring exponents}
%% 
%% Ordinary least squares (OLS) linear regression
%% 
%% \newslide{Measuring exponents}
%% 
%% Check residuals obey a normal distribution
%% 
%% \newslide{Measuring exponents}
%% 
%% Define lognormal
%% 
%% 
%% \newslide{Measuring exponents}
%% 
%% Major Axis (RMA) linear regression
%% 
%% 
%% aka Standardized Major Axis and 
%% originally Reduced Major Axis.
%% 
%% citation
%% 
%% Raynor?


%% give explanation for white/grey matter scalin


  \textbf{Allometry}

White matter = connections\\
Grey matter = computation units

\begin{center}
\includegraphics[height=0.7\textheight]{zhang2000fig2.jpg}  
\end{center}

\hfill{\tiny(from Zhang \& Sejnowski, PNAS, 2000)}


  \textbf{Examples}

Explanation

\hfill{\tiny(from Zhang \& Sejnowski, PNAS, 2000)}



  \textbf{Examples}

1/f noise



\begin{comment}

percolation

\end{comment}


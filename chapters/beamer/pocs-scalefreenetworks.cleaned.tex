%% something i completely agree with from socnet (mason porter):
%%
%% The term 'scale free' is a very unfortunate (and misleading) term that was spread based on the terminology choice of some individuals.  (Some people on this mailing
%% list who know me are probably laughing their heads off and expecting me to go into one of my usual rants.  I'll spare you this time.)  It is used to refer to networks
%% with degree distributions given by power laws (or, for real data, have power law tails) because a commonality of these sorts of scaling laws (which is different from
%% scales, by the way, despite the similar terminology) one finds in fractal structures.  The thing is that networks with those degree distributions _can_ have scales,
%% as discussed in gory detail by John Doyle and collaborators.  It's better to just state the power law aspect of it when one's network has that feature and not comment
%% on whether or not there are scales unless one goes beyond the degree distribution.
%% 
%% One of Doyle's papers that discusses this is here: http://arxiv.org/abs/cond-mat/0501169
%% 
%% I hope this helps.

\section{Original model}

\subsection{Introduction}

\begin{frame}[label=]
 \textbf{Scale-free networks}
 
 
  
   Networks with power-law degree distributions
   have become known as \alert{scale-free} networks.
 
   Scale-free refers specifically to the \alert{degree distribution}
   having a \alert{power-law decay} in its tail:
   $$
   {
     P_k \sim k^{-\gamma} 
     \mbox{\ for `large' $k$}
   }
   $$
 
   One of the seminal works in complex networks:\\
   Laszlo Barab\'{a}si and Reka Albert, Science, 1999:\\
   \alertb{``Emergence of scaling in random networks''}\cite{barabasi1999a}\\
 
   Somewhat misleading nomenclature...
 


\begin{frame}[label=]
  \textbf{Scale-free networks}

  
  
    Scale-free networks are \alert{not fractal} in any sense.
  
    Usually talking about networks whose links are
    \alertb{abstract}, 
    \alert{relational}, 
    \alertb{informational}, 
    \ldots (non-physical)
  
    Primary example: hyperlink network of the Web
  
    Much arguing about whether or networks are `scale-free' or not\ldots
  


\input{nw_powerlaw_graphviz03giants}

\begin{frame}[label=]
 \textbf{Scale-free networks}

 \textbf{The big deal:}
   
   
     We move beyond describing of networks
     to finding \alert{mechanisms} for why
     certain networks are the way they are.
   
 

 \textbf{A big deal for scale-free networks:}
   
   
     How does the exponent $\gamma$ depend on the mechanism?
   
     Do the mechanism details matter?
   
 
 

  \textbf{Heritage}

  \textbf{Work that presaged scale-free networks}
    
     
      1924: \alert{G. Udny Yule}\cite{yule1924a}:\\ \# Species per Genus
     
      1926: \alert{Lotka}\cite{lotka1926a}:\\ \# Scientific papers per author
      
      1953: \alert{Mandelbrot}\cite{mandelbrot1953a}):\\
      Zipf's law for word frequency through optimization
     
      1955: \alert{Herbert Simon}\cite{simon1955a,zipf1949a}:\\ Zipf's law, 
      city size, income, publications, and species per genus
     
      1965/1976: \alert{Derek de Solla Price}\cite{price1965a,price1976a}:\\ Network of Scientific Citations
    
  


\subsection{Model details}

\begin{frame}[label=]
 \textbf{BA model}  
 
 
  
   Barab\'{a}si-Albert model = BA model.
  
   Key ingredients:\\
   \alert{Growth} and \alertb{Preferential Attachment} (PA).
  
   \alert{Step 1}: start with $m_0$ disconnected nodes.
  
   \alert{Step 2}: 
   
     
     \alertb{Growth}---a new node appears at each time step $t=0,1,2, \ldots$.
    
     Each new node makes $m$ links to nodes already present.
    
     \alertb{Preferential attachment}---Probability 
     of connecting to $i$th node is $\propto k_i$.
   
 
   In essence, we have a \alert{rich-gets-richer} scheme.
 


\subsection{Analysis}

\begin{frame}[label=]
 \textbf{BA model}  
 
 
 
   \alert{Definition:} $A_k$ is the \alertb{attachment kernel}
   for a node with degree $k$.
 
     For the original model:
     $$ A_k = k$$
 
   \alert{Definition:} $P_{\textnormal{attach}}(k,t)$ 
   is the attachment probability.
 
     For the original model:
     $$
     P_{\textnormal{attach}}(\mbox{node $i$},t)
     =
     \frac{k_i(t)}
     {
     \sum_{j=1}^{N(t)} k_j(t)
     }
     {
       =
       \frac{k_i(t)}
       {
         \sum_{k=m}^{k_{\textnormal{max}}(t)} k N_k(t)
       }
     }
     $$
     {
       where $N(t) = m_0 + t$ is \# nodes at time $t$\\
     }
     {
       \ \ and $N_k(t)$ is \# degree $k$ nodes at time $t$.
     }
 


\begin{frame}[label=]
 \textbf{Approximate analysis}  

 
 
   When $(N+1)$th node is added, 
   the expected increase in the degree of node $i$ is 
   $$
   E(k_{i,N+1} - k_{i,N}) 
   \simeq 
   m
   \frac{k_{i,N}}
   {
     \sum_{j=1}^{N(t)} k_j(t)
   }.
   $$
 
   Assumes probability of being connected to is \alert{small}.
 
   Dispense with Expectation by assuming (hoping)
   that over longer time frames, degree growth will
   be smooth and stable.
 
   Approximate 
   $k_{i,N+1} - k_{i,N}$ with $\diff{}{t} k_{i,t}$:
   {
     $$
     \alertb{
       \diff{}{t} k_{i,t}
       =
       m
       \frac{k_{i}(t)}
       {
         \sum_{j=1}^{N(t)} k_j(t)
       }
     }
   $$
   where $t = N(t) - m_0$.
   }
 

\begin{frame}[label=]
 \textbf{Approximate analysis}  

 
 
   Deal with denominator: each added node brings $m$ new edges.
   $$
   {
     \therefore 
     \sum_{j=1}^{N(t)} k_j(t)
     =
     2 t m
   }
   $$
 
   The node degree equation now simplifies:
   $$
   \diff{}{t} k_{i,t}
   =
   m
   \frac{k_{i}(t)}
   {
     \sum_{j=1}^{N(t)} k_j(t)
   }
   {
     =
     m
     \frac{k_{i}(t)}
     {
       2 m t
     }
   }
   {
     =
     \frac{1}
     {
       2 t
     }
     k_{i}(t)
   }
   $$
 
   Rearrange and solve:
   $$
   \frac{\dee{k_{i}(t)}}
   {k_{i}(t)}
   =
   \frac{\dee{t}}
   {2t}
   {
     \Rightarrow
     \boxed{\alert{k_i(t) = c_i \,  t^{1/2}.}}
   }
   $$
 
   Next find $c_i$ \ldots
 


\begin{frame}[label=]
 \textbf{Approximate analysis}  

 
 
   Know $i$th node appears at time 
   $$
   t_{i,\textnormal{start}} = \left\{
     \begin{array}{ll}
       i-m_0 & \mbox{for $i>m_0$} \\
       0 & \mbox{for $i \le m_0$}
     \end{array}
   \right.
   $$
 
   So for $i>m_0$ (exclude initial nodes),
   we must have
   $$
   k_i(t) 
   = 
   m 
   \left(
   \frac{t}{t_{i,\textnormal{start}}}
   \right)^{1/2}
   \
   \mbox{for $t \ge t_{i,\textnormal{start}}$}.
   $$
  
   All node degrees grow as \alert{$t^{1/2}$}
   {but
   later nodes have larger $t_{i,\textnormal{start}}$ which
   \alertb{flattens out} growth curve.}
  
   Early nodes do \alert{best} (First-mover advantage).
 
 

\begin{frame}[label=]
 \textbf{Approximate analysis}  

    
   \includegraphics[width=\textwidth]{figsfn_deggrowth_noname}
   
   
    $m = 3$
    $t_{i,\textnormal{start}} = 1, 2, 5, \mbox{\ and\ } 10$.
   
 

\begin{frame}[label=]
 \textbf{Degree distribution}
 
 
  
   So what's the \alert{degree distribution} at time $t$?
  
   Use fact that birth time for added nodes
   is distributed uniformly:
   $$
   P(t_{i,\textnormal{start}}) \dee{t_{i,\textnormal{start}}}
   \simeq
   \frac{\dee{t_{i,\textnormal{start}}}}
   {t+m_0}
   $$
 
   Using
   $$
   k_i(t) 
   = 
   m 
   \left(
   \frac{t}{t_{i,\textnormal{start}}}
   \right)^{1/2}
   \alert{\Rightarrow}
   t_{i,\textnormal{start}}
   = \frac{m^2 t}{k_i(t)^2}.
   $$
   {
     and by understanding that later arriving nodes have lower degrees,
     we can say this:
     $$
     \Prob(k_i < k)
     =
     \Prob(t_{i,\textnormal{start}} > \frac{m^2 t}{k^2}).
     $$
   }

 


\begin{frame}[label=]
  \textbf{Degree distribution}

  
   
    Using the uniformity of start times:
    $$
    \Prob(k_i < k)
    =
    \Prob(t_{i,\textnormal{start}} > \frac{m^2 t}{k^2})
    \simeq
    \frac{\alert{t-\frac{m^2 t}{k^2}}}{\alertb{t+m_0}}.
    $$
   
    Differentiate to find $\Prob(k)$:
    $$
    \Prob(k)
    = 
    \diff{}{k} \Prob(k_i < k)
    {
      =
      \frac{2m^2 t }{(t+m_0) k^3}
    }
    $$
    $$
    {
      \sim 
      \alert{2m^2} \alertb{k^{-3}}
      \mbox{\ as $m\rightarrow \infty$}.
    }
    $$
  


\begin{frame}[label=]
 \textbf{Degree distribution}

 
   
    We thus have a very specific prediction of 
    \alertb{$\Prob(k) \sim k^{-\gamma}$} with \alert{$\gamma=3$}.
   
    Typical for real networks: \alert{$2 < \gamma < 3$}.
   
    Range true more generally for events with size
    distributions that have power-law tails.
  
    \alert{$2 < \gamma < 3$}: finite mean and `infinite' variance
    {\alert{(wild)}}
  
    In practice, $\gamma < 3$ means variance is governed
    by upper cutoff.
  
    \alert{$\gamma > 3$}: finite mean and variance 
    {\alert{(mild)}}
 


\begin{frame}[label=]
 \textbf{Examples}  

  \rowcolors[]{1}{blue!20}{blue!10} 
  \begin{center}
    \begin{tabular}{rl}
      WWW & $\gamma \simeq 2.1$ for in-degree \\
      WWW & $\gamma \simeq 2.45$ for out-degree \\
      Movie actors & $\gamma \simeq 2.3$ \\
      Words (synonyms) & $\gamma \simeq 2.8$ \\
    \end{tabular}
  \end{center}

  {
    The Internet\alert{s} is a different business...
  }


\begin{frame}[label=]
 \textbf{Real data}  

 From Barab\'{a}si and Albert's original paper\cite{barabasi1999a}:

 \includegraphics[width=\textwidth]{barabasi1999a_fig1}
 

\begin{frame}[label=]
 \textbf{Things to do and questions}  

 
  Vary attachment kernel.
  Vary mechanisms:
   
    Add edge deletion
    Add node deletion
    Add edge rewiring
   
  Deal with directed versus undirected networks.
  \alert{Important Q.}:
   Are there distinct universality classes for these networks?  
 
   \alert{Q.}:
   How does changing the model affect $\gamma$?
 
   \alert{Q.}:
   Do we need preferential attachment and growth?
 
   \alert{Q.}:
   Do model details matter?
 
   The answer is (surprisingly) \alert{yes}.
 


\subsection{A more plausible mechanism}

\begin{frame}[label=]
 \textbf{Preferential attachment}  
 
 
  
   Let's look at preferential attachment \alert{(PA)}
   a little more closely.
  
   PA implies arriving nodes have \alert{complete knowledge}
   of the existing network's degree distribution.
 
   For example: If \alertb{$P_{\textnormal{attach}}(k) \propto k$}, we need to determine
   the constant of proportionality.
 
   We need to know what everyone's degree is...
 
   PA is $\therefore$ an \alertb{outrageous} assumption of
   node capability.
 
   But a \alert{very simple mechanism} saves the day\ldots
 


\begin{frame}[label=]
 \textbf{Preferential attachment through randomness}  

 
  Instead of attaching preferentially, allow
   new nodes to attach randomly.
  Now add an \alert{extra step}: new nodes
   then connect to some of their friends' friends.
  Can also do this \alert{at random}.
  We know that friends are \alertb{weird}...
  Assuming the existing network is random,
   we know probability of a \alert{random friend} having
   degree \alertb{$k$} is 
   $$Q_k \propto kP_k$$
  So \alert{rich-gets-richer} scheme can
   now be seen to work in a natural way.
 


\subsection{Robustness}

\begin{frame}[label=]
 \textbf{Robustness}  

 
 
   We've looked at some aspects of 
   contagion on scale-free networks:
   
    
     Facilitate disease-like spreading.
    
     Inhibit threshold-like spreading.
   
 
   Another simple story concerns \alert{system robustness}.
 
   Albert et al., Nature, 2000:\\
   \alertb{``Error and attack tolerance of complex networks''}\cite{albert2000a}
 



\begin{frame}[label=]
 \textbf{Robustness}  

 
  \alertb{Standard random networks} (\erdosrenyi)\\
   versus\\
   \alert{Scale-free networks}
 

 \includegraphics[width=\textwidth]{albert2000a_fig1}
 {\tiny from Albert et al., 2000}


\begin{frame}[label=]
 \textbf{Robustness}  

    
   \includegraphics[width=\textwidth]{albert2000a_fig2}
   {\tiny from Albert et al., 2000}
   
   
    
     Plots of network diameter as a function of fraction of nodes removed
    
     \erdosrenyi\ versus scale-free networks
   
   \alertb{blue symbols} = \\
   {random} removal
   
   \alert{red symbols} = \\
   {targeted} removal \\
   (most connected first)
   

 


\begin{frame}[label=]
  \textbf{Robustness}  
  
  
  
    Scale-free networks are thus \alert{robust to random failures}
    yet \alertb{fragile to targeted ones}.
   
    All very reasonable: \alertb{Hubs} are a big deal.
   
    \alert{But}: next issue is whether hubs are vulnerable or not.
   
    Representing all webpages as the same size node is obviously
    a stretch (e.g., google vs. a random person's webpage)
   
    Most connected nodes are either:
    
    
      Physically larger nodes that may be harder to `target'
    
      or subnetworks of smaller, normal-sized nodes.
    
   
    Need to explore cost of various targeting schemes.
  
  


\section{Redner \& Krapivisky's model}

\subsection{Generalized model}


\begin{frame}[label=]
  \textbf{Generalized model}  

  \textbf{Fooling with the mechanism:}
    
    
      2001: Redner \& Krapivsky (RK)\cite{krapivsky2001a}
      explored the \alert{general attachment kernel}:
      {
      $$
      \Prob(\mbox{attach to node $i$}) \propto A_k  = k_{i}^\nu
      $$
      where $A_k$ is the attachment kernel and $\nu>0$.
      }
    
      RK also looked at changing the details of
      the attachment kernel.
    
      We'll follow RK's approach using
      \wordwikilink{http://en.wikipedia.org/wiki/Rate_equation}{rate equations}.
    
  


  \textbf{Generalized model}

  
  
      Here's the set up:
      $$
      \diff{N_k}{t}
      =
      \frac{1}{A}
      \left[
        A_{k-1} N_{k-1} - A_{k} N_{k}
      \right]
      + \delta_{k1}
      $$
      where $N_k$ is the number of nodes of degree $k$.
    
     
      The \alertb{first term} corresponds to 
      degree $k-1$ nodes becoming degree $k$ nodes.
     
      The \alertb{second term} corresponds to 
      degree $k$ nodes becoming degree $k-1$ nodes.
    
      Detail: $A_0=0$
     One node is added per unit time.
     Seed with some initial network\\
      {(e.g., a connected pair)}
    
  

\subsection{Analysis}

\begin{frame}[label=]
  \textbf{Generalized model}  
  
  
  
    In general, probability of attaching to a \alert{specific node}
    of degree $k$ at time $t$ is
    {
    $$
    \Prob(\mbox{attach to node $i$}) 
    = 
    \frac{A_k}{A(t)}
    $$
    }
    {
    where
    $ A(t) = \sum_{k=1}^{\infty} A_k N_k(t).$
    }
  
    E.g., for BA model, $A_k = k$ and $A = \sum_{k=1}^{\infty} k N_k(t)$.
   For $A_k=k$, we have
    $$
    {
    A(t) 
    = 
    \sum_{k'=1}^{\infty}
    k' N_{k'}(t)
    }
    {
    = 2t     
    }
    $$
    {
      since one edge is being added per unit time.
    }
  
      Detail: we are ignoring initial seed network's edges.
  



\begin{frame}[label=]
  \textbf{Generalized model}  

  
  
    So now
    $$
    \diff{N_k}{t}
    =
    \frac{1}{A}
    \left[
      A_{k-1} N_{k-1} - A_{k} N_{k}
    \right]
    + \delta_{k1}
    $$
    becomes
    $$
    \diff{N_k}{t}
    =
    \frac{1}{2t}
    \left[
      (k-1) N_{k-1} - k N_{k}
    \right]
    + \delta_{k1}
    $$
  
    As for BA method, look for steady-state growing solution:
    {
      $
      \alert{N_k = n_k t}.
      $
    }
  
    We replace $\tdiff{N_k}{t}$ with $\tdiff{n_k t}{t} = n_k$.
  
    We arrive at a difference equation:
    $$
    n_k
    =
    \frac{1}{2\cancel{\alert{t}}}
    \left[
      (k-1) n_{k-1}\cancel{\alert{t}} - k n_{k}\cancel{\alert{t}}
    \right]
    + \delta_{k1}
    $$
  


  \textbf{Generalized model}

  
  
    Rearrange and simply:
    $$
    n_k
    =
    \frac{1}{2}
    (k-1) n_{k-1} 
    - 
    \frac{1}{2}
    k n_{k}
    + \delta_{k1}
    $$
    {
      $$
      \Rightarrow
      (k+2)n_k
      =
      (k-1) n_{k-1} 
      + 2\delta_{k1}
      $$
    }
  
    Two cases:
    {
      $$
      \alert{k=1:}
      n_1 = 2/3
      \mbox{\ since $n_0=0$}
      $$
    }
    {
      $$
      \alert{k>1:}
      n_k
      =
      \frac{(k-1)}{k+2} n_{k-1} 
      $$
    }
  
  

  \textbf{Generalized model}

  
   Now find $n_k$:
    $$
    \alert{k>1:}
    n_k
    =
    \frac{(k-1)}{k+2} n_{k-1} 
    {
      =
      \frac{(k-1)}{k+2}
      \frac{(k-2)}{k+1} n_{k-2} 
    }
    $$
    $$
    {
      =
      \frac{(k-1)}{k+2} 
    \frac{(k-2)}{k+1} 
    \frac{(k-3)}{k} n_{k-3} 
    }
    $$
    $$
    {
      =
      \frac{(k-1)}{k+2} 
      \frac{(k-2)}{k+1} 
      \frac{(k-3)}{k} 
      \frac{(k-4)}{k-1} n_{k-4} 
    }
    $$%
    \begin{overprint}%
      %
      $$
      =
      \frac{(k-1)}{k+2} 
      \frac{(k-2)}{k+1} 
      \frac{(k-3)}{k} 
      \frac{(k-4)}{(k-1)} 
      \frac{(k-5)}{(k-2)} 
      \frac{\cdots}{\cdots}
      \frac{5}{8}
      \frac{4}{7}
      \frac{3}{6}
      \frac{2}{5}
      \frac{1}{4}
      n_1
      $$
      
      $$
      =
      \frac{\cancel{(k-1)}}{k+2} 
      \frac{(k-2)}{k+1} 
      \frac{(k-3)}{k} 
      \frac{(k-4)}{\cancel{(k-1)}}
      \frac{(k-5)}{(k-2)} 
      \frac{\cdots}{\cdots}
      \frac{5}{8}
      \frac{\cancel{4}}{7}
      \frac{3}{6}
      \frac{2}{5}
      \frac{1}{\cancel{4}}
      n_1
      $$
      
      $$
      =
      \frac{\cancel{(k-1)}}{k+2} 
      \frac{\cancel{(k-2)}}{k+1} 
      \frac{{(k-3)}}{k} 
      \frac{{(k-4)}}{\cancel{(k-1)}}
      \frac{{(k-5)}}{\cancel{(k-2)}}
      \frac{\cdots}{\cdots}
      \frac{\cancel{5}}{{8}}
      \frac{\cancel{4}}{{7}}
      \frac{3}{{6}}
      \frac{2}{\cancel{5}}
      \frac{1}{\cancel{4}}
      n_1
      $$
      
      $$
      =
      \frac{\cancel{(k-1)}}{k+2} 
      \frac{\cancel{(k-2)}}{k+1} 
      \frac{\cancel{(k-3)}}{k} 
      \frac{\cancel{(k-4)}}{\cancel{(k-1)}}
      \frac{\cancel{(k-5)}}{\cancel{(k-2)}}
      \frac{\cdots}{\cdots}
      \frac{\cancel{5}}{\cancel{8}}
      \frac{\cancel{4}}{\cancel{7}}
      \frac{3}{\cancel{6}}
      \frac{2}{\cancel{5}}
      \frac{1}{\cancel{4}}
      n_1
      $$
        $$
    {
      \Rightarrow \alert{n_k} = 
      \frac{6}{k(k+1)(k+2)} n_1
    }
    {
      =
      \frac{4}{k(k+1)(k+2)}
    }
    {
      \alert{\sim k^{-3}}
    }
    $$
    
  

\subsection{Universality?}

  \textbf{Universality?}

  
  
    As expected, we have the same result as for the BA model:
    $$
    N_k(t) = n_k(t) t \propto k^{-3} \mbox{\ for large $k$}.
    $$
  
    Now: what happens if we start playing around with 
    the attachment kernel $A_k$?
  
    Again, is the result $\gamma=3$
    \wordwikilink{http://en.wikipedia.org/w/index.php?title=Universality_\%28dynamical_systems\%29&oldid=204738455}{universal}?
  
    Natural modification: $A_k = k^\nu$ with $\nu \ne 1$.
  
    But we'll first explore a more subtle modification of $A_k$ made by Redner/Krapivsky\cite{krapivsky2001a}
  
    Keep $A_k$ \alertb{linear in $k$} but tweak details.
  
    \alert{Idea:} Relax from $A_k = k$ to $A_k \sim k$ as $k \rightarrow \infty$.
  


  \textbf{Universality?}
  
  
  
    Recall we used the normalization:
    $$
    A(t) 
    = 
    \sum_{k'=1}^{\infty}
    k' N_{k'}(t)
    \simeq 2t
    \mbox{\ for large $t$.}
    $$
  
    We now have 
    $$
    A(t) 
    = 
    \sum_{k'=1}^{\infty}
    A_{k'} N_{k'}(t)
    $$
    {
    where we only know the asymptotic behavior of $A_k$.
    }
  
    We assume that \alert{$A = \mu t$}
  
    We'll find $\mu$ later and make sure that our
    assumption is consistent.
  
    As before, also assume $N_k(t) = n_k t$.
  


  \textbf{Universality?}

  
  
    For $A_k = k$ we had 
    $$
    n_k
    =
    \frac{1}{2}
    \left[
      (k-1) n_{k-1} - k n_{k}
    \right]
    + \delta_{k1}
    $$
  
    This now becomes
        $$
        n_k
    =
    \frac{1}{\mu}
    \left[
      A_{k-1} n_{k-1} - A_k n_{k}
    \right]
    + \delta_{k1}
    $$
    {
      $$
      \Rightarrow
      (A_k+\mu)n_k
      =
      A_{k-1} n_{k-1} 
      + \mu \delta_{k1}
      $$
    }
  
    Again two cases:
    {
      $$
      \alert{k=1:}
      n_1 = \frac{\mu}{\mu+A_1}.
      $$
    }
    {
      $$
      \alert{k>1:}
      n_k
      =
      n_{k-1}\frac{A_{k-1}}{\mu + A_k}
      $$
    }

  

  \textbf{Universality?}

  
  
    Dealing with the $k>1$ case:
    $$
    n_k
    =
    n_{k-1}
    \frac{A_{k-1}}{\mu + A_k}
    {
      =
      n_{k-1}
      \frac{A_{k-1}}{A_k}
      \frac{1}{1 + \frac{\mu}{A_k}}
      $$
    }
          
      $$
      =
      \alert{n_{k-2}
        \frac{A_{k-2}}{A_{k-1}}
        \frac{1}{1 + \frac{\mu}{A_{k-1}}}}
      \frac{A_{k-1}}{A_k}
      \frac{1}{1 + \frac{\mu}{A_k}}
      $$
      
      $$
      =
      n_{k-2}
      \frac{A_{k-2}}{\cancel{A_{k_1}}}
      \frac{1}{1 + \frac{\mu}{A_{k-1}}}
      \frac{\cancel{A_{k-1}}}{A_k}
      \frac{1}{1 + \frac{\mu}{A_k}}
      $$
        {
      $$
      =
      n_{1}
      \frac{A_{1}}{A_k}
      \prod_{j=2}^{k}
      \frac{1}{1 + \frac{\mu}{A_j}}
      $$
    }
    {
      $$
      =
      n_{1}
      \frac{A_{1}}{A_k}
      \left(1 + \frac{\mu}{A_1}\right)
      \prod_{j=\alert{1}}^{k}
      \frac{1}{1 + \frac{\mu}{A_j}}
      $$
    }
    {
      $$
      =
      \alert{\frac{\mu}{A_k}}
      \prod_{j=1}^{k}
      \frac{1}{1 + \frac{\mu}{A_j}}
      \mbox{\ since $n_1 = \mu/(\mu+A_1)$}
      $$
    }
  


  \textbf{Universality?}

  
  
    Time for pure excitement: Find \alert{asymptotic behavior} 
    of $n_k$ given $A_k \rightarrow k$ as $k \rightarrow \infty$.
  
    For large $k$:
    $$
    n_k = 
    \frac{\mu}{A_k}
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    {
      =
      \frac{\mu}{A_k}
      \prod_{j=1}^{k}
      \frac{A_j}{A_j + \mu}
    }
    $$
    {
    $$
    =
      \frac{\mu}{\alert{\cancel{A_k}}}
      \frac{A_1}{(A_1+\mu)}
      \frac{A_2}{(A_2+\mu)}
      \cdots
      \frac{k-1}{(k-1+\mu)}
      \frac{\alert{\cancel{k}}}{(k+\mu)}
    $$
    }
    $$
    {
      \propto
      \frac{\Gamma(k)}{\Gamma(k + \mu + 1)}
    }
    {
      \sim
      \frac{\sqrt{2\pi} k^{k+1/2} e^{-k}}
      {\sqrt{2\pi} (k+\mu+1)^{k+\mu+1+1/2}e^{-(k+\mu+1)}}
    }
    $$
    $$
    {
      \alert{ \propto k^{-\mu-1} }
    }
    $$
  
    Since $\mu$ depends on $A_k$, \alert{details matter...}
  


  \textbf{Universality?}

  
  
    Now we need to find $\mu$.
  
    Our assumption again:
    $
    \alertb{A = \mu t = \sum_{k=1}^{\infty} N_k(t) A_k}
    $
  
    Since $N_k = n_k t$, we have the simplification
    $
    \alertb{
    \mu = \sum_{k=1}^{\infty} n_k A_k}
    $
  
    Now subsitute in our expression for $n_k$:
          
    $$
    \mu = 
    \sum_{k=1}^{\infty} 
    \alert{
      \frac{\mu}{A_k}
      \prod_{j=1}^{k}
      \frac{1}{1 + \frac{\mu}{A_j}}
    }
    A_k
    $$
    
    $$
    \mu = 
    \sum_{k=1}^{\infty} 
    \frac{\mu}{\alert{\cancel{A_k}}}
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    \alert{\cancel{A_k}}
    $$
          
    $$
    \alertb{1 \cancel{\mu}} = 
    \sum_{k=1}^{\infty} 
    \frac{\alertb{\cancel{\mu}}}{\alert{\cancel{A_k}}}
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    \alert{\cancel{A_k}}
    $$
        
      Closed form expression for $\mu$.
    
      We can solve for $\mu$ in some cases.
    
      Our assumption that $A = \mu t$ is okay.
  


  \textbf{Universality?}

  
  
    Amazingly, we can adjust $A_k$ and tune $\gamma$
    to be anywhere in $[2,\infty)$.
  
    $\gamma = 2$ is the lower limit since
    $$
    \mu = 
    \sum_{k=1}^{\infty} A_k n_k 
    \sim
    \sum_{k=1}^{\infty} k n_k
    $$
    must be finite.
  
    Let's now look at a specific example of $A_k$
    to see this range of $\gamma$ is possible.
  

  \textbf{Universality?}

  
  
    Consider $A_1 = \alpha$ and $A_k = k$ for $k \ge 2$.
   
    Find $\gamma = \mu + 1$ by finding $\mu$.
  
    Expression for $\mu$: 
    $$
    1 =
    \sum_{k=1}^{\infty} 
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    $$
    {
    $$
    1 =
    \frac{1}{1 + \frac{\mu}{A_1}}
    +
    \sum_{k=2}^{\infty} 
    \prod_{j=1}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    $$
    }
    {
    $$
    1 -
    \frac{1}{1 + \frac{\mu}{A_1}}
    =
    \frac{1}{1 + \frac{\mu}{A_1}}
    \sum_{k=2}^{\infty} 
    \prod_{j=\alert{2}}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    $$
    }
    {
    $$
    \frac{\frac{\mu}{\alpha}}{1 + \frac{\mu}{\alpha}}
    =
    \frac{1}{1 + \frac{\mu}{\alpha}}
    \sum_{k=2}^{\infty} 
    \prod_{j=\alert{2}}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    \mbox{\ since $A_1=\alpha$}
    $$
    }
  


  \textbf{Universality?}
  
  
   Carrying on:
    $$
    \frac{\frac{\mu}{\alpha}}{\cancel{\alert{1 + \frac{\mu}{\alpha}}}}
    =
    \frac{1}{\cancel{\alert{1 + \frac{\mu}{\alpha}}}}
    \sum_{k=2}^{\infty} 
    \prod_{j=\alert{2}}^{k}
    \frac{1}{1 + \frac{\mu}{A_j}}
    $$
    $$
    {
      \frac{\mu}{\alpha}
      =
      \sum_{k=2}^{\infty} 
      \frac{\Gamma(k+1)\Gamma(2+\mu)}{\Gamma(k+\mu+1)}
    }
    $$
  
    Now use result that\cite{krapivsky2001a}
    $$
    \sum_{k=2}^{\infty}
    \frac{\Gamma(a+k)}{\Gamma(b+k)}
    =
    \frac{\Gamma(a+2)}{(b-a-1)\Gamma(b+1)}
    $$
    with $a=1$ and $b=\mu+1$.
  
    $$
    \mu = \alpha \frac{\Gamma(3)}{(\mu+1-1-1)\Gamma(2+\mu)} \Gamma(2+\mu)
    $$
    {
      $$
      \Rightarrow \mu(\mu-1) = 2\alpha
      $$
    }
  


  \textbf{Universality?}

  
   
    $$
    \mu(\mu-1) = 2\alpha \Rightarrow \mu = \frac{1+\sqrt{1+8\alpha}}{2}.
    $$
   Since $\gamma = \mu+1$, we have
    $$
    0 \le \alpha < \infty \Rightarrow 2 \le \gamma < \infty
    $$
   Craziness...    
  


\subsection{Sublinear attachment kernels}

  \textbf{Sublinear attachment kernels}

  
   
    Rich-get-somewhat-richer:
    $$
    A_k \sim k^\nu
    \mbox{\ with $0 < \nu < 1$.}
    $$
  
    General finding by Krapivsky and Redner:\cite{krapivsky2001a}
    $$
    n_k 
    \sim 
    k^{-\nu}
    e^{-c_1 k^{1-\nu} + \mbox{\scriptsize correction terms}}.
    $$
  
    Stretched exponentials (truncated power laws).
  
    aka Weibull distributions.
  
    \alert{Universality}: now details of kernel \alertb{do not} matter.
  
    Distribution of degree is universal providing $\nu<1$.
  


  \textbf{Sublinear attachment kernels}

  \textbf{Details:}
    
    
      For $1/2 < \nu < 1$:
      $$
      n_k \sim k^{-\nu} e^{-\mu \left(\frac{k^{1-\nu} - 2^{1-\nu}}{1-\nu} \right)}
      $$
    
      For $1/3 < \nu < 1/2$:
      $$
      n_k \sim k^{-\nu} e^{-\mu \frac{k^{1-\nu}}{1-\nu} + \frac{\mu^2}{2}\frac{k^{1-2\nu}}{1-2\nu}}
      $$
    
      And for $1/(r+1) < \nu < 1/r$, we have $r$ pieces in exponential.
    
  


\subsection{Superlinear attachment kernels}

  \textbf{Superlinear attachment kernels}

  
  
    Rich-get-much-richer:
    $$
    A_k \sim k^\nu
    \mbox{\ with $\nu > 1$.}
    $$
  
    Now a \alert{winner-take-all} mechanism.
  
    One single node ends up being connected to
    almost all other nodes.
  
    For $\nu>2$, all but a finite \# of nodes connect
    to one node.
  
  

%% todo

%% cites for exponents

%% more exponents

%% history